Label,Sentence
RP,For self-supervised contrastive learning, models can easily collapse and generate trivial constant solutions
RP,Self-supervised learning (SSL) of graph neural networks is emerging as a promising way of leveraging unlabeled data
RP,In psychology, relational learning refers to the ability to recognize and respond to relationship among objects irrespective of the nature of those objects
RP,We consider representation learning of 3D molecular graphs in which each atom is associated with a spatial position in 3D
RP,Complex learning rate schedules have become an integral part of deep learning
RP, In this work, we aim to transfer the success of ensembles from supervised learning to the setting of batch RL
RP,Learning energy-based models (EBMs) is known to be difficult especially on discrete data where gradient-based learning strategies cannot be applied directly
RP,One popular objective for the image-to-image translation task is to independently control the coarse-level object arrangements (posture) and the fine-grained level styling (identity) of the generated image from two exemplar sources
RP,This paper proposes a novel contrastive learning framework, called Self-Contrastive (SelfCon) Learning, that self-contrasts within multiple outputs from the different levels of a multi-exit network
RP, The primary contribution of this work is to further our understanding of the decision boundary geometry of ANN classifiers by utilizing such adversarial perturbations
RP, In this paper, we focus on metamodeling with active learning and the canonical Gaussian Process (GP)
RP,Semi-Supervised Learning (SSL) is fundamentally a missing label problem, in which the label Missing Not At Random (MNAR) problem is more realistic and challenging, compared to the widely-adopted yet naive Missing Completely At Random assumption where both labeled and unlabeled data share the same class distribution
RP,Zeroth-order (ZO, also known as derivative-free) methods, which estimate a noisy gradient  based on the finite difference with two function evaluations, have attracted much attention recently because of its broad applications in machine learning community
RP,Imitation learning is the problem of teaching an agent to replicate expert policy from demonstrations when the underlying reward function is unavailable
RP,Recent work suggests that feature constraints in the training datasets of deep neural networks (DNNs) drive robustness to adversarial noise (Ilyas et al, 2019)
RP,Learning to capture feature relations effectively and efficiently is essential in click-through rate (CTR) prediction of modern recommendation systems
RP, We argue this setting better resembles the real-world data collection and annotation process and hence can help close the gap to real-world scenarios
RP,We present a neat yet effective recursive operation on vision transformers that can improve parameter utilization without involving additional parameters
RP,Conditional generation is a subclass of generative problems when the output of generation is conditioned by a class attributes’ information
RP,Masked speech modeling (MSM) methods such as wav2vec2 or w2v-BERT learn representations over speech frames which are randomly masked within an utterance
RP,Multi-label text classification (MLTC) aims to assign a set of labels to each given document
RP,Recent studies have demonstrated that using machine learning for social applications can lead to injustice in the form of racist, sexist, and otherwise unfair and discriminatory outcomes
RP,Recently, multi-agent reinforcement learning (MARL) adopts the centralized training with decentralized execution (CTDE) framework that trains agents using the data from all agents at a centralized server while each agent takes an action from its observation
RP,Recently, post-training quantization (PTQ) has driven much attention to produce efficient neural networks without long-time retraining
RP,In this work, we build a knowledge graph that captures key attributes of content and notifications in a digital health platform for diabetes management
RP,Persistent Homologies have been successfully used to increase the performance of deep networks trained to detect curvilinear structures and to improve the topological quality of the results
RP,Rate-distortion (R-D) function, a key quantity in information theory, characterizes the fundamental limit of how much a data source can be compressed subject to a fidelity criterion, by any lossy compression algorithm
RP,Mutual information (MI) is a fundamental quantity in information theory and machine learning
RP,Learning generalizable speech representations for unseen samples in different domains has been a challenge with ever increasing importance to date
RP,Towards avoiding collapse in self-supervised learning (SSL), contrastive loss is widely used but often requires a large number of negative samples
RP, In this paper, we specifically consider the problems of domain shifts and subpopulation shifts, where learning invariant representations by aligning domain-specific representations or balancing the risks across domains with regularizers are popular solutions
RP,Adversarial training has been empirically proven to be one of the most effective and reliable defense methods against adversarial attacks
RP,The performance of existing text style transfer models is severely limited by the non-parallel datasets on which the models are trained
RP, More specifically, we question how many parameters of a transformer model can be shared across modalities during contrastive pre-training, and rigorously study architectural design choices that position the proportion of parameters shared along a spectrum
RP, This paper tackles the problem of Trojan detection, namely, identifying Trojaned models -- models trained with poisoned data
RP, This work presents the first experimental demonstration of practical on-chip QNN training with parameter shift
RP,Deep learning models are trained on multiple categories jointly to solve several real-world problems
RP, In this work, we aim to achieve adversarial robustness within larger bounds, against perturbations that may be perceptible, but do not change human (or Oracle) prediction
RP, We focus on a classical measure called Takeuchi's Information Criteria (TIC) to investigate allowed conditions in which the criteria can well explain generalization gaps caused by DNNs
RP, We argue that training tiny models are different from large models: rather than augmenting the data, we should augment the model, since tiny models tend to suffer from under-fitting rather than over-fitting due to limited capacity
RP,Learning invariant representations is an important requirement in training machine learning models that are driven by spurious correlations in the datasets
RP, In this paper, we investigate and discuss what a good representation should be for a general loss (InfoNCE) in graph contrastive learning
RP,One of the objectives of continual learning is to prevent catastrophic forgetting in learning multiple tasks sequentially
RP,Membership inference attacks determine if a given data point is used for training a target model
RP,We introduce Palette, a simple and general framework for image-to-image translation using conditional diffusion models
RP,deep reinforcement learning (DRL) has demonstrated great potentials in solving sequential decision making problems in many applications
RP,Due to spurious correlations, machine learning systems often fail to generalize to environments whose distributions differ from the ones used at training time
RP,Strong adversarial attacks are important for evaluating the true robustness of deep neural networks
RP,This paper presents a systematic study of multi-objective online learning
RP,Learning predictive models for unlabeled spatiotemporal data is challenging in part because visual dynamics can be highly entangled in real scenes, making existing approaches prone to overfit partial modes of physical processes while neglecting to reason about others
RP,Interval Bound Propagation (IBP) is so far the base of state-of-the-art methods for training neural networks with certifiable robustness guarantees when potential adversarial perturbations present, while the convergence of IBP training remains unknown in existing literature
RP,Location encoding is valuable for a multitude of tasks where both the absolute positions and local contexts (image, text, and other types of metadata) of spatial objects are needed for accurate predictions
RP,Current Open-Domain Question Answering (ODQA) model paradigm often contains a retrieving module and a reading module
RP,Accurate uncertainty quantification is a key building block of trustworthy machine learning systems
RP,We present a new method for one shot domain adaptation
RP,We describe a curriculum learning framework capable of discovering optimal curricula in addition to performing standard curriculum learning
RP,We investigate the relationship between commonly considered notions of multiclass calibration and the calibration algorithms used to achieve these notions, leading to two broad contributions
RP, In this paper, we investigate if such performance can be extended to image generation
RP, In this paper, we study the cause of SP from the perspective of example difference
RP,Adversarial imitation learning techniques are based on modeling statistical divergences using agent and expert demonstration data
RP,Denoising Diffusion Probabilistic Models (DDPMs) have emerged as a flexible family of generative models rivaling GANs and autoregressive models in sample quality and likelihoods
RP,Unsupervised dataset alignment estimates a transformation that maps two or more source domains to a shared aligned domain given only the domain datasets
RP,Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (i,e, "make breakfast"), to a fixed set of actionable steps (i,e, "open fridge")
RP, We believe this work has opened up a new perspective for future contrastive learning innovations
RP, In this paper, we intend to investigate whether offline training is able to learn policy representations that elevate performance on downstream MARL tasks
RP,Graph Convolutional Networks (GCNs) is the state-of-the-art method for learning graph-structured data, and training large-scale GCNs requires distributed training across multiple accelerators such that each accelerator is able to hold a partitioned subgraph
RP,Semi-Supervised Learning (SSL) has seen success in many application domains, but this success often relies on the availability of task-specific unlabeled data
RP, We posit that this is because it is difficult to learn the direct mapping from concepts to text and generalize to unseen concept combinations using only training examples
RP,In many prediction problems, spurious correlations are induced by a changing relationship between the label and a nuisance variable that is also correlated with the covariates
RP,Adversarial Propagation (AdvProp) is an effective way to improve recognition models, leveraging adversarial examples
RP,In clustering algorithms, the choice of initial centers is crucial for the quality of the learned clusters
RP,Owing much to the revolution of information technology, recent progress of deep learning benefits incredibly from the vastly enhanced access to data available in various digital formats
RP,While ensemble methods have been widely used for robustness against random perturbations (\ie the average case), ensemble approaches for robustness against adversarial perturbations (\ie the worst case) have remained elusive despite multiple prior attempts
RP, This motivated the development of Byzantine-resilient federated learning algorithms, such as Krum, Trimmed mean, and FoolsGold
RP,`Double descent' delineates the generalization behaviour of models depending on the regime they belong to: under- or over-parameterized
RP,Even though meta-learning has attracted research wide attention in recent years, the generalization problem of meta-learning is still not well addressed
RP,Our goal is to find time-delayed latent causal variables and identify their relations from temporal measured variables
RP,The representation of objects is the building block of higher-level concepts
RP,In model-based reinforcement learning (MBRL) such as Dreamer, the approaches based on observation reconstruction often fail to discard task-irrelevant details, thus struggling to handle visual distractions or generalize to unseen distractions
RP,Detecting out-of-distribution examples is important for safety-critical machine learning applications such as detecting novel biological phenomena and self-driving cars
RP,Batch Reinforcement Learning algorithms aim at learning the best policy from a batch of data without interacting with the environment
RP,While deep reinforcement learning methods have shown impressive results in robot learning, their sample inefficiency makes the learning of complex, long-horizon behaviors with real robot systems infeasible
RP,The goal of conventional federated learning (FL) is to train a global model for a federation of clients with decentralized data, reducing the systemic privacy risk of centralized training
RP,This work develops a novel framework for communication-efficient distributed learning where the models to be learned are overparameterized
RP,We focus on controllable disentangled representation learning (C-Dis-RL), where users can control the partition of the disentangled latent space to factorize dataset attributes (concepts) for downstream tasks
RP,Graph Convolutional Networks (GCNs) have gained an increasing attention thanks to their state-of-the-art (SOTA) performance in graph-based learning tasks
RP,Time series analysis is a widespread task in Natural Sciences, Social Sciences, and Engineering
RP,In this paper, we study the statistical limits of deep learning techniques for solving elliptic partial differential equations (PDEs) from random samples using the Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINNs)
RP, We investigate the question of how to efficiently adapt these models to downstream tasks
RP,We show that an arbitrary lower bound of the optimal value function can be used to improve the Bellman value target during value learning
RP,Deep learning models have been widely used in automatic drug design
RP,Neural text generation models are typically trained by maximizing log-likelihood with the sequence cross entropy loss, which encourages an exact token-by-token match between a target sequence with a generated sequence
RP,While there has been substantial recent work studying  generalization of neural networks, the ability of deep nets in automating the process of feature extraction still evades a thorough mathematical understanding
RP,We study COMP-AMS, a distributed optimization framework based on gradient averaging and adaptive AMSGrad algorithm
RP,Bayesian Optimization has been challenged by the large-scale and high-dimensional datasets, which are common in real-world scenarios
RP,Despite our best efforts, deep learning models remain highly vulnerable to even tiny adversarial perturbations applied to the inputs
RP,Knowledge distillation aims to obtain a small and effective deep model (student) by learning the output from a larger model (teacher)
RP,Given the prevalence of large-scale graphs in real-world applications, the storage and time for training neural models have raised increasing concerns
RP,Meta-learning has emerged as a potent paradigm for quick learning of few-shot tasks, by leveraging the meta-knowledge learned from meta-training tasks
RP,We present HyperCGAN: a conceptually simple and general approach for text-to-image synthesis that uses hypernetworks to condition a GAN model on text
RP,Brain-wide measurements of activity and anatomical connectivity of the C. elegans nervous system in principle allow for the development of detailed mechanistic computational models
RP, In this work, we aim to protect sensitive information in the inference phase of MTC and propose a novel Multi-Trigger-Key (MTK) framework to achieve the privacy-preserving objective
RP,Vision Transformers (ViTs) take all the image patches as tokens and construct multi-head self-attention (MHSA) among them
RP,Question Answering (QA) has been a long-standing research topic in AI and NLP fields, and a wealth of studies has been conducted to attempt to equip QA systems with human-level reasoning capability
RP,We propose POLAR, a polynomial arithmetic framework that leverages polynomial overapproximations with interval remainders for bounded-time reachability analysis of neural network-controlled systems (NNCSs)
RP, We argue that the standard concentration fails to fully characterize the intrinsic robustness of a classification problem, since it ignores data labels which are essential to any classification task
RP,Federated learning (FL) is a privacy-preserving paradigm where multiple participants jointly solve a machine learning problem without sharing raw data
RP,To obtain a joint representation from multimodal data in variational autoencoders (VAEs), it is important to infer the representation from arbitrary subsets of modalities after learning
RP, In this paper, we focus on fully unsupervised AD, in which the entire training dataset, containing both normal and anomalous samples, is unlabeled
RP,Knowledge transfer between heterogeneous source and target networks and tasks has received a lot of attention in recent times as large amounts of quality labelled data can be difficult to obtain in many applications
RP,rQdia (pronounced “Arcadia”) regularizes Q-value distributions with augmented images in pixel-based deep reinforcement learning
RP,We present a two-step hybrid reinforcement learning (RL) policy that is designed to generate interpretable and robust hierarchical policies on the RL problem with graph-based input
RP,Deep neural networks are used for a wide range of regression problems and are typically trained by minimizing the squared or absolute error of output labels
RP,Value approximation using deep neural networks is a critical component of state-of-the-art reinforcement learning algorithms
RP,Conditional set generation learns a mapping from an input sequence of tokens to a set
RP,Graph self-supervised learning has gained increasing attention due to its capacity to learn expressive node representations
RP,We develop a multiple shooting method for learning in deep neural networks based on the Lagrangian perspective on automatic differentiation
RP,We tackle the problem of accelerating certain optimization problems related to steady states in ODE and energy minimization problems common in physics
RP,Pre-trained language models (LMs) have been shown to memorize a substantial amount of knowledge from the pre-training corpora; however, they are still limited in recalling factually correct knowledge given a certain context
RP,Empirical studies on the robustness of graph neural networks (GNNs) have suggested a relation between the vulnerabilities of GNNs to adversarial attacks and the increased presence of heterophily in perturbed graphs (where edges tend to connect nodes with dissimilar features and labels)
RP,deep reinforcement learning (DRL) agents are often sensitive to visual changes that were unseen in their training environments
RP,The Neural Tangent Kernel (NTK), defined as the outer product of the neural network (NN) Jacobians, \Theta_\theta(x_1, x_2) = \left[\partial f(\theta, x_1)\big/\partial \theta\right] \left[\partial f(\theta, x_2)\big/\partial \theta\right]^T, has emerged as a central object of study in deep learning
RP,The advent of ubiquitous machine learning (ML) has led to exciting revolution in computing today
RP,We introduce neural network architectures that model the mechanism that generates data and address the difficult problem of disentangling the multimodal structure of data ensembles
RP,The prototypical network is a prototype classifier based on meta-learning and is widely used for few-shot learning because it classifies unseen examples by constructing class-specific prototypes without adjusting hyper-parameters during meta-testing
RP, In this paper, we explore the possibility of zero-shot learning in RecSys, to enable generalization from an old dataset to an entirely new dataset
RP,We propose to identify directions invariant to a given classifier so that these directions can be controlled in tasks such as style transfer
RP,Knowledge Distillation (KD) is an algorithm that transfers the knowledge of a trained, typically larger, neural network into another model under training
RP,Many real-world applications of reinforcement learning (RL) require the agent to deal with high-dimensional observations such as those generated from a megapixel camera
RP,Second-order optimization has been recently explored in neural network training
RP, In this work, we address the problem of efficiently learning a policy while making a minimal number of state-action queries to the transition function
RP,Recent methods for embodied instruction following are typically trained end-to-end using imitation learning
RP,Training very deep neural networks is still an extremely challenging task
RP,User interface modeling is inherently multimodal, which involves several distinct types of data: images, structures and language
RP,The nearest neighbor search (NNS) problem is widely studied in Euclidean space, and graph-based algorithms are known to outperform other approaches for this task
RP,Assignment, a task to match a limited number of elements, is a fundamental problem in informatics
RP,Recently, as Artificial Intelligence (AI) develops, many companies in various industries are trying to use AI by grafting it into their domains
RP,Transformers have recently been popular for learning and inference in the spatial-temporal domain
RP,Reward signals in reinforcement learning can be expensive signals in many tasks and often require access to direct state
RP,Recent attempts at image steganography make use of advances in deep learning to train an encoder-decoder network pair to hide and retrieve secret messages in images
RP,Unsupervised Anomaly detection (AD) requires building a notion of normalcy, distinguishing in-distribution (ID) and out-of-distribution (OOD) data, using only available ID samples
RP, Our work sheds light on how to attack federated learning systems through multi-agent coordination
RP,We leverage logical composition in reinforcement learning to create a framework that enables an agent to autonomously determine whether a new task can be immediately solved using its existing abilities, or whether a task-specific skill should be learned
RP, In this paper, we focus on evaluating the robustness of MARL agents in continuous control tasks
RP,We are interested in representation learning from labeled or unlabeled data
RP,In distribution compression, one aims to accurately summarize a probability distribution P using a small number of representative points
RP,Keywords: Adversarial attacks, Robustness Certification, Abstract Interpretation, Deep LearningWe introduce the concept of provably robust adversarial examples - connected input regions constructed from standard adversarial examples which are guaranteed to be provably robust to a set of perturbations (such as changes in pixel intensity and geometric transformations)
RP,Representation learning lies at the heart of the empirical success of deep learning for dealing with the curse of dimensionality
RP,We present a novel framework to train a large deep neural network (DNN) for only once, which can then be pruned to any sparsity ratio to preserve competitive accuracy without any re-training
RP,Private multi-winner voting is the task of revealing k-hot binary vectors that satisfy a bounded differential privacy guarantee
RP,Existing methods for style transfer operate either with paired sentences or distributionally matched corpora which differ only in the desired style
RP,Continual Learning (CL) is an emerging machine learning paradigm that aims to learn from a continuous stream of tasks without forgetting knowledge learned from the previous tasks
RP,Factorizing a large matrix into small matrices is a popular strategy for model compression
RP,Distributional Reinforcement Learning (RL) differs from traditional RL by estimating the distribution over returns to capture the intrinsic uncertainty of MDPs
RP,Research on generalization bounds for deep networks seeks to give ways to predict test error using just the training dataset and the network parameters
RP,Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics
RP,Representational learning forms the backbone of most deep learning applications, and the value of a learned representation is intimately tied to its information content regarding different factors of variation
RP,Multivariate time series involve a series of valuable applications in the real world, and the basic premise of which is that multiple variables are interdependent
RP,Building conceptual abstractions from sensory information and then reasoning about them is central to human intelligence
RP,Learning accurate classifiers for novel categories from very few examples, known as few-shot image classification, is a challenging task in statistical machine learning and computer vision
RP, Thus, the question arises: how complex does the inferential model need to be, in order to be able to accurately model the posterior distribution of a given generative model?  In this paper, we identify an important property of the generative map impacting the required size of the encoder
RP, In this work, we study masked image modeling (MIM) and indicate the necessity and challenges of using a semantically meaningful visual tokenizer
RP,Federated Learning is a machine learning technique where a network of clients collaborates with a server to learn a centralized model while keeping data localized
RP,Our goal is to adapt pre-trained neural language models (NLMs) to the unconditional text generation task within the target domain
RP,model-based reinforcement learning methods achieve significant sample efficiency in many tasks, but their performance is often limited by the existence of the model error
RP,Convolutional Neural Networks have achieved remarkable success in face recognition, in part due to the abundant availability of data
RP,Self-supervised learning has witnessed remarkable progress in recent years, in particular with the introduction of augmentation-based contrastive methods
RP,   We then try to understand the principles that might be at work using synthetic experiments: images generated based on a "dead leaves" model
RP,Pruning is a promising approach to compress complex deep learning models in order to deploy them on resource-constrained edge devices
RP,When deployed in the real world, machine learning models inevitably encounter changes in the data distribution, and certain---but not all---distribution shifts could result in significant performance degradation
RP,First proposed by Seide et al (2014) as a heuristic, error feedback (EF) is a very popular mechanism for enforcing convergence of distributed gradient-based optimization methods enhanced with communication compression strategies based on the application of contractive compression operators
RP,Offline reinforcement learning is a promising approach for practical applications since it does not require interactions with real-world environments
RP,In this work, we empirically study the data scaling properties of neural machine translation (NMT)
RP, This paper focuses on heat maps based saliency methods that often provide explanations that look best to humans
RP,Message Passing Neural Networks (MPNNs) are a common type of Graph Neural Network (GNN), in which each node’s representation is computed recursively by aggregating representations (“messages”) from its immediate neighbors akin to a star-shaped pattern
RP,To train machine learning models that are robust to distribution shifts in the data, distributionally robust optimization (DRO) has been proven very effective
RP,It is well known that deep learning models are susceptible to adversarial attacks
RP,Bayesian optimization (BO) is a popular method for efficiently inferring optima of an expensive black-box function via a sequence of queries
RP, One of them considers the state-of-the-art StyleGAN2 trained on FFHQ dataset, where uniform sampling via MaGNET increases distribution precision & recall by 4.12% & 3.01% and decreases gender bias by 41.2%, without requiring labels or retraining
RP,Human intervention is an effective way to inject human knowledge into the training loop of reinforcement learning, which can bring fast learning and ensured training safety
RP,The Transformer (Vaswani et al, 2017) has enjoyed remarkable success at learning representations for discrete-time sequences, such as natural language sentences
RP,We extend semi-supervised learning to the problem of domain adaptation to learn significantly higher-accuracy models that train on one data distribution and test on a different one
RP,It has been observed that graph neural networks (GNN) sometimes struggle to maintain a healthy balance between modeling long-range dependencies across nodes while avoiding unintended consequences such as oversmoothed node representations
RP,Sparse neural networks (NNs) are intensively investigated in literature due to their appeal in saving storage, memory, and computational costs
RP,Most existing sequence generation models produce outputs in one pass, usually left-to-right
RP,deep reinforcement learning (RL) has led to encouraging successes in many challenging control tasks
RP, We hope the proposed SRN can further contribute to the cutting-edge reconstructive methods as a promising backbone, and also benefit the realistic tasks, i,e, real-time/high-resolution HSI reconstruction, solely as a baseline
RP, Few-shot image classification, where the goal is to generalize to tasks with limited labeled data, has seen great progress over the years
RP,Cross-domain imitation learning studies how to leverage expert demonstrations of one agent to train an imitation agent with a different embodiment or morphology
RP,Scaling neural networks to "large" sizes, with billions of parameters, has been shown to yield impressive results on many challenging problems
RP,The empirical success of deep convolutional networks on tasks involving high-dimensional data such as images or audio suggests that they can efficiently approximate certain functions that are well-suited for such tasks
RP,The goal of dynamic scene deblurring is to remove the motion blur present in a given image
RP,Establishing correspondences between images remains a challenging task, especially under large appearance changes due to different viewpoints and intra-class variations
RP,We introduce Particle-SDCA, a gradient-based optimization algorithm for two-layer neural networks in the mean field regime that achieves exponential convergence rate in regularized empirical risk minimization
RP,Continual learning (CL) is a real-world learning paradigm in which a model learns from a stream of incoming data while avoiding forgetting previously learned knowledge
RP,multi-agent reinforcement learning tasks put a high demand on the volume of training samples
RP,Understanding the implicit bias of Stochastic Gradient Descent (SGD) is one of the key challenges in deep learning, especially for overparametrized models, where the local minimizers of the loss function L can form a manifold
RP,Protecting the privacy of user data is crucial when training neural text generation models, which may leak sensitive user information during generation
RP,Structural pruning can simplify network architecture and improve the inference speed
RP,Machine learning methods have recently shown promise in solving partial differential equations (PDEs)
RP,Neural Networks (NN) outperform humans in multiple domains
RP,One practical challenge in reinforcement learning (RL) is how to make quick adaptations when faced with new environments
RP,We propose a framework that protects against sensitive information leakage to facilitate data release with untrusted parties
RP,In this paper, we demonstrate that self-learning techniques like entropy minimization or pseudo-labeling are simple, yet effective techniques for increasing test performance under domain shifts
RP, In this work, we ask an intriguing question: ``Under what kinds of perturbations do ViTs become weaker learners compared to CNNs"? Driven by this question, we conduct a comprehensive examination on the robustness of both ViTs and CNNs under various existing adversarial attacks to understand the underlying reason for their robustness
RP,While deep learning through empirical risk minimization (ERM) has succeeded at achieving human-level performance at a variety of complex tasks, ERM generalizes poorly to distribution shift
RP,Vertical Federated Learning (VFL) is a distributed learning paradigm that allows multiple agents to jointly train a global model when each agent holds a different subset of features for the same sample(s)
RP,Meta-learning aims to extract meta-knowledge from historical tasks to accelerate learning on new tasks
RP,Pruning plays an essential role in deploying deep neural nets (DNNs) to the hardware of limited memory or computation
RP,Multilingual machine translation aims to develop a single model for multiple language directions
RP,Unsupervised domain adaptation methods aim to generalize well on unlabeled test data that may have a different (shifted) distribution from the training data
RP,Molecular graph representation learning is a fundamental problem in modern drug and material discovery
RP,Having consumed huge amounts of training data and computational resource, large-scale pre-trained models are often considered key assets of AI service providers
RP,Consistency training has been widely adopted and shown great promise in deep learning
RP, In this paper we study the principles behind attention and its connections with prior art
RP,The expected improvement (EI) is a popular technique to handle the tradeoff between exploration and exploitation under uncertainty
RP,    Many applications of machine learning require predicting flexible probability distributions over model outputs
RP,As a seminal tool in self-supervised representation learning, contrastive learning has gained unprecedented attention in recent years
RP,The problem of molecular generation has received significant attention recently
RP,Increasing concerns have been raised on deep learning fairness in recent years
RP, It further considers the intra-contrastive relation within the positive and negative pairs to narrow the gap between the sampled and true distribution, which is important when datasets are less curated
RP,Learning to collaborate is critical in multi-agent reinforcement learning (MARL)
RP,Current LiDAR-only 3D detection methods inevitably suffer from the sparsity of point clouds
RP,We consider learning causal relationships under conditional moment restrictions
RP,While remarkable progress in imbalanced supervised learning has been made recently, less attention has been given to the setting of imbalanced semi-supervised learning (SSL) where not only is a few labeled data provided, but the underlying data distribution can be severely imbalanced
RP,Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al,  2020)
RP,To achieve a satisfactory generalization performance on prediction tasks in an unseen domain, existing Domain generalization (DG) approaches often rely on the strict assumption of fixed domain-invariant features and common hypotheses learned from a set of training domains
RP,Despite their excellent performance on in-distribution (ID) data, deep neural networks often confidently predict on out-of-distribution (OOD) samples that come from novel classes instead of flagging them for expert evaluation
RP,Robotic manipulation planning is the problem of finding a sequence of robot configurations that involves interactions with objects in the scene, e,g,, grasp, placement, tool-use, etc
RP,Large-batch training is an important direction for distributed machine learning, which can improve the utilization of large-scale clusters and therefore accelerate the training process
RP,This paper studies the cooperative learning of two generative flow models, in which the two models are iteratively updated based on the jointly synthesized examples
RP,This paper tackles the problem of learning value functions from undirected state-only experience (state transitions without action labels i,e, (s,s',r) tuples)
RP, In this paper, motivated by multi-task learning of shareable feature representations, we consider a novel problem of learning a shared generative model that is useful across various visual perception tasks
RP,We present a new method LiST for efficient fine-tuning of large pre-trained language models (PLMs) in few-shot learning settings
RP,Text-based games (TBG) have emerged as promising environments for driving research in grounded language understanding and studying problems like generalization and sample efficiency
RP,Universal value functions are used to score the long-term utility of actions to achieve a goal from the current state
RP,Online reinforcement learning (RL) can suffer from poor exploration, sparse reward, insufficient data, and overhead caused by inefficient interactions between an immature policy and a complicated environment
RP,This paper studies the problem of information security in the distributed learning framework
RP,Antibodies are versatile proteins that bind to pathogens like viruses and stimulate the adaptive immune system
RP,Vision transformers (ViTs) have gained popularity recently
RP,In this paper, we propose Multiresolution Equivariant Graph Variational Autoencoders (MGVAE), the first hierarchical generative model to learn and generate graphs in a multiresolution and equivariant manner
RP,We propose a principled method to learn a set of human-readable logic rules to explain temporal point processes
RP,Sparsity has been identified as an important characteristic in learning neural networks that generalize well, forming the key idea in constructing minimal representations
RP,Recently, positional encoding of input coordinates has been found crucial to enable learning of high-frequency functions with multilayer perceptrons taking low-dimensional coordinate values
RP,Guided image synthesis enables everyday users to create and edit photo-realistic images with minimum effort
RP,We study the problem of hypergraph reasoning in large domains, e,g,, predicting the relationship between several entities based on the input facts
RP,A topic model is often formulated as a generative model that explains how each word of a document is generated given a set of topics and document-specific topic proportions
RP, This paper studies whether the models' comparable performance are sheer coincidence, or they can be unified into a single framework
RP, This leads us to ponder the computational advantages and functional role of these “grandmother cells
RP,We study the problem of controllable text generation (CTG): steering a language model (LM) to generate text with a desired attribute
RP,Pre-training on massive unlabeled datasets greatly improves accuracy under distribution shifts
RP, This allows learning an individually fair representation where similar individuals are mapped close together, by using adversarial training to minimize the distance between the representations of similar individuals
RP,Soft-greedy operators, namely \varepsilon-greedy and softmax, remain a common choice to induce a basic level of exploration for action-value methods in reinforcement learning
RP,Stochastic simulations such as large-scale, spatiotemporal, age-structured epidemic models are computationally expensive at fine-grained resolution
RP,Instance-dependent label noise (IDN) widely exists in real-world datasets and usually misleads the training of deep neural networks
RP,Anomaly segmentation, which localizes defective areas, is an important component in large-scale industrial manufacturing
RP,Active learning (AL) aims at reducing labeling efforts by identifying the most valuable unlabeled data points from a large pool
RP,We develop a fast and reliable method for solving large-scale optimal transport (OT) problems at an unprecedented combination of speed and accuracy
RP,Markov chain Monte Carlo (MCMC), such as Langevin dynamics, is valid for approximating intractable distributions
RP,We introduce a general approach, called invariance through inference, for improving the test-time performance of a behavior agent in deployment environments with unknown perceptual variations
RP,Antonymic and synonymic pairs may both occur nearby in word embeddings spaces because they have similar distributional information
RP,L_p Regression on Structured Inputs is an important problem in data analysis and machine learning where we find a vector \(\mathbf{x\in\mathbb R^{d\) that minimizes \(\|\mathbf{A\mathbf{x-\mathbf{b\|_p\) for a structured matrix \(\mathbf{A\in\mathbb R^{n \times d\) and response vector \(\mathbf{b\in\mathbb R^{n\)
RP,Learning sparse coordination graphs adaptive to the coordination dynamics among agents is a long-standing problem in cooperative multi-agent learning
RP,Recent research has shown that numerous human-interpretable directions exist in the latent space of GANs
RP,In constrained reinforcement learning (RL), a learning agent seeks to not only optimize the overall reward but also satisfy the additional safety, diversity, or budget constraints
RP, We first seek to formally understand the transfer of robustness from classifiers trained on proxy distributions to the real data distribution
RP,Cross-Entropy Method (CEM) is a popular approach to planning in model-based reinforcement learning
RP,Machine learning models that achieve high overall accuracy often make systematic errors on important subgroups (or slices) of data
RP, In this paper, we investigate this intriguing problem from a new perspective, i,e, injecting appropriate forms of sparsity during adversarial training
RP,Data-driven (deep) learning approaches for image classification are prone to adversarial attacks
RP, This paper tackles the following question: Can one incorporate previous knowledge aggregation while learning channel attention more efficiently? To this end, we propose a Previous Knowledge Channel Attention Module( PKCAM), that captures channel-wise relations across different layers to model the global context
RP, This work proposes to tackle the explore-vs-exploit dilemma using a multi-stage approach that explicitly disentangles these two strategies within each episode
RP, With our work, we want to raise awareness and incentivize future developments of proper countermeasures
RP,Unsupervised reinforcement learning (RL) studies how to leverage environment statistics to learn useful behaviors without the cost of reward engineering
RP,Despite the tremendous success of applying traditional backtrack-style combinatorial search methods in various NP-complete domains such as SAT and CSP as well as using deep reinforcement learning (RL) to tackle two-player games such as Go, PSPACE-hard AI planning has remained out of reach for current AI planning systems
RP,Self-attention, an architectural motif designed to model long-range interactions in sequential data, has driven numerous recent breakthroughs in natural language processing and beyond
RP,Self-supervised visual representation learning has attracted significant research interest
RP,We present a conditional variational auto-encoder (VAE) which, to avoid the substantial cost of training from scratch, uses an architecture and training objective capable of leveraging a foundation model in the form of a pretrained unconditional VAE
RP,Automatic evaluations for natural language generation conventionally rely on token-level or embedding-level comparisons with the text references
RP,Non-local attention module has been proven to be crucial for image restoration
RP,Federated learning allows multiple parties to collaboratively train a joint model without sharing local data
RP,We propose GRAph Neural Diffusion with a source term (GRAND++) for graph deep learning with a limited number of labeled nodes, i,e, low-labeling rate
RP,Learning to autonomously assemble shapes is a crucial skill for many robotic applications
RP,Modeling many-body systems has been a long-standing challenge in science, from classical and quantum physics to computational biology
RP,Monotone Operator Equilibrium Models (monDEQs) represent a class of models that combine the powerful deep equilibrium paradigm with convergence guarantees
RP,Graph Neural Networks (GNNs) exploit signals from node features and the input graph topology to improve node classification task performance
RP,Inspired by the promising results of Transformers in object detection in images, it is interesting to formulate Transformer based methods for temporal action localization (TAL) in videos
RP,We study unsupervised data selection for semi-supervised learning (SSL), where a large-scale unlabeled data is available and a small subset of data is budgeted for label acquisition
RP,Neural machine translation (NMT) systems have received massive attention from academia and industry
RP,Multilingual representations pre-trained with monolingual data offer unmatched task performances between languages
RP,Reconstructing medical images from partial measurements is an important inverse problem in Computed Tomography (CT) and Magnetic Resonance Imaging (MRI)
RP, In this work, we focus on certifying the robustness of offline RL in the presence of poisoning attacks, where a subset of training trajectories could be arbitrarily manipulated
RP,Despite advances in hierarchical reinforcement learning, its applications to path planning in autonomous driving on highways are challenging
RP,Many deep learning applications benefit from using large models with billions of parameters
RP,Importance sampling is a promising strategy for improving the convergence rate of stochastic gradient methods
RP,In prediction problems, coarse and imprecise sources of input can provide rich information about labels, but are not readily used by discriminative learners
RP,This work develops mixup to graph data
RP, In this paper, we focus on partially observable environments and propose to learn a minimal set of state representations that capture sufficient information for decision-making, termed Action-Sufficient state Representations (ASRs)
RP,Neural processes (NPs) aim to stochastically complete unseen data points based on a given context dataset
RP,Gradient boosting is the most popular method of constructing ensembles that allow getting state-of-the-art results on many tasks
RP,As Artificial Intelligence as a Service gains popularity, protecting well-trained models as intellectual property is becoming increasingly important
RP, Our work builds upon neural radiance fields (NeRFs), which implicitly model the volumetric density and directionally-emitted radiance of a scene
RP,We consider the problem of sequential robotic manipulation of deformable objects using tools
RP,Distribution shifts, in which the training distribution differs from the testing distribution, can significantly degrade the performance of Graph Neural Networks (GNNs)
RP,How to extract as much learning signal from each trajectory data has been a key problem in reinforcement learning (RL), where sample inefficiency has posed serious challenges for practical applications
RP,An abundance of neural network models and algorithms for diverse tasks on graphs have been developed in the past five years
RP,This paper provides a novel framework that learns canonical embeddings for non-rigid shape matching
RP,Kernel Point Convolution (KPConv) achieves cutting-edge performance on 3D point cloud applications
RP,Comparison of data representations is a complex multi-aspect problem that has not enjoyed a complete solution yet
RP,Minwise hashing (MinHash) is an important and practical algorithm for generating random hashes to approximate the Jaccard (resemblance) similarity in massive binary (0/1) data
RP,Generative adversarial networks (GANs) have attained photo-realistic quality in image generation
RP,As an important problem in causal inference, we discuss the identification and estimation of treatment effects (TEs) under limited overlap; that is, when subjects with certain features belong to a single treatment group
RP,Federated learning has been deployed to train machine learning models from decentralized client data on mobile devices in practice
RP,Defining a valid graph distance is a challenging task in graph machine learning because we need to consider the theoretical validity of the distance, its computational complexity, and effectiveness as a distance between graphs
RP, In this paper we initiate the first study of this phenomenon for graph data
RP,We study different notions of equivariance as an inductive bias in Reinforcement Learning (RL) and propose new mechanisms for recovering representations that are equivariant to both an agent’s action, and symmetry transformations of the state-action pairs
RP,In imitation learning, one aims to learn task-solving policies using access to near-optimal expert trajectories collected from the task environment
RP,Existing domain adaptation methods tend to treat every domain equally and align them all perfectly
RP,In this paper, we analyze the number of neurons and training parameters that a neural network needs to approximate multivariate functions of bounded second mixed derivatives --- Korobov functions
RP, This paper formally studies how adaptive methods help performance in GANs
RP,In this paper, we aim to improve the generalization ability of DR models from source training domains with rich supervision signals to target domains without any relevant labels, in the zero-shot setting
RP,The field of Continual Learning (CL) seeks to develop algorithms that accumulate knowledge and skills over time through interaction with non-stationary environments
RP, We focus on the first-order setting, which is arguably the most relevant for variational inference, and show that the biases added by the generalized Gauß-Newton approximation, which is applied by VOGN, can seriously affect the quality of the learned approximation
RP, This work explores a new paradigm for continual learning -- learning to dynamically prompt the model to learn tasks sequentially under different task transitions
RP,Motivated by recent advances in both theoretical and applied aspects of multiplayer games, spanning from e-sports to multi-agent generative adversarial networks, we focus on min-max optimization in team zero-sum games
RP,Learning informative representations from image-based observations is a fundamental problem in deep reinforcement learning (RL)
RP,One-shot Neural Architecture Search (NAS) usually constructs an over-parameterized network, which we call a supernet, and typically adopts sharing parameters among the sub-models to improve computational efficiency
RP,Self-supervised learning (SSL) is a scalable way to learn general visual representations since it learns without labels
RP,Regression that predicts continuous quantity is a central part of applications using computational imaging and computer vision technologies
RP, In this paper, we aim to better understand the behavior of LS and explore new algorithms for more effective LS on improving adversarial robustness
RP,Single Domain generalization (SDG) is a challenging scenario of Domain generalization, where only one source domain is available to train the model
RP,Coordination graph is a promising approach to model agent collaboration in multi-agent reinforcement learning
RP,We introduce reducible held-out loss selection (RHOLS), a technique for faster model training which selects a sequence of training points that are “just right”
RP,While recent automated data augmentation works lead to state-of-the-art results, their design spaces and the derived data augmentation strategies still incorporate strong human priors
RP,The recent success of deep learning has encouraged many researchers to explore the deep/concatenated variants of classical kernel methods
RP,   This paper studies what the observed structure of embeddings can tell us about the natural processes that generate new knowledge or concepts
RP,The lifelong learning paradigm in machine learning is an attractive alternative to the more prominent isolated learning scheme not only due to its resemblance to biological learning, but also its potential to reduce energy waste by obviating excessive model re-training
RP,Batch RL has seen a surge in popularity and is applicable in many practical scenarios where past data is available
RP,Generative model based approaches have led to significant advances in zero-shot learning (ZSL) over the past few-years
RP, Disentangled and invariant representation are two vital goals for representation learning and many approaches have been proposed to achieve one of them
RP,Multi-label classification tasks such as OCR and multi-object recognition are a major focus of the growing machine learning as a service industry
RP,Graph convolutional networks (GCNs), which propagate the node features through the edges and learn how to transform the aggregated features under label supervision, have achieved great success in supervised feature extraction for both graph-level and node-level classification tasks
RP,Mixture-of-experts (MoE) is becoming popular due to its success in improving the model quality, especially in Transformers
RP,Online learning via Bayes' theorem allows new data to be continuously integrated into an agent's current beliefs
RP,Several recent works questioned the value of inheriting weight in structured neural network pruning because they empirically found training from scratch can match or even outperform finetuning a pruned model
RP,Recent advances in quantized neural networks (QNNs) are closing the performance gap with the full precision neural networks
RP,Supervised contrastive learning optimizes a loss that pushes together embeddings of points from the same class while pulling apart embeddings of points from different classes
RP, These studies test sudden changes in the robot's mass and inertia, and they evaluate in an environment (PyBullet) whose dynamics differs from training (NimblePhysics)
RP,Semi-Discrete Optimal Transport (SDOT) transforms a continuous distribution to a discrete distribution
RP,Real-world machine learning deployments are characterized by mismatches between the source (training) and target (test) distributions that may cause performance drops
RP,Non-stationary casual structures are prevalent in real-world physical systems
RP, We consider ILO in the setting where the expert and the learner agents operate in different environments, with the source of the discrepancy being the transition dynamics model
RP,An efficient way for a deep reinforcement learning agent to explore can be to learn a set of skills that achieves a uniform distribution of terminal states
RP, We believe this is not yet the case for information retrieval, because these pre-training methods are not well adapted to this task
RP, We find this to be effective in ablation study comparing SEAL to the static version (§4) where energy function is fixed after pretraining
RP,Many exploration strategies are built upon the optimism in the face of the uncertainty (OFU) principle for reinforcement learning
RP, We study a linear teacher-student setup exhibiting epoch-wise double descent similar to that in deep neural networks
RP,Continual learning is the ability to learn from new experiences without forgetting previous experiences
RP,End-to-end (geometric) deep learning has seen first successes in approximating the solution of combinatorial optimization problems
RP,Efficient exploration is important for reinforcement learners (RL) to achieve high rewards
RP, This regularizer is motivated by a recent guarantee bound of the generalization error
RP,We study the problem of cross-domain lossy compression where the reconstruction distribution is different from the source distribution in order to account for distributional shift due to processing
RP,Forgetting is often seen as an unwanted characteristic in both human and machine learning
RP,Humans understand a set of canonical geometric transformations (such as translation, rotation and scaling) that support generalization by being untethered to any specific object
RP,Hierarchical forecasting is a key problem in many practical multivariate forecasting applications - the goal is to simultaneously predict a large number of correlated time series that are arranged in a pre-specified aggregation hierarchy
RP,Existing abstractive summarization models lack explicit control mechanisms that would allow users to influence the stylistic features of the model outputs
RP,We are concerned with the problem of distributional prediction with incomplete features: The goal is to estimate the distribution of target variables given feature vectors with some of the elements missing
RP, We study such biases and, in particular, show how transformer-based architectures can enable a more natural implementation of missingness, which side-steps these issues and improves the reliability of model debugging in practice
RP,Many animals and humans process the visual field with varying spatial resolution (foveated vision) and use peripheral processing to make eye movements and point the fovea to acquire high-resolution information about objects of interest
RP,Inverse reinforcement learning methods aim to retrieve the reward function of a Markov decision process based on a dataset of expert demonstrations
RP,Score-based generative models (SGMs) have demonstrated remarkable synthesis quality
RP,Models for image segmentation, node classification and many other tasks map a single input to multiple labels
RP,  A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long dependencies
RP,Learning strong representations for multi-modal retrieval is an important problem for many applications, such as recommendation and search
RP,In image generation, generative models can be evaluated naturally by visually inspecting model outputs
RP,We show how to derive state-of-the-art unsupervised neural machine translation systems from generatively pre-trained language models
RP,The problem of domain adaptation conventionally considers the setting where a source domain has plenty of labeled data, and a target domain (with a different data distribution) has plenty of unlabeled data but none or very limited labeled data
RP, We aim for TorchGeo to become a new standard for reproducibility and for driving progress at the intersection of deep learning and remotely sensed geospatial data
RP,Graph neural networks (GNNs) and their variants have demonstrated superior performance in learning graph representations by aggregating features based on graph or hypergraph structures
RP,The generalization of model-based reinforcement learning (MBRL) methods to environments with unseen dynamics is an important yet challenging problem
RP,Visual imitation learning is an effective approach for intelligent agents to obtain control policies from visual demonstration sequences
RP, This paper argues that for a transferable concept to be learned, the system of relations that define it must be coherent across domains
RP,We empirically show that the test error of deep networks can be estimated by training the same architecture on the same training set but with two different runs of Stochastic Gradient Descent (SGD), and then measuring the disagreement rate between the two networks on unlabeled test data
RP,Graph neural networks (GNN) have shown great advantages in many graph-based learning tasks but often fail to predict accurately for a task based on sets of nodes such as link/motif prediction and so on
RP,Structured pruning methods which are capable of delivering a densely pruned network are among the most popular techniques in the realm of neural network pruning, where most methods prune the original network at a filter or layer level
RP,Deep learning has yielded extraordinary results in vision and natural language processing, but this achievement comes at a cost
RP,Data augmentation is essential to achieve state-of-the-art performance in many deep learning applications
RP,We present a generic method for recurrently using the same parameters for many different convolution layers to build a deep network
RP, We consider the feed-forward network (both multi-layer perceptron and convolutional network) as a series of bipartite graphs which establish the connection from input to output
RP,The objective in goal-based reinforcement learning is to learn a policy to reach a particular goal state within the environment
RP,The interventional nature of recommendation has attracted increasing attention in recent years
RP,Personalized item-recommendation (ItemRec) and targeted item-marketing are two sides of the same coin
RP,ML prediction APIs from providers like Amazon and Google have made it simple to use ML in applications
RP,Learning from set-structured data is a fundamental problem that has recently attracted increasing attention, where a series of summary networks are introduced to deal with the set input
RP, This paper investigates what learning goals admit better sample complexities in the setting of m-player general-sum Markov games with H steps, S states, and A_i actions per player
RP, To study the viability of using inductive GNNs to solve the SCS problem, we introduce feature-contribution ratio (FCR), a metric to quantify the contribution of a node's features and that of its neighborhood in predicting node labels, and use this new metric as a model selection reward
RP,The problem of identifying algorithmic recourse for people affected by machine learning model decisions has received much attention recently
RP,Neuronal representations within artificial neural networks are commonly understood as logits, representing the log-odds score of presence (versus absence) of features within the stimulus
RP,Models of human behavior for prediction and collaboration tend to fall into two categories: ones that learn from large amounts of data via imitation learning, and ones that assume human behavior to be noisily-optimal for some reward function
RP,Trained Neural Networks (NNs) can be viewed as data-dependent kernel machines, with predictions determined by the inner product of last-layer representations across inputs, referred to as the feature kernel
RP,Reinforcement learning has recently shown promise in learning quality solutions in a number of combinatorial optimization problems
RP,Recent studies have shown that pre-trained classifiers are increasingly powerful to improve the performance on different tasks, e.g, neural language processing, image classification
RP,Reinforcement learning (RL)'s efficiency can drastically degrade on long-horizon tasks due to sparse rewards and the RL policy can be fragile to small changes in deployed environments
RP,Developing sequential models traditionally involves two stages - training and application
RP,The problem of detecting out-of-distribution (OOD) examples in neural networks has been widely studied in the literature, with state-of-the-art techniques being supervised in that they require fine-tuning on OOD data to achieve high-quality OOD detection
RP,Spurious correlations pose a fundamental challenge for building robust machine learning models
RP,An effective weighting scheme for training samples is essential for learning tasks
RP,Overparameterized transformer-based architectures have shown remarkable performance in recent years, achieving state-of-the-art results in speech processing tasks such as speech recognition, speech synthesis, keyword spotting, and speech enhancement et al, The main assumption is that with the underlying self-attention mechanism, transformers can ultimately capture the long-range temporal dependency from speech signals
RP, We tackle rigid body protein-protein docking, i,e, computationally predicting the 3D structure of a protein-protein complex from the individual unbound structures, assuming no three-dimensional flexibility during binding
RP,Combinatorial optimization problems with parameters to be predicted from side information are commonly seen in a variety of problems during the paradigm shift from reactive decision making to proactive decision making
RP,The Abstraction and Reasoning Corpus (ARC) is a set of procedural tasks that tests an agent's ability to flexibly solve novel problems
RP,There has been great interest in enhancing the robustness of neural network classifiers to defend against adversarial perturbations through adversarial training, while balancing the trade-off between robust accuracy and standard accuracy
RP,When finetuning a pretrained language model for natural language generation tasks, one is currently faced with a tradeoff
RP,Recent advances in machine learning have brought opportunities for the ever-increasing use of AI in the real world
RP,Biological spiking neural networks (SNNs) can temporally encode information in their outputs, e,g, in the rank order in which neurons fire, whereas artificial neural networks (ANNs) conventionally do not
RP,The efficiency of neural networks is very important in large-scale deployment scenarios such as mobile applications, internet of things, and edge computing
RP,Graph Neural Networks (GNNs) are playing a more and more important role for analyzing unstructured data from the complex real world
RP,In distributed training of deep neural networks or Federated Learning (FL), people usually run Stochastic Gradient Descent (SGD) or its variants on each machine and communicate with other machines periodically
RP,Multiple intriguing problems are hovering in adversarial training, including robust overfitting, robustness overestimation, and robustness-accuracy trade-off
RP,Reward hacking---where RL agents exploit gaps in misspecified proxy rewards---has been widely observed, but not yet systematically studied
RP,Despite being widely used, face recognition models suffer from bias: the probability of a false positive (incorrect face match) strongly depends on sensitive attributes such as the ethnicity of the face
RP, In this work, we consider a challenging problem of few-shot learning in image classification, especially when the target data distribution in the few-shot phase is different from the source, training, data distribution, in a sense that it includes new image classes not encountered during training
RP,An unaddressed challenge in zero-shot coordination is to take advantage of the semantic relationship between the features of an action and the features of observations
RP, This kind of overfitting tendency hinders matching structural similarity between images, causing an inconsistency in global visual information such as backgrounds
RP,To support local inference on an edge device, it is necessary to deploy a compact machine learning model on such a device
RP,Real economies can be seen as a sequential imperfect-information game with many heterogeneous, interacting strategic agents of various agent types, such as consumers, firms, and governments
RP,In this paper, we study the representation of neural networks from the view of kernels
RP,We study the problem of learning a good set of policies, so that when combined together, they can solve a wide variety of unseen reinforcement learning tasks with no or very little new data
RP,During pretraining, the Pre-LayerNorm transformer suffers from a gradient magnitude mismatch: gradients at early layers are much larger than at later layers, while the optimal weighting of residuals is larger at earlier than at later layers
RP,Co-speech gestures are a principal component in conveying messages and enhancing interaction experiences between humans
RP,Vision Transformer (ViT) is emerging as the state-of-the-art architecture for image recognition
RP, We argue in this paper that this property can be detrimental for tasks such as graph dictionary or partition learning, and we relax it by proposing a new semi-relaxed Gromov-Wasserstein divergence
RP, This divergence further leads to a dilemma: "Should we prioritize the learned model's generic performance (for future use at the server) or its personalized performance (for each client)?" These two, seemingly competing goals have divided the community to focus on one or the other, yet in this paper we show that it is possible to approach both at the same time
RP,deep reinforcement learning (RL) has proved to be a competitive heuristic for solving small-sized instances of traveling salesman problems (TSP), but its performance on larger-sized instances is insufficient
RP,deep reinforcement learning can generate complex control policies, but requires large amounts of training data to work effectively
RP,To accelerate the training of graph convolutional networks (GCN), many sampling-based methods have been developed for approximating the embedding aggregation
RP,Federated learning data is drawn from a distribution of distributions: clients are drawn from a meta-distribution, and their data are drawn from local data distributions
RP, We examine the difficulty in steering based on whether features musically follow a prime or not, using existing music as a proxy
RP, In this work, motivated by the promising results of sequence-to-sequence transfer learning for low-resource Natural Language Processing (NLP), we demonstrate that a general-purpose Transformer model can perform multi-task AMT, jointly transcribing arbitrary combinations of musical instruments across several transcription datasets
RP,Aiming to find a program satisfying the user intent given input-output examples, program synthesis has attracted increasing interest in the area of machine learning
RP,Recently, lots of algorithms have been proposed for learning a fair classifier from centralized data
RP,The release of tabular benchmarks, such as NAS-Bench-101 and NAS-Bench-201, has significantly lowered the computational overhead for conducting scientific research in neural architecture search (NAS)
RP,We introduce Contrastive Intrinsic Control (CIC) - an algorithm for unsupervised skill discovery that maximizes the mutual information between skills and state transitions
RP,Graph neural networks (GNNs) have achieved great success in graph representation learning, which has tremendously facilitated various real-world applications
RP,We propose a reinforcement learning based approach to the problem of query object localization, where an agent is trained to localize objects of interest specified by a small exemplary set
RP,Standard deep reinforcement learning (DRL) agents aim to maximize expected reward, considering collected experiences equally in formulating a policy
RP,Domain generalization aims to learn a predictive model from multiple different but related source tasks that can generalize well to a target task without the need of accessing any target data
RP,Numerous recent works utilize bi-Lipschitz regularization of neural network layers to preserve relative distances between data instances in the feature spaces of each layer
RP,Latent representations help unravel complex phenomena
RP,   We aim to disentangle interpretable latent factors "one at a time", or OAT factor learning, making no prior assumptions about the number or distribution of factors, in a completely unsupervised manner
RP,Graph Neural Networks (GNNs) have enabled the power of deep learning to be applied to inputs beyond the Euclidean domain, with applications ranging from social networks and product recommendation engines to the life sciences
RP,Lidars are widely used in applications such as autonomous driving and augmented reality
RP,We study the dynamics of a neural network in function space when optimizing the mean squared error via gradient flow
RP,We study the power-law asymptotics of learning curves for Gaussian process regression (GPR)
RP,The core of cross-modal retrieval is to measure the content similarity between data of different modalities
RP,Multi-label classification is a challenging structured prediction task in which a set of output class labels are predicted for each input
RP,Dimensionality reduction (DR) and visualization of high-dimensional data is of theoretical and practical value in machine learning and related fields
RP,Graph contrastive learning (GCL) is a newly popular paradigm for self-supervised graph representation learning and offers an alternative to reconstruction-based methods
RP, In this paper, we consider using Gradient Descent (GD) with a large learning rate on a homogeneous matrix factorization problem, i,e, \min_{X, Y \|A - XY^\top\|_{\sf F^2
RP, We instead consider the task of probabilistic robustness, which assumes the input follows a known probabilistic distribution and seeks to bound the probability of a given network failing against the input
RP,Causal discovery aims to learn a causal graph from observational data
RP,We introduce a memory-driven semi-parametric approach to text-to-image generation, which is based on both parametric and non-parametric techniques
RP,We introduce Explanatory Learning (EL), an explanation-driven machine learning framework to use existing knowledge buried in symbolic sequences expressed in an unknown language
RP,Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery
RP,While stochastic latent variable models (LVMs) now achieve state-of-the-art performance on natural image generation, they are still inferior to deterministic models on speech
RP, In this work, we aim to accelerate inference and training of coordinate-based MLPs for implicit neural representations by proposing a new split MLP architecture, CoordX
RP,Differentiable physics has recently been shown as a powerful tool for solving soft-body manipulation tasks
RP, In this paper, we tackle the challenge of jointly quantifying in-distribution and out-of-distribution (OOD) uncertainties
RP,Neural networks are traditionally represented in terms of their weights
RP,Few-shot learning (FSL) is the process of rapid generalization from abundant base samples to inadequate novel samples
RP,Graph neural networks achieve high accuracy in link prediction by jointly leveraging graph topology and node attributes
RP, In this work, we posit that learning the interdependence between actions is crucial for RL agents acting under a varying action set
RP,We present a framework for learning compositional, rational skill models (RatSkills) that support efficient planning and inverse planning for achieving novel goals and recognizing activities
RP,Generalisation to unseen contexts remains a challenge for embodied navigation agents
RP,Though image transformers have shown competitive results with convolutional neural networks in computer vision tasks, lacking inductive biases such as locality still poses problems in terms of model efficiency especially for embedded applications
RP,Many types of data are generated at least partly by discrete causes that are sparsely active
RP,The recent framework of performative prediction is aimed at capturing settings where predictions influence the target/outcome they try to predict
RP,We propose a novel prediction interval (PI) method for uncertainty quantification, which addresses three major issues with the state-of-the-art PI methods
RP,Despite rapid advances in continual learning, a large body of research is devoted to improving performance in the existing setups
RP,We study non-convex subgradient flows for training two-layer ReLU neural networks from a convex geometry and duality perspective
RP,We hypothesize that due to the greedy nature of learning in multi-modal deep neural networks (DNNs), these models tend to rely on just one modality while under-utilizing the other modalities
RP,There has been emerging interest in using transductive learning for adversarial robustness (Goldwasser et al, NeurIPS 2020; Wu et al, ICML 2020; Wang et al, ArXiv 2021)
RP,The gradient flow of a function over the space of probability densities with respect to the Wasserstein metric often exhibits nice properties and has been utilized in several machine learning applications
RP,The architecture and the parameters of neural networks are often optimized independently, which requires costly retraining of the parameters whenever the architecture is modified
RP,Owing to the benefits for customers (lower prices), drivers (higher revenues), aggregation companies (higher revenues) and the environment (fewer vehicles), on-demand ride pooling (e,g,, Uber pool, Grab Share) has become quite popular
RP,The success of neural networks (NNs) in a wide range of applications has led to increased interest in understanding the underlying learning dynamics of these models
RP,For many differentially private algorithms, such as the prominent noisy stochastic gradient descent (DP-SGD), the analysis needed to bound the privacy leakage of a single training run is well understood
RP,Safe deployment of trained ML models requires determining when input samples go out-of-distribution (OOD) and refraining from making uncertain predictions on them
RP,In many domains, including healthcare, biology, and climate science, time series are irregularly sampled with variable time between successive observations and different subsets of variables (sensors) are observed at different time points, even after alignment to start events
RP,Recently, Neural ODE (Ordinary Differential Equation) models have been proposed, which use ordinary differential equation solving to predict the output of neural network
RP,This paper studies zero-shot domain adaptation where each domain is indexed on a multi-dimensional array, and we only have data from a small subset of domains
RP,We study the controllability of large-scale networked dynamical systems when complete knowledge of network structure is unavailable
RP,A highly desirable property of a reinforcement learning (RL) agent -- and a major difficulty for deep RL approaches -- is the ability to generalize policies learned on a few tasks over a high-dimensional observation space to similar tasks not seen during training
RP,Goal-Conditioned continuous control tasks remain challenging due to the sparse reward signals
RP, Our paper explores the potential for transferring between these two representations by empirically investigating the feasibility of the transfer, the benefits of the transfer, and shedding light on why the transfer works
RP, We explore the hypothesis that these intermediate layers  might be directly exploited by linear probing
RP,To localize an object referent, humans attend to different locations in the scene and visual cues depending on the utterance
RP,Playing text-based games requires language understanding and sequential decision making
RP,In this paper, we question the rationale behind propagating large numbers of parameters through a distributed system during federated learning
RP,Pre-trained language models (PLMs) aim to learn universal language representations by conducting self-supervised training tasks on large-scale corpora
RP,Current Reinforcement Learning algorithms have reached new heights in performance
RP,We present LSeg, a novel model for language-driven semantic image segmentation
RP,Dequantisation is a general technique used for transforming data described by a discrete random variable x into a continuous (latent) random variable z, for the purpose of it being modeled by likelihood-based density models
RP,Animating images has become increasingly realistic, as well as efficient due to the remarkable progress of Generative Adversarial Networks (GANs) and auto-encoder
RP,Searching for novel molecular compounds with desired properties is an important problem in drug discovery
RP, In this paper, we explore the power of a “heavy edge” oracle in multiple graph edge streaming models
RP,Many practical modeling tasks require making predictions using tabular data composed of heterogeneous feature types (e,g,, text-based, categorical, continuous, etc)
RP,Recent works have shown that adversarial examples can improve the performance of representation learning tasks
RP, This example highlights how the same input image can be posed as two separate binary classification problems
RP,The method of Random Fourier Feature (RFF) has been popular for large-scale learning, which generates non-linear random features of the data
RP,Differentiable Neural Architecture Search (NAS) has emerged as a simple and efficient method for the automated design of neural networks
RP,Discovering causal structures from data is a challenging inference problem of fundamental importance in all areas of science
RP,Neural network pruning is a fruitful area of research with surging interest in high sparsity regimes
RP,We study the stochastic shortest path (SSP) problem in reinforcement learning with linear function approximation, where the transition kernel is represented as a linear mixture of unknown models
RP,Predictive uncertainties can be characterized by two properties---calibration and sharpness
RP, Our work aims at shedding new light on the task by looking at it through the prism of two familiar and related frameworks: text-to-image and image-to-image retrieval
RP,We present a meta-learning framework for learning new visual concepts quickly, from just one or a few examples, guided by multiple naturally occurring data streams: simultaneously looking at images, reading sentences that describe the objects in the scene, and interpreting supplemental sentences that relate the novel concept with other concepts
RP,In open-set recognition (OSR), classifiers should be able to reject unknown-class samples while maintaining robust closed-set classification performance
RP,This paper proposes a new and simple way of training sparse neural networks
RP,Most segmentation losses are arguably variants of the Cross-Entropy (CE) or Dice losses
RP,Graph neural networks (GNNs) have been used extensively for addressing problems in drug design and discovery
RP, In this paper, we study a simple adversarial augmentation method that can modify training data to be hard positives/negatives without distorting the key information about their original identities
RP, In this work, we argue the problems of existing models can be traced down to the two sub-tasks of the face restoration problem, i,e, face generation and face reconstruction, and the fragile balance between them
RP,Neural Processes (NPs) are a class of stochastic processes parametrized by neural networks
RP, In this paper, we explored the possibility of using contrastive methods to learn a disentangled representation, a discriminative approach that is drastically different from previous approaches
RP,Conventional supervised learning typically assumes that the learning task can be solved by learning a single function since the data is sampled from a fixed distribution
RP,This paper presents a framework for analyzing the expressiveness and learning of relational models applied to hypergraph reasoning tasks
RP,We present a memory-efficient neural ODE framework PNODE based on high-level adjoint algorithmic differentiation
RP, In this work, we make the attempt to analyze the over-smoothing problem from the perspective of graph, where such problem was first discovered and explored
RP,A recent emerging trend in the literature on learning in games has been concerned with providing accelerated learning dynamics for correlated and coarse correlated equilibria in normal-form games
RP,We introduce Self-GenomeNet, a novel contrastive self-supervised learning method for nucleotide-level genomic data, which substantially improves the quality of the learned representations and performance compared to the current state-of-the-art deep learning frameworks
RP,Pre-trained contextual language models are ubiquitously employed for language understanding tasks, but are unsuitable for resource-constrained systems
RP,Musical expression requires control of both what notes that are played, and how they are performed
RP,Current dense text retrieval models face two typical challenges
RP,Generalization error bounds measure the deviation of performance on unseen test data from performance on training data
RP,Our world is full of asymmetries
RP,Most neural text-to-speech (TTS) models require \langlespeech, transcript\rangle paired data from the desired speaker for high-quality speech synthesis, which limits the usage of large amounts of untranscribed data for training
RP, We discuss how these approaches relate to each other and baseline strategies under various assumptions on the dataset
RP,The problem of processing very long time-series data (e,g,, a length of more than 10,000) is a long-standing research problem in machine learning
RP,Data augmentation is becoming essential for improving regression accuracy in critical applications including manufacturing and finance
RP,Recovering sparse parameters from observational data is a fundamental problem in machine learning with wide applications
RP,In this paper, we investigate the performance of two first-order optimization algorithms, obtained from forward Euler discretization of finite-time optimization flows
RP,Online 3D Bin Packing Problem (3D-BPP) has widespread applications in industrial automation and has aroused enthusiastic research interest recently
RP, The focus of this paper is on directed graphs
RP,Self-attention (SA) is a critical component of Transformer neural networks that have succeeded in automatic speech recognition (ASR)
RP, In this paper, we argue that query efficiency in the zeroth-order setting is connected to an adversary's traversal through the data manifold
RP,Deep neural networks are usually initialized with random weights, with adequately selected initial variance to ensure stable signal propagation during training
RP,Certifiably robust neural networks employ provable run-time defenses against adversarial examples by checking if the model is locally robust at the input under evaluation
RP,Machine learning systems often experience a distribution shift between training and testing
RP,To be viable for safety-critical applications, such as autonomous driving and assistive robotics, autonomous agents should adhere to safety constraints throughout the interactions with their environments
RP, We study the trade-off between fairness and robustness, by analyzing the adversarial (worst-case) bias against group fairness in machine learning and by comparing it with the effect of similar adversarial manipulations on regular models
RP, This work stands on a different view; establishing a mathematical connection between graph convolution and graph-regularized PCA (GPCA)
RP,We introduce Latent Temporal Flows (LatTe-Flows), a method for probabilistic multivariate time-series analysis tailored for high dimensional systems whose temporal dynamics are driven by variations in a lower-dimensional discriminative subspace
RP, This paper aims to improve their performance further by utilizing the architectural advantages of the underlying neural network, as the current state-of-the-art visual pretext tasks for self-supervised learning do not enjoy the benefit, i,e, they are architecture-agnostic
RP,In recent years, large pre-trained Transformer networks have demonstrated dramatic improvements in many Natural Language Processing (NLP) tasks
RP,The ability to extract entities and their relations from unstructured text is essential for automated maintenance of large-scale knowledge graphs
RP,Time series are common in a wide range of domains and tasks such as stock market partitioning, sleep stage labelling, and human activity recognition, where segmentation, i,e, splitting time series into segments that correspond to given categories, is often required
RP,Neighbor sampling is a commonly used technique for training Graph Neural Networks (GNNs) on large graphs
RP,Specifying reward functions for complex tasks like object manipulation or driving is challenging to do by hand
RP,Learning fine-grained embeddings is essential for extending the generalizability of models pretrained on "coarsely" annotated labels (e,g,, animals)
RP,A recent branch of the black-box adversarial attack literature has recognised the benefits of incorporating both the surrogate gradient information usually associated with transfer-based methods and the flexible query-based numerical optimisation typical of score-based methods
RP,The effective control of microscopic collectives has many promising applications, from environmental remediation to targeted drug delivery
RP,In this paper, we argue that energy-based sequence models backed by expressive parametric families can result in uncomputable and inapproximable partition functions
RP, This task is difficult because stable materials only exist in a low-dimensional subspace of all possible periodic arrangements of atoms: 1) the coordinates must lie in the local energy minimum defined by quantum mechanics, and 2) global stability also requires the structure to follow the complex, yet specific bonding preferences between different atom types
RP,Low-precision optimization is widely used to accelerate large-scale deep learning
RP, Our goals are to obtain a compositional scene representation and to perform scene generation by modeling statistical relationships between scenes as well as between objects within a scene
RP,In this paper, we conjecture that if the permutation invariance of neural networks is taken into account, SGD solutions will likely have no barrier in the linear interpolation between them
RP,Visual search, recommendation, and contrastive similarity learning power a wide breadth of technologies that impact billions of users across the world
RP,Several recent works empirically found finetuning learning rate is crucial to the final performance in structured neural network pruning
RP,Sketching is a dimensionality reduction technique where one compresses a matrix by linear combinations that are typically chosen at random
RP,A dynamical system of spiking neurons with only feedforward connections can classify spatiotemporal patterns without recurrent connections
RP,Active Learning (AL) has the potential to reduce labeling cost when training natural language processing models, but its effectiveness with the large pretrained transformer language models that power today's NLP is uncertain
RP,Fairness is becoming a rising concern in machine learning
RP, We are motivated by the fact that the vulnerable memory locations are very rare, device-specific, and sparsely distributed
RP,Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn
RP,We study how different output layers in a  deep neural network learn and forget in continual learning settings
RP, This discrepancy presents a major challenge when we attempt to take RL algorithms developed for episodic simulated environments and run  them on real-world platforms, such as robots
RP,We propose StyleNeRF, a 3D-aware generative model for photo-realistic high-resolution image synthesis with high multi-view  consistency, which can be trained on unstructured 2D images
RP,Large pre-trained language models have been used to generate code, providing a flexible interface for synthesizing programs from natural language specifications
RP,Deep metric learning (DML) enables learning with less supervision through its emphasis on the similarity structure of representations
RP, In this paper, we aim to provide sufficient conditions for this phenomenon considering different factors that could affect both, such as the norm of last layer norm, Jacobian norm, and data augmentations (DA)
RP,Here, we show that the robust overfitting shall be viewed as the early part of an epoch-wise double descent --- the robust test error will start to decrease again after training the model for a considerable number of epochs
RP, In this paper, we aim to unravel the non-trivial relationships between the probability distribution of the data, perceptual distances, and unsupervised machine learning
RP,Perceiving and manipulating 3D articulated objects (e,g,, cabinets, doors) in human environments is an important yet challenging task for future home-assistant robots
RP,Despite the impressive performance of deep networks in vision, language, and healthcare, unpredictable behaviors on samples from the distribution different than the training distribution cause severe problems in deployment
RP,Machine learning systems deployed in the wild are often trained on a source distribution that differs from the target distribution on which it is deployed
RP,  We argue that such structured output, grounded in both modalities, is a clear step towards the high-level understanding of multimodal information
RP,In distributed settings, collaborations between different entities, such as financial institutions, medical centers, and retail markets, are crucial to providing improved service and performance
RP,We consider the problem of goal-directed planning under a deterministic transition model
RP,Denoising diffusion probabilistic models have recently received much research attention since they outperform alternative approaches, such as GANs, and currently provide state-of-the-art generative performance
RP,Estimating the discrepancy between two densities (p and q) is central to machine learning
RP,A core challenge in the interpretation of deep neural networks is identifying commonalities between the underlying algorithms implemented by distinct networks trained for the same task
RP,We introduce environment predictive coding, a self-supervised approach to learn environment-level representations for embodied agents
RP,Supervised federated learning (FL) enables multiple clients to share the trained model without sharing their labeled data
RP,The empirical success of deep learning is often attributed to SGD’s mysterious ability to avoid sharp local minima in the loss landscape, as sharp minima are  known to lead to poor generalization
RP,We study efficient algorithms for reinforcement learning in Markov decision processes, whose complexity is independent of the number of states
RP,In this work we perform a systematic study of various feature representations for few-shot classification, including representations learned from MAML, supervised classification, and several common self-supervised tasks
RP,Offline reinforcement learning (RL) algorithms can acquire effective policies by utilizing only previously collected experience, without any online interaction
RP,Domain generalization deals with the difference in the distribution between the training and testing datasets, i,e, the domain shift problem, by extracting domain-invariant features
RP,The magnitude of a finite metric space is a recently-introduced invariant quantity
RP,  Generalization in Reinforcement Learning (RL) is usually measured according to  concepts from supervised learning
RP,Robust multi-agent trajectory prediction is essential for the safe control of robotic systems
RP,In this paper, we consider a new multi-armed bandit (MAB) framework motivated by three common complications in online recommender systems in practice: (i) the platform (learning agent) cannot sample an intended product directly and has to incentivize customers to select this product (e,g,, promotions and coupons); (ii) customer feedbacks are often received later than their selection times; and (iii) customer preferences among products are influenced and reinforced by historical feedbacks
RP,Enhancing the user experience is an essential task for application service providers
RP,Symbolic reasoning, rule-based symbol manipulation, is a hallmark of human intelligence
RP,Recent self-supervised contrastive learning methods greatly benefit from the Siamese structure that aims at minimizing distances between positive pairs
RP, In this paper, we focus on a specific category of logical reasoning, named mytask, and propose a new large scale benchmark, named mydata, targeted for learning and evaluating models' capabilities towards mytask
RP,Adversarial examples pose a unique challenge for deep learning systems
RP,Multi-head attention is a driving force behind state-of-the-art transformers which achieve remarkable performance across a variety of natural language processing (NLP) and computer vision tasks
RP, Here, we address the task of online continual learning with noisy labels, which is a more realistic, practical, and challenging continual learning setup by assuming ground-truth labels may be noisy
RP, Our work sheds light on the possible role of recurrent connectivity in early vision as well as the roles of fixational drift and temporal-frequency selective cells in the visual system
RP,We reveal an intriguing connection between adversarial attacks and cycle monotone maps, also known as optimal transport maps
RP,Federated Learning (FL) has emerged as the tool of choice for training deep models over heterogeneous and decentralized datasets
RP,Energy-based models (EBMs) are generative models that are usually trained via maximum likelihood estimation
RP,For real-time forecasting in domains like public health and macroeconomics, data collection is a non-trivial and demanding task
RP,In the present work, we propose a federated learning protocol with bi-directional security guarantees
RP, Next, we discuss why SAM can be helpful in the noisy label setting where we first show that it can help to improve generalization even for linear classifiers
RP,A commonly held belief in deep-learning based long-tailed classiﬁcation is that the representations learned from long-tailed data are ”good enough” and the performance bottleneck is the classiﬁcation head atop the representation learner
RP,As deep learning becomes computationally intensive, the data parallelism is an essential option for the efficient training of high-performance models
RP,    In this work, beyond empirical observations, we aim to: (1) analyze the heterophily and oversmoothing problems from a unified theoretical perspective, (2) identify the common causes of the two problems based on our theories, and (3) propose simple yet effective strategies to address the common causes
RP,While deep neural networks for classification have shown impressive predictive performance, e,g, in image classification, they generally tend to be overconfident
RP,Recent research has revealed that the security of deep neural networks that directly process 3D point clouds to classify objects can be threatened by adversarial samples
RP, We focus on a critical bottleneck, namely the performance of planning and navigation
RP,Few shot learning is an important problem in machine learning as large labelled datasets take considerable time and effort to assemble
RP,Despite the increasing scale of datasets in machine learning, generalization to unseen regions of the data distribution remains crucial
RP,Natural language modeling with limited training data is challenging problem, and many algorithms make use of large-scale pretrained language models (PLMs) for this due to its great generalization ability
RP,Many causal and policy effects of interest are defined by linear functionals of high-dimensional or non-parametric regression functions
RP,We study the problem of learning dynamics that can produce hierarchically organized continuous outputs consisting of the flexible chaining of re-usable motor ‘motifs’ from which complex behavior is generated
RP,While ML tools are becoming increasingly used in industrial applications, adversarial examples remain a critical flaw of neural networks
RP,Quantile regression has a natural extension to generative modelling by leveraging a stronger convergence in pointwise rather than in distribution
RP,Graph neural networks (GNNs) have been increasingly deployed in various applications that involve learning on non-Euclidean data
RP, In this work, we focus on the change in representations of observed data that arises when previously unobserved classes appear in the incoming data stream, and new classes must be distinguished from previous ones
RP,Recently, Vision Transformer (ViT) has continuously established new milestones in the computer vision field, while the high computation and memory cost makes its propagation in industrial production difficult
RP, We investigate whether pretrained models are better active learners, capable of asking for example labels that disambiguate between the possible tasks a user may be trying to specify
RP,We study how the choice of visual perspective affects learning and generalization in the context of physical manipulation from raw sensor observations
RP,With the rapid advancement in deep generative models, recent neural text-to-speech models have succeeded in synthesizing human-like speech, even in an end-to-end manner
RP,By introducing randomness on environment parameters that fundamentally affect the dynamics, domain randomization (DR) imposes diversity to the policy trained by deep reinforcement learning, and thus improves its capability of generalization
RP, We believe this framework can be used for many downstream applications such as guided BCI training in the future
RP,In reinforcement learning (RL) the use of simulators is ubiquitous, allowing cheaper and safer agent training than training directly in the real target environment
RP,Meta-learning enables algorithms to quickly learn a newly encountered task with just a few labeled examples by transferring previously learned knowledge
RP,Despite the tremendous empirical success of deep learning models to solve various learning tasks, our theoretical understanding of their generalization ability is very limited
RP,Maximum likelihood estimation (MLE) is the predominant algorithm for training text generation models
RP,Neural networks have recently been used to model the dynamics of diverse physical systems
RP,The ability to identify whether or not a test sample belongs to one of the semantic classes in a classifier's training set is critical to practical deployment of the model
RP,We study the problem of aligning the supports of distributions
RP, We hope this novel theoretic perspective sheds light on the further improvements in Transformer-based language representation models
RP,We propose a framework to continuously learn object-centric representations for visual learning and understanding
RP,Mixup, a convex interpolation technique for data augmentation, has achieved great success in deep neural networks
RP, Next, we consider the property of ``scale-freeness'' enjoyed by AdamW and by its proximal counterpart: their updates are invariant to component-wise rescaling of the gradients
RP,Are all bits useful? In this work, we propose SimpleBits, a method to synthesize simplified inputs by reducing information content, and carefully measure the effect of such simplification on learning
RP, This work considers a server that hosts a labeled dataset and wishes to leverage clients with unlabeled data for supervised learning
RP,Many modern machine learning algorithms such as generative adversarial networks (GANs) and adversarial training can be formulated as minimax optimization
RP,Equivariance is becoming an increasingly popular design choice to build data efficient neural networks by exploiting prior knowledge about the symmetries of the problem at hand
RP,Encoding known symmetries into world models can improve generalization
RP,We investigate the possibility of using the embeddings produced by a lightweight network more effectively with a nonlinear classification layer
RP,When extrinsic rewards are sparse, artificial agents struggle to explore an environment
RP, In this work, we argue that we need to enable system designers to estimate the shifts an RS would induce; evaluate, before deployment, whether the shifts are undesirable; and even actively optimize to avoid such shifts
RP,Analyzing the worst-case performance of deep neural networks against input perturbations amounts to solving a large-scale non-convex optimization problem, for which several past works have proposed convex relaxations as a promising alternative
RP,A recent line of ground-breaking results for permutation-based SGD  has corroborated a widely observed phenomenon: random permutations offer faster convergence than with-replacement sampling
RP,Deep Neural Networks (DNNs), either pre-trained (e,g,, GAN generator) or untrained (e,g,, deep image prior), could act as overparameterized image priors that help solve various image inverse problems
RP,State-of-the-art neural network verifiers are fundamentally based on one of two paradigms: encoding the whole problem via tight multi-neuron convex relaxations or applying a Branch-and-Bound (BaB) procedure which leverages imprecise but fast bounds on a large number of easier subproblems
RP,Many text generation systems benefit from using a retriever to retrieve passages from a textual knowledge corpus (e,g,, Wikipedia) which are then provided as additional context to the generator
RP,deep reinforcement learning (RL) has achieved remarkable success in known environments where the agents are trained, yet the agents do not necessarily know what they don’t know
RP,Discovery and learning of an underlying spatiotemporal hierarchy in sequential data is an important topic for machine learning
RP,In this paper, we introduce the online and streaming MAP inference and learning problems for Non-symmetric Determinantal Point Processes (NDPPs) where data points arrive in an arbitrary order and the algorithms are constrained to use a single-pass over the data as well as sub-linear memory
RP,out-of-distribution (OOD) detection is the task of determining whether an input lies outside the training data distribution
RP,Modern deep reinforcement learning (DRL) methods allow simulated characters to learn complex skills such as locomotion from scratch
RP, In this paper, we examine the inductive biases that make physics-inspired models successful in practice
RP, This is in part due to the difficulties involved in prototyping new computational paradigms with existing frameworks
RP,While theoretically appealing, the application of the Wasserstein distance to large-scale machine learning problems has been hampered by its prohibitive computational cost
RP,Solving real-life sequential decision making problems under partial observability involves an exploration-exploitation problem
RP, This correspondence gives interpretability to the resulting model based on linear control theory
RP,Graph metric learning methods aim to learn the distance metric over graphs such that similar graphs are closer and dissimilar graphs are farther apart
RP,In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions
RP,Lasso is a celebrated method for variable selection in linear models, but it faces challenges when the covariates are moderately or strongly correlated
RP,Detecting out-of-distribution (OOD) inputs is a central challenge for safely deploying machine learning models in the real world
RP,In this work we propose a HyperTransformer, a transformer based model that generates all weights of a CNN model directly from the support samples
RP,Large pre-trained language models perform remarkably well on tasks that can be done "in one pass", such as generating realistic text or synthesizing computer programs
RP,Adversarial training is widely believed to be a reliable approach to improve model robustness against adversarial attack
RP,Definitions of the distance between two machine learning models either characterize the similarity of the models' predictions or of their weights
RP,Connectionist Temporal Classification (CTC) loss is commonly used in sequence learning applications
RP,Image clustering methods have rapidly improved their ability to discover object categories
RP,Transfer in Reinforcement Learning aims to improve learning performance on target tasks using knowledge from experienced source tasks
RP,Sequence labeling task (part-of-speech tagging, named entity recognition) is one of the most common in NLP
RP,The advancement of dynamics models enables model-based planning in complex environments
RP,This paper demonstrates how time-constrained multi-robot task allocation (MRTA) problems can be modeled as a Markov Decision Process (MDP) over graphs, such that approximate solutions can be modeled as a policy using Reinforcement Learning (RL) methods
RP,Many problems such as node classification and link prediction in network data can be solved using graph embeddings, and a number of algorithms are known for constructing such embeddings
RP, This conclusion is based on a careful study of the behaviour of infinite width networks trained by Bayesian inference and finite width networks trained by gradient descent
RP, In this paper, we point out that computation distribution and scale augmentation are the keys to detecting small faces from low-resolution images
RP,We introduce DictFormer with efficient shared dictionary to provide a compact, fast, and accurate transformer model
RP,We present the ﬁrst framework of Certifying Robust Policies for reinforcement learning (CROP) against adversarial state perturbations
RP,Despite the recent success of Graph Neural Networks (GNNs), training GNNs on large graphs remains challenging
RP,Nonlinear ICA is a fundamental problem in machine learning, aiming to identify the underlying independent components (sources) from data which is assumed to be a nonlinear function (mixing function) of these sources
RP,Learning rate schedulers have been widely adopted in training deep neural networks
RP,Noise-contrastive estimation (NCE) is a statistically consistent method for learning unnormalized probabilistic models
RP,The prevalence of graph structures has attracted a surge of research interest in graph data
RP,A major bottleneck for applying deep reinforcement learning to real-world problems is its sample inefficiency, particularly when training policies from high-dimensional inputs such as images
RP,Current causal discovery methods either fail to scale, model only limited forms of functional relationships, or cannot handle missing values
RP, We argue that the core reason for such security and privacy issues is the naive exchange of high-dimensional model parameters in federated learning algorithms
RP,Cross-entropy loss and focal loss are the most common choices when training deep neural networks for classification problems
RP, Through extensive experiments, this paper studies the importance of these design decisions
RP,Data quality is a common problem in machine learning, especially in high-stakes settings such as healthcare
RP, In this paper, we strive to broaden this understanding by showing that alternative non-Fourier embedding functions can indeed be used for positional encoding
RP,Be it in natural language generation or in the image generation, massive performances gains have been achieved in the last years
RP, We tackle the problem of learning regexes faster from positive and negative strings by relying on a novel approach called `neural example splitting'
RP,The rapid spread of the COVID-19 pandemic has resulted in an unprecedented amount of sequence data of the SARS-CoV-2 viral genome --- millions of sequences and counting
RP, This finding casts doubt on the utility of these approaches, in the hands of a practitioner, for detecting a model’s reliance on spurious signals
RP,Neural data compression based on nonlinear transform coding has made great progress over the last few years, mainly due to improvements in prior models, quantization methods and nonlinear transforms
RP,Model-agnostic meta-learning (MAML) is arguably one of the most popular meta-learning algorithms nowadays
RP,Reasoning about visual relationships is central to how humans interpret the visual world
RP,This paper explores a simple method for improving the zero-shot learning abilities of language models
RP,Offline reinforcement learning, which seeks to utilize offline/historical data to optimize sequential decision-making strategies, has gained surging prominence in recent studies
RP,Pretraining Neural Language Models (NLMs) over a large corpus involves chunking the text into training examples, which are contiguous text segments of sizes processable by the neural architecture
RP,Recently, large-scale Contrastive Language-Image Pre-training (CLIP) has attracted unprecedented attention for its impressive zero-shot recognition ability and excellent transferability to downstream tasks
RP, We hope that this will drive the improvement in design of algorithms that can account for the intricacies in the dataset and thereby, enable a step forward in direction of effective learning in real-world settings
RP,When a dynamical system can be modeled as a sequence of observations, Granger causality is a powerful approach for detecting predictive interactions between its variables
RP,We investigate the robustness of vision transformers (ViTs) through the lens of their special patch-based architectural structure, i,e, they process an image as a sequence of image patches
RP,Artificial neural networks coupled with learning-based methods have enabled robots to tackle increasingly complex tasks, but often at the expense of requiring large amounts of learning experience
RP,This paper investigates continual learning in the setting of class-incremental learning (CIL)
RP,In this paper, we develop a new approach to conformal prediction in which we aim to output a precise set of promising prediction candidates that is guaranteed to contain a limited number of incorrect answers
RP,In this paper, we propose a f-divergence Thermodynamic Variational Objective (f-TVO)
RP, We find this observation extends to practical settings; when computed over an off-policy dataset, the Bellman error bears little relationship to the accuracy of the value function
RP,We investigate the challenge of modeling the belief state of a partially observable Markov system, given sample-access to its dynamics model
RP, In this work, we observe that the poor performance is due to a gradient conflict issue: the gradients of different sub-networks conflict with that of the supernet more severely in ViTs than CNNs, which leads to early saturation in training and inferior convergence
RP,Transformer-based models have achieved great success in various NLP, vision, and speech tasks
RP, This problem is even more pronounced in FL, compared to centralized training, due to the fact that FL users are often reluctant to label their private data and edge devices do not provide an ideal interface to assist with annotation
RP, We argue that these capabilities suit the central role of a Meta-Reinforcement Learning algorithm
RP, We call this exploration bonus discriminator disagreement intrinsic reward, or DISDAIN
RP,The deep learning models' sensitivity to small input perturbations raises security concerns and limits their use for applications where reliability is critical
RP,In Federated learning (FL), participating clients typically possess non-i,i,d, data, posing a significant challenge to generalization to unseen distributions
RP,Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars
RP,Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and attempts at straightforwardly applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead
RP,Recent methods for self-supervised learning can be grouped into two paradigms: contrastive and non-contrastive approaches
RP, Our work is expected to pave a way for inspiring new research directions of sparse network training in the future
RP, In this setting, the agent aims to learn a set of near-optimal goal-conditioned policies to reach the L-controllable states: states that are incrementally reachable from an initial state s_0 within L steps in expectation
RP,Recently, contrastive learning has risen to be a promising approach for large-scale self-supervised learning
RP, Here, we study more human-like RL agents which incorporate an established model of human-irrationality, the Rational Inattention (RI) model
RP,We introduce a new constrained optimization method for policy gradient reinforcement learning, which uses two trust regions to regulate each policy update
RP,In this paper, we study the automatic hypothesis generation (HG) problem, focusing on explainability
RP,Meta-Imitation Learning is a promising technique for the robot to learn a new task from observing one or a few human demonstrations
RP,The convolution with orthogonal input-output Jacobian matrix, i,e,, orthogonal convolution,  has attracted substantial attention due to their excellent properties
RP,In state-of-the-art self-supervised learning (SSL) pre-training produces semantically good representations by encouraging them to be invariant under meaningful transformations prescribed from human knowledge
RP,We propose Compressed Vertical Federated Learning (C-VFL) for communication-efficient training on vertically partitioned data
RP, Our work seeks to reconcile these discrepant properties by investigating the origin of the block structure in relation to the data and training methods
RP,Smoothed particle hydrodynamics (SPH) is a mesh-free Lagrangian method for obtaining approximate numerical solutions of the equations of fluid dynamics, which has been widely applied to weakly- and strongly compressible turbulence in astrophysics and engineering applications
RP,Online class-incremental continual learning (CL) deals with the sequential task learning problem in a realistic non-stationary setting with a single-pass through of data
RP,Algorithmic bias is of increasing concern, both to the research community, and society at large
RP,Humans are expert explorers
RP,Equivariance has emerged as a desirable property of representations of objects subject to identity-preserving transformations that constitute a group, such as translations and rotations
RP,Learning to infer the conditional posterior model is a key step for robust meta-learning
RP,Language model (LM) pre-training has proven useful for a wide variety of language processing tasks, including tasks that require nontrivial planning and reasoning capabilities
RP,Federated learning (FL) provides a distributed learning framework for multiple participants to collaborate learning without sharing raw data
RP,While current methods for training robust deep learning models optimize robust accuracy, in practice, the resulting models are often both robust and inaccurate on numerous samples, providing a false sense of safety for those
RP,While combinatorial problems are of great academic and practical importance, previous approaches like explicit heuristics and reinforcement learning have been complex and costly
RP,Fine-tuning large pretrained language models on downstream tasks has become the de-facto learning paradigm in NLP
RP,Existing techniques for model inversion typically rely on hard-to-tune regularizers, such as total variation or feature regularization, which must be individually calibrated for each network in order to produce adequate images
RP, This implies that even achieving a population minimax optimal solution to the Wasserstein GAN objective is likely insufficient for distribution learning
RP,Off-policy Actor-Critic algorithms have demonstrated phenomenal experimental performance but still require better explanations
RP,We prove that minimizing a weighted mean results in optimizing the higher-order moments of the loss distribution such as the variance, skewness, and kurtosis
RP,In spite of the high performance and reliability of deep learning algorithms in broad range everyday applications, many investigations tend to show that a lot of models exhibit biases, discriminating against some subgroups of the population
RP,The prevalent approach in self-supervised image generation is to operate on pixel level representations
RP,Recent advances in large-scale language models (Raffel et al, 2019; Brown et al, 2020) have brought significant qualitative and quantitative improvements in machine-driven text generation
RP,Domain adaptation seeks to mitigate the shift between training on the source data and testing on the target data
RP,Value factorisation proves to be a useful technique in multi-agent reinforcement learning (MARL), but the underlying mechanism is not yet fully understood
RP,Adversarial transferability enables attackers to generate adversarial examples from the source model to attack the target model, which has raised security concerns about the deployment of DNNs in practice
RP,Federated averaging, the most popular aggregation approach in federated learning, is known to be vulnerable to failures and adversarial updates from clients that wish to disrupt training
RP,Advanced applications of modern machine learning will likely involve combinations of trained networks, as are already used in spectacular systems such as DeepMind's AlphaGo
RP,We present a differentiable approach to learn the probabilistic factors used for inference by a nonparametric belief propagation algorithm
RP,Understanding how neural dynamics give rise to behaviour is one of the most fundamental questions in systems neuroscience
RP,Neural architecture search  (NAS)  has demonstrated success in discovering promising architectures for vision or language modeling tasks, and it has recently been introduced to searching for graph neural networks  (GNNs)  as well
RP,In this paper, we revisit the problem of effectively using public data to improve the privacy/utility trade-offs for differentially private (DP) model training
RP,Text summarization aims to condense long documents and retain key information
RP,Translating source code from one programming language to another is a critical, time-consuming task in modernizing legacy applications and codebases
RP, In this paper, we investigate an intriguing question: Can we leverage algorithms that defend against noisy labels corruptions to defend against general backdoor attacks? We first discuss the limitations of directly using the noisy-label defense algorithms to defend against backdoor attacks
RP,Antimicrobial peptides (AMPs) have shown promising results in broad-spectrum antibiotics and resistant infection treatments, which makes it attract plenty of attention in drug discovery
RP,Communication is a key bottleneck in federated learning where a large number of edge devices collaboratively learn a model under the orchestration of a central server without sharing their own training data
RP, In this paper, we consider the problem of continuous control for various robot manipulation tasks with an explicit representation that promotes skill reuse while learning multiple tasks, related through the reward function
RP,In this paper, we present a novel method to learn a Bayesian neural network robust against adversarial attacks
RP,Minimax problems are receiving an increasing amount of attention in a wide range of applications in machine learning (ML), for instance, reinforcement learning, robust optimization, adversarial learning, and distributed computing, to mention but a few
RP,This work presents a probabilistic channel pruning method to accelerate Convolutional Neural Networks (CNNs)
RP,Adaptive gradient methods such as Adam have gained increasing popularity in deep learning optimization
RP,Conditional contrastive learning frameworks consider the conditional sampling procedure that constructs positive or negative data pairs conditioned on specific variables
RP,The architectures of convolution neural networks (CNN) have a great impact on the predictive performance and efficiency of the model
RP, This motivates us to reconsider whether homophily is truly necessary for good GNN performance
RP, We hope such a data-centric view can motivate researchers to rethink representation learning when investigating how to best apply RL to real-world tasks
RP,This work investigates the compatibility between label smoothing (LS) and knowledge distillation (KD)
RP,Automatic portrait video matting is an under-constrained problem
RP,Federated learning (FL) enables edge clients to train collaboratively while preserving individual's data privacy
RP,Reinforcement learning algorithms struggle on tasks with complex hierarchical dependency structures
RP,Multi-hop logical reasoning is an established problem in the field of representation learning on knowledge graphs (KGs)
RP,Traditional inverse reinforcement learning (IRL) methods require a loop to find the optimal policy for each reward update (called an inner loop), resulting in very time-consuming reward estimation
RP,Binary density ratio estimation (DRE), the problem of estimating the ratio p_1/p_2 given their empirical samples, provides the foundation for many state-of-the-art machine learning algorithms such as contrastive representation learning and covariate shift adaptation
RP,The use of deep neural networks has been highly successful in reinforcement learning and control, although few theoretical guarantees for deep learning exist for these problems
RP,Vision-based reinforcement learning requires efficient and robust representations of image-based observations, especially when the images contain distracting (task-irrelevant) elements such as shadows, clouds, and light
RP,Implicit shape models are promising 3D representations for modeling arbitrary locations, with Signed Distance Functions (SDFs) particularly suitable for clear mesh surface reconstruction
RP,Despite the success of modeling networked systems via graph neural networks (GNN), applying GNN for the model-based control is pessimistic since the non-convexity of GNN models hinders solving model-based control problems
RP, In this paper, we consider the training dynamics of gradient flow on kernel least-squares objectives, which is a limiting dynamics of SGD trained neural networks
RP,Graph Neural Networks (GNNs) have achieved great success in various tasks, but their performance highly relies on a large number of labeled nodes, which typically requires considerable human effort
RP,Reliable AI agents should be mindful of the limits of their knowledge and consult humans when sensing that they do not have sufficient knowledge to make sound decisions
RP,Vision transformers have delivered tremendous success in representation learning
RP,The implicit bias induced by the training of neural networks has become a topic of rigorous study
RP,State-of-the-art deep learning algorithms rely on distributed training to tackle the increasing model size and training data
RP,Deep generative models excel at generating complex, high-dimensional data, often exhibiting impressive generalization beyond the training distribution
RP,Federated learning allows collaborative workers to solve a machine learning problem while preserving data privacy
RP,Conditional generative adversarial networks (cGANs) have shown superior results in class-conditional generation tasks
RP,Complex sequential tasks in continuous-control settings often require agents to successfully traverse a set of ``narrow passages'' in their state space
RP,Recent work indicates that Deep Clustering (DC) methods are a viable option for unsupervised representations learning of visual features
RP,The current landscape of multi-agent expert imitation is broadly dominated by two families of algorithms - Behavioral Cloning (BC) and Adversarial Imitation Learning (AIL)
RP,Recently, deep AUC maximization (DAM) has achieved great success in different domains (e,g,, medical image classification)
RP, In this work, we empirically study an adaptive inference strategy for semantic segmentation that adjusts the model to the test sample before producing the final prediction
RP, We study the problem of test time robustification, i,e, using the test input to improve model robustness
RP, This raises a critical question: Is it possible to estimate when neural networks stop learning based on only the training data? In this research work, we introduce the stability property of data in layers and based on this property, we predict the near optimal epoch number of a CNN
RP,In this paper, we present a novel approach to learn texture mapping for a 3D surface and apply it to document image unwarping
RP,In this paper, we establish the first finite-time convergence result of the actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward
RP,The recently proposed  Sharpness-Aware  Minimization  (SAM)  improves generalization by minimizing a perturbed loss defined as the maximum loss within a neighborhood in the parameter space
RP,Prioritized Experience Replay (ER) has been empirically shown to improve sample efficiency across many domains and attracted great attention; however, there is little theoretical understanding of why such prioritized sampling helps and its limitations
RP,Federated learning has evolved to improve a single global model under data heterogeneity (as a curse) or to develop multiple personalized models using data heterogeneity (as a blessing)
RP, We consider the inverse problem: what can a curriculum tell us about how a learning system acquired a task? Using recurrent neural networks (RNNs) and models of common experimental neuroscience tasks, we demonstrate that curricula can be used to differentiate learning principles using target-based and a representation-based loss functions as use cases
RP,Recent work has shown that models trained to the same objective, and which achieve similar measures of accuracy on consistent test data, may nonetheless behave very differently on individual predictions
RP,Federated learning (FL) allows multiple clients with (private) data to collaboratively train a common machine learning model without sharing their private training data
RP, In this paper, we examine MCC through the lens of learning to rank (LTR) in the deep learning setting
RP,Graph neural networks (GNNs) are one of the most popular approaches to using deep learning on graph-structured data, and they have shown state-of-the-art performances on a variety of tasks
RP,Complex, long-horizon planning and its combinatorial nature pose steep challenges for learning-based agents
RP, This paper illustrates that such conditions are far from sufficient
RP,Exploring in an unknown system can place an agent in dangerous situations, exposing to potentially catastrophic hazards
RP,Modern time series corpora, in particular those coming from sensor-based data, exhibit characteristics that have so far not been adequately addressed in the literature on representation learning for time series
RP,Learning generative object models from unlabelled videos is a long standing problem and is required for causal scene modeling
RP,Neural networks are highly effective tools for image reconstruction problems such as denoising and compressive sensing
RP,Multi-task learning (MTL) is a field involved with learning multiple tasks simultaneously typically through using shared model parameters
RP,We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs
RP,Abstractive multi document summarization has evolved as a task through the basic sequence to sequence approaches to transformer and graph based techniques
RP,Machine learning-based methods have been proved to be quite successful in different domains
RP,Unsupervised semantic segmentation aims to discover and localize semantically meaningful categories within image corpora without any form of annotation
RP,Robotic agents performing domestic chores using natural language directives re-quire to learn the complex task of navigating an environment and interacting with objects in it
RP,We propose learning via retracing, a novel self-supervised approach for learning the state representation (and the associated dynamics model) for reinforcement learning tasks
RP,Quantization of deep neural networks (DNN) has been proven effective for compressing and accelerating DNN models
RP,We study the performance of the gradient play algorithm for stochastic games (SGs), where each agent tries to maximize its own total discounted reward by making decisions independently based on current state information which is shared between agents
RP,  Quantifying the data uncertainty in learning tasks is often done by learning a prediction interval or prediction set of the label given the input
RP, In this novel setting, the goal is to solve the class distribution mismatch problem between labeled and unlabeled data, where at the test time every input instance either needs to be classified into one of the existing classes or a new unseen class needs to be initialized and the instance assigned to it
RP,Offline reinforcement learning enables learning from a fixed dataset, without further interactions with the environment
RP, We survey a wide variety of techniques in active learning (AL), domain shift detection (DS), and multi-domain sampling to examine this challenging setting for question answering and sentiment analysis
RP,Self-supervised learning alleviates the massive demands for annotations in deep learning, and recent advances are mainly dominated by contrastive learning
RP,Early stopping is a simple and widely used method to prevent over-training neural networks
RP,We advocate for a practical Maximum Likelihood Estimation (MLE) approach towards designing loss functions for regression and forecasting, as an alternative to the typical approach of direct empirical risk minimization on a specific target metric
RP,Many realistic multi-agent problems naturally require agents to be capable of performing asynchronously without waiting for other agents to terminate (e,g,, multi-robot domains)
RP, In this paper, we point out that this issue can be addressed by balancing information flow from the initial model and training dataset to the local adaptation
RP,One of the challenges of graph-based semi-supervised learning over ordinary supervised learning for classification tasks lies in label utilization
RP,Despite the empirical success of the deep Q network (DQN) reinforcement learning algorithm and its variants, DQN is still not well understood and it does not guarantee convergence
RP,Adaptive gradient methods, such as Adam, have achieved tremendous success in machine learning
RP,Implicit layers are computational modules that output the solution to some problem depending on the input and the layer parameters
RP, In this paper, we explore utilizing this structural locality within non-parametric language models, which generate sequences that reference retrieved examples from an external source
RP, This naturally leads to the following questions: Can a self-attention layer of ViT express any convolution operation? In this work, we prove that a single ViT layer with image patches as the input can perform any convolution operation constructively, where the multi-head attention mechanism and the relative positional encoding play essential roles
RP,Recommender (RS) and Advertising/Marketing Systems (AS) play the key roles in E-commerce companies like Amazaon and  Alibaba
RP, This might help to shed some light on why larger learning rates can lead to better generalization in some practical scenarios
RP,Deep generative modeling has long been viewed as a challenging unsupervised learning problem, partly due to the lack of labels and the high dimension of the data
RP,This paper argues that continual learning methods can benefit by splitting the capacity of the learner across multiple models
RP,Graph sparsification concerns data reduction where an edge-reduced graph of a similar structure is preferred
RP,Irregularly sampled time series commonly occur in several domains where they present a significant challenge to standard deep learning models
RP,Emphatic temporal difference (ETD) learning (Sutton et al, 2016) is a successful method to conduct the off-policy value function evaluation with function approximation
RP, In this work, we argue that the morphological information is still very useful and propose a transformer policy model that effectively encodes such information
RP, In this paper, we study the role of the pretraining distribution on the emergence of in-context learning
RP,Energy-based Model (EBM) offers a powerful approach for modeling discrete structure, but both inference and learning of EBM are hard as it involves sampling from discrete distributions
RP,Gradient descent (GD) plays a crucial role in the success of deep learning, but it is still not fully understood how GD finds minima that generalize well
RP,While large pre-trained models have enabled impressive results on a variety of downstream tasks, the largest existing models still make errors, and even accurate predictions may become outdated over time
RP,This paper investigates unsupervised learning of Full-Waveform Inversion (FWI), which has been widely used in geophysics to estimate subsurface velocity maps from seismic data
RP,In several domains such as natural language processing, it has been empirically reported that simple addition and subtraction in a somehow learned embedding space capture analogical relations
RP,Generative adversarial networks (GANs) with clustered latent spaces can perform conditional generation in a completely unsupervised manner
RP,Consistency regularization, referring to enforcing consistency across a model's responses to different views of the same input, is widely used for self-supervised image representation learning
RP,Continual learning seeks to train a single network for multiple tasks (one after another), where training data for each task is only available during the training of that task
RP, We study a common setting where our task is not purely opaque
RP,Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality samples such as image and audio samples
RP,Ensembles are a very effective way of increasing both the robustness and accuracy of a learning system
RP,Molecular conformation generation, which is to generate 3 dimensional coordinates of all the atoms in a molecule, is an important task for bioinformatics and pharmacology
RP,Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights
RP, This paper focuses on the fully test-time adaptation setting, where only unlabeled data from the target distribution is required
RP,Spatial convolutions are widely used in numerous deep video models
RP,We present Path Integral Sampler~(PIS), a novel algorithm to draw samples from unnormalized probability density functions
RP, This paper examines the neuron activation patterns of deep learning-based classification models and explores whether the models' performances can be explained through neurons' activation behavior
RP,The goal of the paper is to design active learning strategies which lead to domain adaptation under an assumption of Lipschitz functions
RP,Target Propagation (TP) algorithms compute targets instead of gradients along neural networks and propagate them backward in a way that is similar yet different than gradient back-propagation (BP)
RP,An agent's functionality is largely determined by its design, i,e, skeletal structure and joint attributes (e,g,, length, size, strength)
RP,Pruning overparameterized neural networks to obtain memory-and-compute-efficient sparse networks is an active area of research
RP,We propose a minimax formulation for removing backdoors from a given poisoned model based on a small set of clean data
RP,Attention mechanisms take an expectation of a data representation with respect to probability weights
RP,This paper presents a simple MLP-like architecture, CycleMLP, which is a versatile backbone for visual recognition and dense predictions
RP,We consider adversarial attacks to a black-box model when no queries are allowed
RP,Graph Neural Networks (GNNs) are proven to be a powerful machinery for learning complex dependencies in multivariate spatio-temporal processes
RP,Understanding the robustness of machine learning models to adversarial examples generated by test-time adversaries is a problem of great interest
RP,We present a method to compute the derivative of a learning task with respect to a dataset
RP,We show how to train a rotation-equivariant representation to extract local keypoints for image matching
RP,A key goal of unsupervised representation learning is ``inverting'' a data generating process to recover its latent properties
RP,Risk management is critical in decision making, and mean-variance (MV) trade-off is one of the most common criteria
RP,Reinforcement learning (RL) experiments have notoriously high variance, and minor details can have disproportionately large effects on measured outcomes
RP,deep reinforcement learning (Deep RL) has recently seen significant progress in developing algorithms for generalization
RP,We derive an online algorithm for unsupervised learning based on representing every input \mathbf{x_t by a high dimensional vector \mathbf{y_t with pairwise inner products that approximately match input similarities as measured by a kernel function: \mathbf{y_s \cdot \mathbf{y_{t \approx f(\mathbf{x_s, \mathbf{x_{t)
RP, In this paper, we explore the non-contrastive representation learning for deep clustering, termed NCC, which is based on BYOL, a representative method without negative examples
RP, In this work we study rapid learning of attributes that are previously not labeled in the dataset
RP,The most significant barrier to the advancement of Neural Architecture Search (NAS) is its demand for large computational resources, which hinders scientifically sound empirical evaluations of NAS methods
RP,Training a task-oriented dialogue agent can be naturally formulated as Offline reinforcement learning (RL) problem, where the agent aims to learn a conversational strategy to achieve user goals, only from a dialogue corpus
RP,Numerous physical systems are described by ordinary or partial differential equations whose solutions are given by holomorphic or meromorphic functions in the complex domain
RP,Building embodied intelligent agents that can interact with 3D indoor environments has received increasing research attention in recent years
RP,  We focus on Green's function of Poisson and Helmholtz equations in bounded domains, unbounded domains and domains with interfaces
RP, This paper studies the generalized Gradient Descent-Ascent (GDA) flow in a large class of non-convex non-concave Zero-Sum games dubbed Hidden Convex-Concave games, a class of games that includes GANs
RP,We study the problem of cost-sensitive hierarchical classification where a label taxonomy has a cost-sensitive loss associated with it, which represents the cost of (wrong) predictions at different levels of the hierarchy
RP,We introduce Softmax Gradient Tampering, a technique for modifying the gradients in the backward pass of neural networks in order to enhance their accuracy
RP,Genetic barcoding coupled with single-cell sequencing technology enables direct measurement of cell-to-cell transitions and gene-expression evolution over a long timespan
RP,   In this paper, we aim to relax these assumptions using techniques from the AI domain
RP, In this paper, we investigate how to transfer the inductive-biases implicit in generative-approaches to contrastive methods
RP,Evidence from cognitive psychology suggests that understanding spatio-temporal object interactions and dynamics can be essential for recognizing actions in complex videos
RP,Computational cost to train state-of-the-art deep models in many learning problems is rapidly increasing due to more sophisticated models and larger datasets
RP,We study the problem of learning verifiably safe parameters for programs that use neural networks as well as symbolic, human-written code
RP,Gigantic pre-trained models have become central to natural language processing (NLP), serving as the starting point for fine-tuning towards a range of downstream tasks
RP, This work aims to understand latent correlation maximization-based deep multiview learning from a latent component identification viewpoint
RP,As the most critical components in a sentence, subject, predicate, and object require special attention in the video captioning task
RP,Supervised learning usually requires a large amount of labelled data
RP,In model-free deep reinforcement learning (RL) algorithms, using noisy value estimates to supervise policy evaluation and optimization is detrimental to the sample efficiency
RP, In this work, we study the problem of learning SED from a training set of graph pairs and their SED values
RP,Large pre-trained models such as CLIP offer consistent accuracy across a range of data distributions when performing zero-shot inference (i,e, without fine-tuning on a specific dataset)
RP,Additive manufacturing suffers from imperfections in hardware control and material consistency
RP, Specifically, we consider the k-means problem augmented with a predictor that, given any point, returns its cluster label in an approximately optimal clustering up to some, possibly adversarial, error
RP,We present a novel framework for Distributing Black-Box Optimization (DiBB)
RP,We argue that a form of the valuable information provided by the auxiliary information is its implied data clustering information
RP,We describe a novel architecture for modeling the cost-to-go function in approximate dynamic programming problems involving country-scale, real-life electrical power generation systems
RP,Graph Neural Networks (GNNs) are gaining extensive attention for their application in graph data
RP,Humans find structure in natural phenomena by absorbing stimuli from multiple input sources such as vision, text, and speech
RP,In this paper, we advocate for two stages in a neural network's decision making process
RP,The theory of function approximation in reinforcement learning (RL) typically considers low capacity representations that incur a tradeoff between approximation error, stability and generalization
RP,We present the first provable Least-Squares Value Iteration (LSVI) algorithm that achieves runtime complexity sublinear in the number of actions
RP,Adversarial attacks carefully perturb natural inputs, so that a machine learning algorithm produces erroneous decisions on them
RP,In this paper we propose a new generative model of text, Step-unrolled Denoising Autoencoder (SUNDAE), that does not rely on autoregressive models
RP,In this paper, we focus on the problem of identifying bad training data when the underlying cause is unknown in advance
RP, Our studies are based on the synthetic dataset DSprites and the face dataset UTKFace
RP,Goal-conditioned reinforcement learning (RL) has shown great success recently at solving a wide range of tasks(e,g,, navigation, robotic manipulation)
RP,We analyze neural networks composed of bijective flows and injective expansive elements
RP,Humans can generalize from one or a few examples, and even from very little pre-training on similar tasks
RP,This paper investigates two techniques for developing efficient self-supervised vision transformers (EsViT) for visual representation learning
RP,We present AutoOED, an Automated Optimal Experimental Design platform powered by machine learning to accelerate discovering solutions with optimal objective trade-offs
RP,Artificial intelligence are gaining more attractions for program analysis and semantic understanding
RP, This paper focuses on the scenario where sensitive data are distributed among individual participants, who jointly train a model through federated learning, using both secure multiparty computation (MPC) to ensure the confidentiality of individual gradient updates, and differential privacy to avoid data leakage in the resulting model
RP,Efficient performance estimation of architectures drawn from large search spaces is essential to Neural Architecture Search
RP, We present a novel method, CrossMatch, for semi-supervised object detection
RP,Audio-visual navigation task requires an agent to find a sound source in a realistic, unmapped 3D environment by utilizing egocentric audio-visual observations
RP,Understanding and explaining the mistakes made by trained models is critical to many machine learning objectives, such as improving robustness, addressing concept drift, and mitigating biases
RP, In this paper, we study  training and inference of neural networks under the MPC setup
RP,Drawing inspiration from gradient-based meta-learning methods with infinitely small gradient steps, we introduce Continuous-Time Meta-Learning (COMLN), a meta-learning algorithm where adaptation follows the dynamics of a gradient vector field
RP,Many problems in RL, such as meta RL, robust RL, and generalization in RL can be cast as POMDPs
RP,Evaluating the worst-case performance of a reinforcement learning (RL) agent under the strongest/optimal adversarial perturbations on state observations (within some constraints) is crucial for understanding the robustness of RL agents
RP, In this paper, we study calibration emphasizing these scenarios
RP,We present Sequential Neural Variational Inference (SNVI), an approach to perform Bayesian inference in models with intractable likelihoods
RP, This work argues that when the transformations in train T^\text{tr and test T^\text{te are (arbitrary) symmetry transformations induced by a collection of known m equivalence relations, the task of finding a robust OOD classifier can be defined as finding the simplest causal model that defines a causal connection between the target labels and the symmetry transformations that are associated with label changes
RP,Estimating prediction uncertainty and confidence of deep learning models is crucial for mission-critical machine learning applications, such as biomedical imaging for diagnostics or therapy, and self-driving cars
RP,Recent work in synthetic data generation in the time-series domain has focused on the use of Generative Adversarial Networks
RP,In this paper, we propose THOMAS, a joint multi-agent trajectory prediction framework allowing for efficient and consistent prediction of multi-agent multi-modal trajectories
RP,Graph neural networks (GNNs) have been demonstrated powerful expressiveness on graph representation with different message passing schemes, but fail to improve the prediction performance by stacking layers because of over-smoothing
RP,Transfer learning from ImageNet pre-trained models has become essential for many computer vision tasks
RP,It has been well recognized that neural network based image classifiers are easily fooled by images with tiny perturbations crafted by an adversary
RP,Overparameterized neural networks generalize well but are expensive to train
RP,Both animals and artificial agents benefit from state representations that support rapid transfer of learning across tasks and which enable them to efficiently traverse their environments to reach rewarding states
RP,A robot operating in a household makes observations of multiple objects as it moves around over the course of days or weeks
RP,Few-shot learning (FSL) aims to learn a classifier that can be easily adapted to accommodate new tasks not seen during training, given only a few examples
RP,Bias mitigation algorithms aim to reduce the performance disparity between different protected groups
RP,A major challenge in real-world reinforcement learning (RL) is the sparsity of reward feedback
RP,Preference-based reinforcement learning (RL) has shown potential for teaching agents to perform the target tasks without a costly, pre-defined reward function by learning the reward with a supervisor’s preference between the two agent behaviors
RP, This success stems from Focal loss regularizing the entropy of the network's prediction (controlled by the hyper-parameter \gamma), thereby reining in the network's overconfidence
RP,Vision Transformer (ViT) has recently demonstrated promise in computer vision problems
RP,Humans commonly solve complex problems by decomposing them into easier subproblems and then combining the subproblem solutions
RP,Group equivariant Convolutional Neural Networks (G-CNNs) constrain features to respect the chosen symmetries, and lead to better generalization when these symmetries appear in the data
RP,Despite successes across a broad range of applications, Transformers have limited capability in systematic generalization
RP,out-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks
RP, In this paper, we examine some critical assessments concerning the development and subsequent evaluation of language models and offer an alternative account
RP, In this paper, we are the first to observe that some of this performance can be recovered when training with a loss tailored to DP-SGD; we challenge cross-entropy as the de facto loss for deep learning with DP
RP,DALL\cdotE has shown an impressive ability of composition-based systematic generalization in image generation
RP,Molecular design and synthesis planning are two critical steps in the process of molecular discovery that we propose to formulate as a single shared task of conditional synthetic pathway generation
RP,Accurately estimating sound sources' temporal location, spatial location and semantic identity label from multi-channel sound raw waveforms is crucial for an agent to understand the 3D environment acoustically
RP, To that end, the paper asks the question how does conservatism affect offline learning? The proposed answer studies conservatism in light of value function optimization, approximate objectives that upper bound underestimations and behavior cloning as auxilary regularization objective
RP,Message-passing algorithms based on the Belief Propagation (BP) equations constitute a well-known distributed computational scheme
RP,We present Reward-Switching Policy Optimization (RSPO), a paradigm to discover diverse strategies in complex RL environments by iteratively finding novel policies that are both locally optimal and sufficiently different from existing ones
RP,Time series (TS) Anomaly detection (AD) plays an essential role in various applications, e,g,, fraud detection in finance and healthcare monitoring
RP,Recent Semi-Supervised Object Detection methods are mainly based on self-training, i,e, generating hard pseudo-labels by a teacher model on unlabeled data as supervisory signals
RP, In this work, we examine the high-dimensional asymptotics of random feature regression in the presence of structured data, allowing for arbitrary input correlations and arbitrary alignment between the data and the weights of the target function
RP,We present a new framework AMOS that pretrains text encoders with an Adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators
RP,Stochastic gradient descent and other first-order variants, such as Adam and AdaGrad, are commonly used in the field of deep learning due to their computational efficiency and low-storage memory requirements
RP,Self-driving cars must detect vehicles, pedestrians, and other trafﬁc participants accurately to operate safely
RP,We explore a new perspective on video understanding by casting the video recognition problem as an image recognition task
RP,Despite extensive progress on image generation, deep generative models are suboptimal when applied to lossless compression
RP, In this paper we investigate possible reasons behind this performance gap, namely, the indistinguishability of tokens, and mismatch between training and inference
RP,Variational inequalities in general and saddle point problems in particular are increasingly relevant in machine learning applications, including adversarial learning, GANs, transport and robust optimization
RP, Here we theoretically study the acquisition of systematic knowledge by simple neural networks
RP, In this paper, we solve the problem of synthesizing irregular and intermittent time-series where values can be missing and may not have specific frequencies, which is far more challenging than existing settings
RP,Recently Graph Injection Attack (GIA) emerges as a practical attack scenario on Graph Neural Networks (GNNs), where the adversary can merely inject few malicious nodes instead of modifying existing nodes or edges, i,e, Graph Modification Attack (GMA)
RP,Reinforcement learning can train policies that effectively perform complex tasks
RP,The study of provable adversarial robustness for deep neural networks (DNNs) has mainly focused on static supervised learning tasks such as image classification
RP,We often see undesirable tradeoffs in robust machine learning where out-of-distribution (OOD) accuracy is at odds with in-distribution (ID) accuracy
RP,Training neural networks requires increasing amounts of memory
RP,Local divisive normalization provides a phenomenological description of many nonlinear response properties of neurons across visual cortical areas
RP,Interpretable and explainable machine learning has seen a recent surge of interest
RP,The kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) compresses an n point distributional summary into a \sqrt n point summary with better-than-Monte-Carlo maximum mean discrepancy for a target kernel k by leveraging a less smooth square-root kernel
RP,Open set recognition involves identifying data instances encountered during test time that do not belong to known classes in the training set
RP,The study of language emergence aims to understand how human languages are shaped by perceptual grounding and communicative intent
RP,Devising augmentations for graph contrastive learning is challenging due to their irregular structure, and drastic distribution shifts and nonequivalent feature spaces across datasets
RA, To prevent such collapse, we develop two novel methods by decorrelating on different dimensions on the instance embedding stacking matrix, i,e, Instance-wise (ICL) and Feature-wise (FCL) Contrastive Learning
RA, In this work, we propose the LaGraph, a theoretically grounded predictive SSL framework based on latent graph prediction
RA, In this work, we propose an unsupervised learning method for addressing the relational learning problem where we learn the underlying relationship between a pair of data irrespective of the nature of those data
RA, In this work, we conduct analyses in the spherical coordinate system (SCS) for the complete identification of 3D graph structures
RA, This leads to the proposal of ABEL: an automatic scheduler which decays the learning rate by keeping track of the weight norm
RA, We propose, MSG, a model-free dynamic programming based offline RL method that trains an ensemble of independent Q-functions, and updates a policy to act conservatively with respect to the uncertainties derived from the ensemble
RA, In this study, we propose ratio matching with gradient-guided importance sampling (RMwGGIS) to alleviate the above limitations
RA, To approach this objective, we propose PIVQGAN with two novel techniques in the framework of StyleGAN2
RA,This paper proposes a novel contrastive learning framework, called Self-Contrastive (SelfCon) Learning, that self-contrasts within multiple outputs from the different levels of a multi-exit network
RA, For this purpose, we define adversarial subspaces, which are spanned by orthogonal directions of minimal perturbation to the decision boundary from any given input sample
RA, We recognize that, for the GP, the bias-variance trade-off regulation is made by optimization of the two hyperparameters: the length scale and noise-term
RA, Our method is three-fold: 1) We propose Class-Aware Propensity (CAP) that exploits the unlabeled data to train an improved classifier using the biased labeled data
RA,  Specifically, we first formulate the problem of learning a sampling policy as a Markov decision process
RA, In several robotic locomotion tasks, simulated in the MuJoCo platform, we observe that existing models fail in distinguishing and imitating different modes of behavior in both cases of discrete and continuous latent variables
RA, To understand how adversarially robust optimizations/representations compare to human vision, we performed a psychophysics experiment using a metamer task similar to Freeman & Simoncelli, 2011, Wallis et al, 2016 and Deza et al, 2019 where we evaluated how well human observers could distinguish between images synthesized to match adversarially robust representations compared to non-robust representations and a texture synthesis model of peripheral vision (Texforms a la Long et al, 2018)
RA, In this paper, we proposed a novel plug-in operation, Dynamic Parameterized Operation (DPO), to learn both explicit and implicit interaction instance-wisely
RA, To address the semi-supervised long-tailed recognition problem, we present an alternate sampling framework combining the intuitions from successful methods in these two research areas
RA,We present a neat yet effective recursive operation on vision transformers that can improve parameter utilization without involving additional parameters
RA, In this paper, we present a new stochastic contrastive conditional generative adversarial network (InfoSCC-GAN) with explorable latent space
RA, We propose ask2mask (ATM), a novel approach to focus on specific samples during MSM pre-training
RA, To address this issue, this paper uses latent label representations to model label correlations implicitly
RA, We consider the impact of demographic shift and present a class of algorithms, called Shifty algorithms, that provide high-confidence behavioral guarantees that hold under demographic shift
RA, Therefore, we consider the missing training data problem in MARL, and then propose the imputation assisted multiagent reinforcement learning (IA-MARL)
RA, In this study, we pioneeringly confirm that properly incorporating activation quantization into the PTQ reconstruction benefits the final accuracy
RA,In this work, we build a knowledge graph that captures key attributes of content and notifications in a digital health platform for diabetes management
RA, In this paper, we introduce an approach that relies on a new filtration function to account for location during network training
RA, By contrast, this paper makes the first attempt at an algorithm for sandwiching the R-D function of arbitrary sources requiring only data samples
RA,  In this work, we view mutual information estimation from the perspective of importance sampling
RA, This paper presents SynCLR, a synthesis framework for contrastive learning of speech representations that can be generalized over unseen domains
RA, After refuting their claims, we introduce vector decomposition for analyzing the collapse based on the gradient analysis of  l2  normalized vector
RA, Instead, we shed new light on addressing distribution shifts by directly eliminating domain-related spurious correlations with augmentation, leading to a simple technique based on mixup, called LISA (Learning Invariant Representations via Selective Augmentation)
RA, As the initial effort to investigate this problem, we reveal the facts that adversarially trained models present two distinguished behaviors from naturally trained models in imbalanced datasets: (1) Compared to natural training, adversarially trained models can suffer much worse performance on under-represented classes, when the training dataset is extremely imbalanced
RA,   In this work, we propose LaMer, a novel text style transfer framework based on large-scale language models
RA, Inspired by this, we investigate how to build a modality-shared Contrastive Language-Image Pre-training framework (MS-CLIP)
RA, To this end, we propose innovative priors such as diversity and topological simplicity to not only increase the chances of finding the appropriate triggers but also improve the quality of the found triggers
RA, To this end, we further propose probabilistic gradient pruning to firstly identify gradients with potentially large errors and then remove them
RA, We propose a novel solution to achieve this objective that is significantly faster ( ∼200×  on ImageNet) than the naive solution
RA, We discuss the ideal goals of an adversarial defense algorithm beyond perceptual limits, and further highlight the shortcomings of naively extending existing training algorithms to higher perturbation bounds
RA, We examine several approximation methods to estimate TIC with feasible computational load and investigate the accuracy trade-off
RA,We introduce Network Augmentation (NetAug), a new training method for improving the performance of tiny neural networks
RA, Thus, we aim to fill the missing gap in the literature by tackling a more realistic setting that can leverage partially available sensitive or group information during training
RA, We find that the two new metrics can be regulated by the temperature coefficient in InfoNCE loss
RA, The proposed method called nonconvex continual learning (NCCL) adapts the learning rates of both previous and current tasks with the gradients
RA, Thus, this attack could be used as an auditing tool to quantify the private information that a model leaks about the individual data points in its training set
RA,We introduce Palette, a simple and general framework for image-to-image translation using conditional diffusion models
RA, In this work, we propose a pessimistic model selection (PMS) approach for offline DRL with a theoretical guarantee, which features a tuning-free framework for finding the best policy among a set of candidate models
RA, This is done by leveraging a diverse set of training environments to reduce the effect of spurious features and build an invariant predictor
RA, In this work, we formulate an adversarial attack using a branch-and-bound (BaB) procedure on ReLU neural networks and search adversarial examples in the activation space corresponding to binary variables in a mixed integer programming (MIP) formulation
RA,This paper presents a systematic study of multi-objective online learning
RA, We name this phenomenon spatiotemporal mode collapse and explore it for the first time in predictive learning
RA, We show that for a randomly initialized two-layer ReLU neural network trained with IBP, with sufficiently small perturbation radius and large network width, gradient descent for IBP training can converge to zero training error with a linear convergence rate for logistic loss, and the training can converge to a point where the robustness certification by IBP accurately reflects the true robustness of the model, with a high probability
RA, We introduceSphere2Vec, a location encoder, which can directly encode spherical coordinates while preserving spherical distances
RA, In this work, we propose a novel method KG-FiD, which filters noisy passages by leveraging the structural relationship among the retrieved passages with a knowledge graph
RA, Inspired by control theory, we propose a prediction algorithm that guarantees calibration even under distribution shift, and achieves strong performance on metrics such as sharpness and proper scores
RA,We present a new method for one shot domain adaptation
RA,We describe a curriculum learning framework capable of discovering optimal curricula in addition to performing standard curriculum learning
RA, Along the way, we highlight certain philosophical issues with the closely related and popular notion of confidence calibration
RA, To this end, we integrate the ViT architecture into generative adversarial networks (GANs)
RA, Last but not least, we find that by adjusting the weights of each training example, a solution on the SP Pareto front can be obtained, resulting in a better SP trade-off for LL
RA, In this paper, we show that rewards dervied by mirror descent  ensures minimization of a Bregman divergence in terms of a rigorous regret bound of \mathcal{O(1/T) for a particular condition of step sizes \{\eta_t\_{t=1^T
RA, We introduce Differentiable Diffusion Sampler Search (DDSS): a method that learns few-step samplers for any pre-trained DDPM by using gradient descent
RA, Therefore, we propose to unify and generalize previous flow-based approaches under a single non-adversarial framework, which we prove is equivalent to minimizing an upper bound on the Jensen-Shannon Divergence (JSD)
RA, We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions
RA, In this work, we explore contrastive learning from a new perspective, inspired by the recent works showing that properly designed weight perturbations or quantization help the models learn a smoother loss landscape
RA, We introduce the first offline dataset based on StarCraftII with diverse quality levels and propose a multi-agent decision transformer (MADT) for effective offline learning
RA, To this end, we propose PipeGCN, a simple-yet-effective scheme that hides the communication overhead by pipelining inter-partition communication with intra-partition computation
RA, We present a simple framework called "generate, annotate, and learn (GAL)" that uses unconditional language models to synthesize in-domain unlabeled data, helping advance SSL and KD on NLP and tabular tasks
RA, Therefore, we propose Scene Knowledge Graph (SKG), a generic representation of machine imagination, that (1) allows us to collect and unify a diverse set of knowledge resources from different domains and modalities that capture common sense, and (2) decompose the direct mapping from concepts to text with SKG as the contextualized intermediate representation
RA, To build predictive models that perform well regardless of the nuisance-label relationship, we develop Nuisance-Randomized Distillation (NURD)
RA, In this paper, we introduce Fast AdvProp, which aggressively revamps AdvProp’s costly training components, rendering the method nearly as cheap as the vanilla training setting
RA, We propose a new initialization scheme for the k-median problem in the general metric space (e,g,, discrete space induced by graphs), based on the construction of metric embedding tree structure of the data
RA, To tackle this issue, this paper introduces and investigates a new concept called ''learnability lock'' for securing the process of data authorization
RA, Specifically, we propose to train a MIMO model with adversarial training ({MAT), where each sub-model can be trained on a different attack type
RA, In this work, we develop tesseract—a defense against this directed deviation attack, a state-of-the-art model poisoning attack
RA, We address these shortcomings by leveraging influence functions in order to derive suitable expressions of the population loss and its lower bound, while imposing minimal assumptions on the form of the parametric model
RA, To this end, we propose a new regularization mechanism for meta-learning -- Minimax-Meta Regularization
RA, We evaluate LEAP on a number of datasets including video and motion capture data
RA, Inspired by this, we propose a new framework to extract object-centric representation from single 2D images by learning to predict future scenes in the presence of moving objects
RA, In this work, we propose a reconstruction-free MBRL agent, called DreamerPro, to achieve this goal
RA, To set the stage for more realistic out-of-distribution detection, we depart from small-scale settings and explore large-scale multiclass and multi-label settings with high-resolution images and thousands of classes
RA, To alleviate this issue, most existing approaches introduce a regularization term to favor state-action pairs from the dataset
RA, To mitigate this issue, meta-reinforcement learning methods aim to enable fast learning on novel tasks by learning how to learn
RA, In this work, we propose APPLE, a personalized cross-silo FL framework that adaptively learns how much each client can benefit from other clients’ models
RA,This work develops a novel framework for communication-efficient distributed learning where the models to be learned are overparameterized
RA, To encourage both comprehensive C-Dis-RL and convexity simultaneously, we propose a simple yet efficient method: Controllable Interpolation Regularization (CIR), which creates a positive loop where the disentanglement and convexity can help each other
RA, To this end, we aim to develop low-cost GCNs with improved trainability, as inspired by recent findings in deep neural network optimization which show that not all data/(model components) are equally important
RA, In this paper, we build upon recent work using the signature of a path as a feature map and investigate a computationally efficient technique to approximate these features based on linear random projections
RA, To simplify the problem, we focus on a prototype elliptic PDE: the Schr\"odinger equation on a hypercube with zero Dirichlet boundary condition, which has wide application in the quantum-mechanical systems
RA, We analyze several fine-tuning methods across a diverse set of image classification tasks across two spectra investigating the amount and similarity of downstream data to that of pretraining one
RA,  We experiment on Atari games, FetchEnv tasks and a challenging physically simulated car push and reach task
RA,  In this paper, we propose \method, which incorporates physicochemical knowledge into deep models, leading to unsupervised drug design
RA, To address this challenge, we propose a novel Edit-Invariant Sequence Loss (EISL), which computes the matching loss of a target n-gram with all n-grams in the generated sequence
RA,As a step toward this goal, we analyze learning and generalization of a three-layer neural network with ReLU activations in a regime that goes beyond the linear approximation of the network, and is hence not captured by the common Neural Tangent Kernel
RA,We study COMP-AMS, a distributed optimization framework based on gradient averaging and adaptive AMSGrad algorithm
RA, We show that even with proper network design, such learned representation often leads to collision in the latent space: two points with significantly different observations collide in the learned latent space, leading to degraded optimization performance
RA, We develop an evolution-based algorithm—SparseEvo—for the problem and evaluate against both convolutional deep neural networks and vision transformers
RA, Based on this observation, we found that the sharpness gap between the teacher and student output may cause this degradation problem
RA, To alleviate the concerns, we propose and study the problem of graph condensation for graph neural networks (GNNs)
RA, Remarkably, derived from the causal inference principle of front-door adjustment, we propose two frustratingly easy but effective deconfounder algorithms, i,e, sampling multiple versions of the meta-knowledge via Dropout and grouping the meta-knowledge into multiple bins
RA,We present HyperCGAN: a conceptually simple and general approach for text-to-image synthesis that uses hypernetworks to condition a GAN model on text
RA, To overcome these challenges, we introduce a connectome-constrained latent variable model (CC-LVM) of the unobserved voltage dynamics of the entire C. elegans nervous system and the observed calcium signals
RA, We obtain such an MTK model by training it with a newly generated training set
RA, In this work, we propose to reorganize image tokens during the feed-forward process of ViT models
RA, To approximate the complicated human reasoning process, state-of-the-art QA systems commonly use pre-trained language models (LMs) to access knowledge encoded in LMs together with elaborately designed modules based on Graph Neural Networks (GNNs) to perform reasoning over knowledge graphs (KGs)
RA,We propose POLAR, a polynomial arithmetic framework that leverages polynomial overapproximations with interval remainders for bounded-time reachability analysis of neural network-controlled systems (NNCSs)
RA, Building on a novel definition of label uncertainty, we empirically demonstrate that error regions induced by state-of-the-art models tend to have much higher label uncertainty than randomly-selected subsets
RA, In this paper, we propose a novel FL paradigm empowered by the NTK framework
RA, To alleviate these problems, we propose to explicitly minimize the divergence between inferences from arbitrary subsets and the surrogate joint posterior that approximates the true joint posterior
RA, To tackle this problem effectively, we propose to improve the robustness of one-class classification trained on self-supervised representations using a data refinement process
RA, We, in this paper, propose a novel adversarial multi-armed bandit approach which automatically learns to route source representations to appropriate target representations following which they are combined in meaningful ways to produce accurate target models
RA,rQdia (pronounced “Arcadia”) regularizes Q-value distributions with augmented images in pixel-based deep reinforcement learning
RA,We present a two-step hybrid reinforcement learning (RL) policy that is designed to generate interpretable and robust hierarchical policies on the RL problem with graph-based input
RA, This work introduces a novel approach using binary-encoded labels (BEL) to generalize the application of binary classification to regression, and a taxonomy identifying the key design aspects of such formulation
RA, To mitigate this issue, we propose Random Fourier Network (RFN) with a band-agnostic dimension-less initialization scheme, as a drop-in replacement of the standard multi-layer perceptrons
RA, We propose a novel data augmentation approach that recovers informative orders for labels using their dependence information
RA, However, we observe that different pretext tasks affect downstream tasks differently cross datasets, which suggests that searching pretext tasks is crucial for graph self-supervised learning
RA,We develop a multiple shooting method for learning in deep neural networks based on the Lagrangian perspective on automatic differentiation
RA, We reparametrize the optimization variables as the output of a neural network
RA,   We present Knowledge Infused Decoding (KID)---a novel decoding algorithm for generative LMs, which dynamically infuses external knowledge into each step of the LM decoding
RA, In this work, we formalize the relation between heterophily and robustness, bridging two topics previously investigated by separate lines of research
RA, To address this problem, we leverage the sequential nature of RL to learn robust representations that encode only task-relevant information from observations based on the unsupervised multi-view setting
RA, We perform the first in-depth analysis of the compute and memory requirements for NTK computation in finite width networks
RA, In particular, we express the domain knowledge as first-order logic rules and embed these logic rules in a probabilistic graphical model
RA,We introduce neural network architectures that model the mechanism that generates data and address the difficult problem of disentangling the multimodal structure of data ensembles
RA, In this paper, we analyze how a prototype classifier works equally well without fine-tuning and meta-learning
RA, We develop an algorithm, dubbed ZEro-Shot Recommenders (ZESRec), that is trained on an old dataset and generalize to a new one where there are neither overlapping users nor overlapping items, a setting that contrasts typical cross-domain RecSys that has either overlapping users or items
RA,We propose to identify directions invariant to a given classifier so that these directions can be controlled in tasks such as style transfer
RA, In this work, we propose an instance-specific label smoothing technique, Pseudo-KD, which is efficiently learnt from the data
RA, We initiate the formal study of latent state discovery in the presence of such exogenous noise sources by proposing a new model, the Exogenous Block MDP (EX-BMDP), for rich observation RL
RA, To address the issue, we propose Kernel Stochastic Gradient Descent (Kernel SGD) which projects the optimization problem to a transformed space with the Hessian matrix of kernel machines
RA, In particular, we leverage ideas from Bayesian optimal experimental design to guide the selection of state-action queries for efficient learning
RA, In contrast, we propose a modular method with structured representations that (1) builds a semantic map of the scene, and (2) performs exploration with a semantic search policy, to achieve the natural language goal
RA,  In this work, we rectify this situation by developing a new type of transformation which is perfectly compatible with a variant of ReLUs -- Leaky ReLUs
RA, In this paper, we present VUT, a Versatile UI Transformer that takes multimodal input and simultaneously accomplishes 5 distinct tasks with the same model
RA, In this paper, we show that graph-based approaches are also well suited for hyperbolic geometry
RA, This paper proposes WeaveNet, a differentiable solver for diverse non-linear assignment problems
RA, This paper demonstrates a HYPOCRITE for hypocrisy that generates homoglyph adversarial examples for natural language web services in the physical world
RA, We propose a novel visual memory network architecture for the learning and inference problem in the spatial-temporal domain
RA, We devise a model that leverages CLIP to ground objects in a scene described by the goal text paired with spatial relationship rules to provide an off-the-shelf reward signal on only raw pixels to learn a set of robotic manipulation tasks
RA, In this paper, we propose a novel algorithm for steganography that takes advantage of the fact that neural networks are sensitive to tiny perturbations
RA, We go a step further and investigate how the \emph {geometrical compactness of the ID feature distribution makes isolating and detecting outliers easier, especially in the realistic situation when ID training data is polluted (i,e, ID data contains some OOD data that is used for learning the feature extractor parameters)
RA,We propose a model-based multi-agent reinforcement learning attack framework against federated learning systems
RA,We leverage logical composition in reinforcement learning to create a framework that enables an agent to autonomously determine whether a new task can be immediately solved using its existing abilities, or whether a task-specific skill should be learned
RA, In particular, we propose the first model-based approach to perform adversarial attacks for cooperative MARL
RA, Our main idea is to generalize the mean-shift algorithm by constraining the search space of nearest neighbors, resulting in semantically purer representations
RA, To address this deficiency, we introduce a simple meta-procedure---Compress++---for speeding up any input thinning algorithm while suffering at most a factor of four in error
RA, We present a novel method for generating these regions in a scalable manner which works by iteratively refining the region initially obtained via sampling until certification with state-of-the-art methods is achieved
RA, In this paper, we first reveal the fact that under some noise assumption in the stochastic control model, we can obtain the linear spectral feature of its corresponding Markov transition operator in closed-form for free
RA,We present a novel framework to train a large deep neural network (DNN) for only once, which can then be pruned to any sparsity ratio to preserve competitive accuracy without any re-training
RA, We propose three new privacy-preserving multi-label mechanisms: Binary, \tau, and Powerset voting
RA, In this paper, we relax this restriction and consider data sources with additional confounding differences, from which the desired style needs to be inferred
RA, Specifically, all prior CL methods discard samples overflowed from the EM and can never retrieve them back for subsequent training steps, incurring loss of information that would exacerbate catastrophic forgetting
RA, We analyze this previously unexplored problem, make observations, and address it by introducing Fisher information to weigh the importance of parameters affecting the model prediction
RA, In this paper, we propose to learn smooth continuous quantile functions represented by monotonic rational-quadratic splines, which also naturally solve the quantile crossing problem
RA, Upon investigating several GAN models and architectures, we find that this turns out to be the case
RA, We propose a new class of discrepancies based on the optimal loss for a decision task -- two distributions are different if the optimal decision loss is higher on their mixture than on each individual distribution
RA, We propose a novel algorithm that relies on a weak form of supervision where the data is partitioned into sets according to certain inactive factors of variation
RA,  In this framework, we derive the mathematical representation of the variable space, and then use a tensor network based on the idea of low-rank approximation to model the variable space
RA, This enables humans to reason systematically about novel domains, a problem with which machine learning (ML) models tend to struggle
RA, We derive the optimization objective for Firth penalized multinomial logistic and cosine classifiers, and empirically evaluate that it is consistently effective across the board for few-shot image classification, regardless of (1) the feature representations from different backbones, (2) the number of samples per class, and (3) the number of classes
RA, We present a self-supervised framework iBOT that can perform masked prediction with an online tokenizer
RA, To address this issue, we propose a new federated learning framework called FAFL in which the goal is to minimize the worst-case weighted client losses over an uncertainty set
RA, To tackle these problems, our approach focuses on topics to bridge the semantic gap between these corpora and the target domain corpus, and relates them at a topic level
RA, In this paper, we propose the Environment Dynamics Decomposition (ED2), a novel world model construction framework that models the environment in a decomposing manner
RA, In this paper, we show that besides the imbalanced class volume distribution, other variations such as ethnicity, head pose, occlusion and blur can also significantly affect accuracy
RA, We identify this as an important angle of investigation and propose an evaluation standard that aims to quantify and communicate transfer learning performance in an informative yet accessible setup
RA,We analyze the relationships and shared structure among different prediction tasks on a dataset of retinal images using linear probes: linear regression models trained on some "target'' task, using embeddings from a deep convolutional (CNN) model trained on some "source'' task as input
RA, To address these limitations, this paper presents an adaptive, activation-based, structured pruning approach to automatically and efficiently generate small, accurate, and hardware-efficient models that meet user requirements
RA, We argue that a sensible method for firing off a warning has to both (a) detect harmful shifts while ignoring benign ones, and (b) allow continuous monitoring of model performance without increasing the false alarm rate
RA, In this work we propose six practical extensions of EF21: partial participation, stochastic approximation, variance reduction, proximal setting, momentum and bidirectional compression
RA, This paper proposes an action representation learning framework for offline RL based on a pseudometric, which measures both the behavioral relation and the data-distributional relation between actions
RA,In this work, we empirically study the data scaling properties of neural machine translation (NMT)
RA, These notions allow a new evaluation of  saliency methods, that experimentally provides a novel and stronger justification for several heuristic tricks in the field (T.v. regularization, upscaling)
RA, Our work stands between these two regimes: we introduce a general framework to uplift any MPNN to be more expressive, with limited scalability overhead and greatly improved practical performance
RA,  In this paper, we show how different variants of DRO are simply instances of a finite-sum composite optimization for which we provide scalable methods
RA, To produce more robust and effective attacks, we propose a nested evolutionary algorithm able to produce multi-network (decision-based) black-box adversarial attacks based on Instagram inspired image filters
RA, In this paper, we instead consider the H-entropy, a generalization of Shannon entropy from work in statistical decision theory (DeGroot, 1962; Rao, 1984), which contains a broad class of uncertainty measures parameterized by a problem-specific loss function corresponding to a downstream task
RA, In response, we develop a differential geometry based sampler -coined MaGNET- that, given any trained DGN, produces samples that are uniformly distributed on the learned manifold
RA, In this work, we develop a novel human-in-the-loop learning method called Human-AI Copilot Optimization (HACO)
RA, We design a generative point process model for this case based on a continuous-time Transformer
RA,We extend semi-supervised learning to the problem of domain adaptation to learn significantly higher-accuracy models that train on one data distribution and test on a different one
RA, To address this issue (among other things), two separate strategies have recently been proposed, namely implicit and unfolded GNNs
RA, In this work, we define an extended class of subnetworks in randomly initialized NNs called disguised subnetworks, which are not only "hidden" in the random networks but also "disguised" -- hence can only be "unmasked" with certain transformations on weights
RA, In this work, we propose to model editing processes, modeling the whole process of iteratively generating sequences
RA, Our first contribution is a programmatically interpretable RL framework that conducts program architecture search on top of a continuous relaxation of the architecture space defined by programming language grammar rules
RA, In this paper, we firstly conduct a retrospective analysis on aforementioned assumptions, through which we indicate the imminent aspiration for an authentically practical-oriented network in reconstructive community
RA, We show that a simple transfer-learning based approach can be used to train adversarially robust few-shot classifiers
RA, We propose a benchmark of three increasingly complex cross-domain imitation learning task, ranging from simple rigid transformation of the expert domain to arbitrary transformation of the state and action spaces
RA, In this paper, we propose a two-stage framework based on distillation that realizes the modelling benefits of the large models, while largely preserving the computational benefits of inference with more lightweight models
RA, In this paper, we study this through the lens of kernel methods, by considering simple hierarchical kernels with two or three convolution and pooling layers, inspired by convolutional kernel networks
RA, Instead, we propose a more direct way to make an image sharper by exploiting the inverse task of deblurring, namely reblurring
RA, In this work, we introduce a strong image matching learner, dubbed Visual Transformatcher, which builds on the success of the Transformers in vision domains
RA,We introduce Particle-SDCA, a gradient-based optimization algorithm for two-layer neural networks in the mean field regime that achieves exponential convergence rate in regularized empirical risk minimization
RA, In this paper, we thoroughly compare the continual learning performance over the combination of 5 PLMs and 4 veins of CL methods on 3 benchmarks in 2 typical incremental settings
RA, We propose a containerized learning framework to solve these problems
RA, This upper bound is minimax optimal and improves the previous O(\kappa^2) upper bound (HaoChen et al, 2020)
RA, In this paper, we propose SeqPATE that adapts PATE to text generation while satisfying DP
RA, We propose Hardware-Aware Latency Pruning (HALP) that formulates structural pruning as a global resource allocation optimization problem, aiming at maximizing the accuracy while constraining latency under a predefined budget
RA, In this work, we propose the physics-informed neural operator (PINO), where we combine the operating-learning and function-optimization frameworks, and this improves convergence rates and accuracy over both PINN and FNO models
RA, To fill this gap, we propose Human-NN-Interface (HNI), a framework using a structural representation of visual concepts as a ”language” for humans and NN to communicate, interact, and exchange knowledge
RA, In this paper, we propose a principled framework for adaptive RL, called AdaRL, that adapts reliably and efficiently to changes across domains with a few samples from the target domain, even in partially observable environments
RA,We propose a framework that protects against sensitive information leakage to facilitate data release with untrusted parties
RA,In this paper, we demonstrate that self-learning techniques like entropy minimization or pseudo-labeling are simple, yet effective techniques for increasing test performance under domain shifts
RA, Based on the insights drawn, we have developed a dedicated attack framework, dubbed Patch-Fool, that fools the self-attention mechanism by attacking the basic component (i,e, a single patch) participating in self-attention calculations with a series of attention-aware optimization techniques
RA, In this paper, we propose data augmented invariant regularization (DAIR)
RA, To this end, we propose RVFR, a novel robust VFL training and inference framework
RA, We propose a MUlti-Subspace structured Meta-Learning (MUSML) algorithm to learn the subspace bases
RA, We further study the overlap of pruned models for similar tasks and how the overlap changes for different layers
RA, In this paper, we propose switch-GLAT, a non-autoregressive multilingual machine translation model with a code-switch decoder
RA, To address these issues, we propose AdaTime, a standard framework to systematically and fairly evaluate different domain adaptation methods on time series data
RA, To cope with this challenge, we propose the Graph Multi-View Pre-training (GraphMVP) framework where self-supervised learning (SSL) is performed by leveraging the correspondence and consistency between 2D topological structures and 3D geometric views
RA, This raises an important problem: how to prevent these models from being maliciously copied when they are running on customers' computing device? We answer this question by adding a set of confusion neurons into the pre-trained model, where the position of these neurons is encoded into a few integers that are easy to be encrypted
RA, In this work, we investigate these two aspects and propose an integrated framework, DM-CT, that incorporates both the data-level and model-level consistency training
RA, Specifically, we show that attention builds upon a long history of prior work on manifold learning and image processing, including methods such as kernel-based regression, non-local means, locally linear embedding, subspace clustering and sparse coding
RA, In this paper, we introduce and study the EI technique as a new tool for the contextual bandit problem which is a generalization of the standard bandit
RA, We propose Autoregressive Quantile Flows, a flexible class of probabilistic models over high-dimensional variables that can be used to accurately capture predictive aleatoric uncertainties
RA, By investigating the connection between contrastive learning and neighborhood component analysis (NCA), we provide a novel stochastic nearest neighbor viewpoint of contrastive learning and subsequently propose a series of contrastive losses that outperform the existing ones
RA, In this work, we propose a data-efficient generative model that can be learned from datasets with orders of magnitude smaller sizes than common benchmarks
RA, In this paper, we first propose a new fairness goal, termed Equalized Robustness (ER), to impose fair model robustness against unseen distribution shifts across majority and minority groups
RA, In this paper, to further improve the performance of CL and enhance its robustness on uncurated datasets, we propose a doubly CL strategy that contrasts positive samples and negative ones within themselves separately
RA, In this paper, we first propose a new collaboration criterion to evaluate collaboration from three perspectives, which arrives at a form of the mutual information between global state and joint policy
RA, In this paper, we present a new multi-modal framework named SFD (Sparse Fuse Dense) to tackle these issues
RA, To address this issue, we propose a method that transforms conditional moment restrictions to unconditional moment restrictions through importance weighting using a conditional density ratio estimator
RA, We present a novel approach that relies only on a form of a distribution alignment but no sampling strategy where rather than aligning the pseudo-labels during inference, we move the distribution alignment component into the respective cross entropy loss computations for both the supervised and unsupervised losses
RA, We convert a large set of supervised datasets,  each with multiple prompts using varying natural language
RA, Unlike most existing approaches, instead of constructing a single hypothesis shared among domains, we propose a LAtent Sub-Space Orientation (LASSO) method that explores diverse latent sub-spaces and learning individual hypotheses on those sub-spaces
RA, In turn, we develop a new ensemble-based procedure for semi-supervised novelty detection (SSND) that only utilizes a mixture of unlabeled ID and OOD samples to achieve good detection performance on near OOD data
RA, To achieve such interactions, traditional approaches require hand-designed features and object representations, and it still remains an open question how to describe such interactions with arbitrary objects in a flexible and efficient way
RA, In this paper, we propose a novel algorithm LookSAM to significantly reduce its additional training cost
RA, We start from proposing a generative framework that trains an energy-based model with a normalizing flow as an amortized sampler to rapidly initialize the MCMC chains of the energy-based model
RA, We show that tabular Q-learning in discrete Markov decision processes (MDPs) learns the same value function under any arbitrary refinement of the action space
RA, Correspondingly, we propose a general multi-task oriented generative modeling (MGM) framework, by coupling a discriminative multi-task network with a generative network
RA,We present a new method LiST for efficient fine-tuning of large pre-trained language models (PLMs) in few-shot learning settings
RA, In a departure from deep RL approaches, in this paper, we propose a general method inspired by case-based reasoning to train agents and generalize out of the training distribution
RA, In contrast to prior methods that learn a monolithic function to approximate the value, we propose a bi-linear decomposition of the value function
RA, Our method, ``Pareto policy pool (P3)'', does not need to tune the trade-off weight but can produce policies allocated at different regions of the Pareto front
RA, We propose new aggregation rules, Tmean(·), to the federated learning algorithm, and propose a federated learning framework based on Byzantine-resilient aggregation algorithm
RA, In this paper, we propose a generative model to automatically design the CDRs of antibodies with enhanced binding specificity or neutralization capabilities
RA, This paper proposes a unified ViT compression framework that seamlessly assembles three effective techniques: pruning, layer skipping, and knowledge distillation
RA,In this paper, we propose Multiresolution Equivariant Graph Variational Autoencoders (MGVAE), the first hierarchical generative model to learn and generate graphs in a multiresolution and equivariant manner
RA,We propose a principled method to learn a set of human-readable logic rules to explain temporal point processes
RA, In this paper we present a powerful approach to learning minimal representations
RA,  In this work, we generalize positional encoding with Fourier features to non-Euclidean manifolds
RA, To address these issues, we introduce a new image synthesis and editing method, Stochastic Differential Editing (SDEdit), based on a diffusion model generative prior, which synthesizes realistic images by iteratively denoising through a stochastic differential equation (SDE)
RA, We observe that in logical reasoning, logical rules (e,g,, my parent's parent is my grandparent) usually apply locally (e,g,, only three people are involved in a grandparent rule), and sparsely (e,g,, the grandparent relationship is sparse across all pairs of people in the world)
RA, This paper introduces a new topic-modeling framework where each document is viewed as a set of word embedding vectors and each topic is modeled as an embedding vector in the same embedding space
RA, We find that all linear performance leaders effectively add only a nuclear-norm based regularizer, or a Frobenius-norm based regularizer
RA, Here, we propose that such cells can serve as prototype memory priors that bias and shape the distributed feature processing within the image generation process in the brain.
RA, To this end, we first propose a framework based on step-wise energy-based models (EBMs) that is efficient in sampling and flexible in a wide range of practical CTG scenarios
RA, We begin by showing on 4 benchmark datasets that out-of-the-box contrastive pre-training (even without large-scale unlabeled data) is competitive with other UDA methods
RA, In this work, we introduce LASSI, the first representation learning method for certifying individual fairness of high-dimensional data
RA, In this work, we investigate a simple soft-greedy operator, which we call resmax, that takes actions proportionally to their suboptimality gap: the residual to the estimated maximal value
RA,  We propose Interactive Neural Process (INP),  a Bayesian active learning framework to proactively learn a deep learning surrogate model and accelerate simulation
RA, To directly tackle the empirical distribution mismatch problem, we propose posterior transition matrix (PTM) to posteriorly model label noise given limited observed noisy labels achieving statistically consistent classifiers
RA, This paper proposes a novel anomaly segmentation network (AnoSeg) that can directly generate an accurate anomaly map using self-supervised learning
RA, Our paper investigates a new setting in active learning---how to conduct active learning without relying on pre-labeled data, which is an under-explored yet of great practical value
RA,We develop a fast and reliable method for solving large-scale optimal transport (OT) problems at an unprecedented combination of speed and accuracy
RA, This paper proposes the amortized Langevin dynamics (ALD), wherein datapoint-wise MCMC iterations are entirely replaced with updates of an inference model that maps observations into latent variables
RA,We introduce a general approach, called invariance through inference, for improving the test-time performance of a behavior agent in deployment environments with unknown perceptual variations
RA, In this work, we propose the repelling parasiamese neural network, a model which considers a siamese network for synonymy and a parasiamese network for antonymy, both sharing the same base network
RA, We show that for a large class of structured inputs, such as combinations of low-rank matrices, sparse matrices, and Vandermonde matrices, L_p regression can be approximately solved using runtime that is polynomial in p
RA, This paper studies this problem and proposes a novel method using the variance of payoff functions to construct context-aware sparse coordination topologies
RA, In this paper, we develop an automatic procedure for finding directions that lead to foreground-background image separation, and we use these directions to train an image segmentation model without human supervision
RA, Particularly, we propose a simple meta-algorithm such that given any reward-free RL oracle, the approachability and constrained RL problems can be directly solved with negligible overheads in sample complexity
RA, We circumvent this challenge by using additional data from proxy distributions learned by advanced  generative models
RA, In this paper, we propose Decentralized CEM (DecentCEM) where an ensemble of CEM instances run independently from one another and each performs a local improvement of its own sampling distribution
RA, To be useful to a practitioner, these methods must identify slices that are both underperforming and coherent (i,e, united by a human-understandable concept)
RA, We introduce two alternatives for sparse adversarial training: (i) static sparsity, by leveraging recent results from the lottery ticket hypothesis to identify critical sparse subnetworks arising from the early training; (ii) dynamic sparsity, by allowing the sparse subnetwork to adaptively adjust its connectivity pattern (while sticking to the same sparsity ratio) throughout training
RA, This means that an adversarial crafted image which is sufficiently close (visually indistinguishable) to its representative class can often be misclassified to be a member of a different class
RA, Our proposed module PKCAM is easily integrated into any feed-forward CNN architectures and trained in an end-to-end fashion with a negligible footprint due to its lightweight property
RA, Our algorithm, called eXploit-Then-eXplore (XTX), begins each episode using an exploitation policy that imitates a set of promising trajectories from the past, and then switches over to an exploration policy aimed at discovering novel actions that lead to unseen state spaces
RA, In this work, we present the first study that analyzes and models adversarial attacks based on physical domain constraints in EEG-based BCIs
RA, To this end, we propose an unsupervised RL method designed for high-dimensional, stochastic environments based on an adversarial game between two policies (which we call Explore and Control) controlling a single body and competing over the amount of observation entropy the agent experiences
RA, To better understanding why these approaches work we study the interplay of the policy and value networks in A*-based deep RL and show the surprising effectiveness of the policy network, further enhanced by the value network, as a guiding heuristic for A*
RA, We show that bounded-norm Transformer layers create sparse variables: they can represent sparse Lipschitz functions of the input sequence, with sample complexity scaling only logarithmically with the context length
RA, We formulate the latter as estimating the mutual information between the representation and a space of manually labelled concepts
RA,We present a conditional variational auto-encoder (VAE) which, to avoid the substantial cost of training from scratch, uses an architecture and training objective capable of leveraging a foundation model in the form of a pretrained unconditional VAE
RA, This is different from human evaluation manners, in which people also form pictures of the text contents in their minds during reading
RA, To address this problem, we propose Cross-Layer Attention (CLA) module in this paper
RA, This enables applications of machine learning in settings of inherently distributed, undisclosable data such as in the medical domain
RA,We propose GRAph Neural Diffusion with a source term (GRAND++) for graph deep learning with a limited number of labeled nodes, i,e, low-labeling rate
RA, By focusing on shape alignment, rather than semantic cues, we can achieve across category generalization and scaling
RA, In this paper, we leverage insights from the scalarization technique in differential geometry to model many-body systems by learning the gradient vector fields, which are SE(3) and permutation equivariant
RA, In this work, we propose the first scalable and precise monDEQ verifier, based on two key ideas: (i) a novel convex relaxation which enables efficient inclusion checks, and (ii) non-trivial mathematical insights characterizing the fixpoint operations at the heart of monDEQs on sets rather than concrete inputs
RA, We observe that solutions to these polynomial graph filter models are also solutions to an overdetermined system of equations
RA, In this paper, to address the above two challenges, a novel {\em Global Segmentation Mask Transformer (GSMT) is proposed
RA, We formalize these concepts in a three-step data-centric SSL method that improves FixMatch in stability and accuracy by 8% on CIFAR-10 (0.08% labeled) and 14% on ImageNet-1K (0.2% labeled)
RA, In this paper, we observe an inherent property of the NMT system, that is, NMT systems’ efficiency is related to the output length instead of the input length
RA, In this work, we analyze the limitations according to which previous alignments become very resource-intensive, viz,, (i) the inability to sufficiently leverage data and (ii) that alignments are not trained properly
RA, To address this issue, we propose a fully unsupervised technique for inverse problem solving, leveraging the recently introduced score-based generative models
RA, We propose the first certification framework COPA to certify the number of poisoning trajectories that can be tolerated regarding different certification criteria
RA, To overcome this challenge, we propose a spatially hierarchical reinforcement learning method for state space and policy space
RA, In this work, we carefully analyze these challenges and find configurations where training larger models becomes less communication-intensive
RA, In this work, we propose stochastic reweighted gradient (SRG), a variance-reduced stochastic gradient method based solely on importance sampling that can improve on the asymptotic error of stochastic gradient descent (SGD) in the strongly convex and smooth case
RA, In this work, we propose a method for jointly inferring labels across a collection of data samples, where each sample consists of an observation and a prior belief about the label
RA, To this end, we propose \mathcal{G-Mixup to augment graphs for graph classification by interpolating the generator (i,e, graphon) of different classes of graphs
RA, We build a generative environment model for the structural relationships among variables in the system and present a principled way to characterize ASRs based on structural constraints and the goal of maximizing cumulative reward in policy learning
RA, To improve the prediction accuracy, many variants of NPs have investigated context embedding approaches that generally design novel network architectures and aggregation functions satisfying permutation invariant
RA, In this paper, we propose a new look at the hyperparameter selection problem in ensemble models
RA, In this paper, we propose Non-Transferable Learning (NTL), a novel approach that captures the exclusive data representation in the learned model and restricts the model generalization ability to certain domains
RA,We present a method for composing photorealistic scenes from captured images of objects
RA, In this work, we propose a novel framework, named DiffSkill, that uses a differentiable physics simulator for skill abstraction to solve long-horizon deformable object manipulation tasks from sensory observations
RA, We ask: (1) how useful are existing domain generalization methods for tackling distribution shifts on graph data? (2) are GNNs capable of generalizing to test graphs from unseen distributions? As a first step to answering these questions, we curate GDS, a benchmark of 8 datasets reflecting a diverse range of distribution shifts across graphs
RA, Inspired by distributional and state-marginal matching literatures in RL, we demonstrate that all these approaches are essentially doing hindsight information matching (HIM) -- training policies that can output the rest of trajectory that matches a given future state information statistics
RA, In this paper we present the first provable guarantees for one of the best-studied families of graph neural network models, Graph Convolutional Networks (GCNs), for semi- supervised community detection tasks
RA,This paper provides a novel framework that learns canonical embeddings for non-rigid shape matching
RA, To mitigate the above issues, we propose a module named Mobile Attention Kernel Point Convolution (MAKPConv) to improve the efficiency and quality of KPConv
RA, We propose a method for comparing two data representations
RA, In this paper, we propose {\bf Circulant MinHash (C-MinHash) and provide the surprising theoretical results that using only two independent random permutations in a circulant manner leads to uniformly smaller Jaccard estimation variance than that of the classical MinHash with K independent permutations
RA, We introduce LatentKeypointGAN, a two-stage GAN which is trained end-to-end on the classical GAN objective with internal conditioning on a set of space keypoints
RA, We use a latent variable to model a prognostic score which is widely used in biostatistics and sufficient for TEs; i,e, we build a generative prognostic model
RA, In this paper, instead of modeling the distribution shift with a block-cyclic pattern as previous works, we model it with a mixture of distributions that gradually changes between daytime modes and nighttime modes, and find this intuitive model to better match the observations in practical federated learning systems
RA, Therefore, instead of using such categorical labels, we define a node distance between WL subtrees with tree edit distance and propose an efficient calculation algorithm
RA, Intuitively, we favor local minima with a small loss in a neighborhood around them
RA, This is achieved through an equivariant Lie algebraic parameterization of state and action encodings, equivariant latent transition models, and the use of symmetry-based losses
RA, Is it possible to formalize these conceptual benefits and devise algorithms to use offline datasets to yield provable improvements to the sample-efficiency of imitation learning? In this work, we answer this question affirmatively and present training objectives which use an offline dataset to learn an approximate factored dynamics model whose structure enables the extraction of a latent action space
RA, In this work, we relax such uniform alignment by using a domain graph to encode domain adjacency, e,g,, a graph of states in the US with each state as a domain and each edge indicating adjacency, thereby allowing domains to align flexibly based on the graph structure
RA,In this paper, we analyze the number of neurons and training parameters that a neural network needs to approximate multivariate functions of bounded second mixed derivatives --- Korobov functions
RA, This requires strong locality properties from the representation space, ie, the close allocations of each small group of relevant texts, which is hard to generalize to domains without sufficient training data
RA, We propose a taxonomy of settings, where each setting is described as a set of assumptions
RA, Hence, we propose gMORE, a method that is similar to MORE but differs by incorporating gradient information
RA, Specifically, our method, Learning to Prompt for Continual Learning  (L2P), prepends a subset of learnable parameters (called Prompts) from a larger set (called Prompt Pool) to the input embeddings
RA, Moreover In this generalized framework, we establish that even asymptotic last iterate or time average convergence to a Nash Equilibrium is not possible using Gradient Descent Ascent (GDA), its optimistic variant and extra gradient
RA, To this end, we investigate Predictive Consistent Representations (PCR) that enforces predictive consistency on a learned dynamic model
RA, We propose Supernet with Unbiased Meta-Features for Neural Architecture Search (SUMNAS), a supernet learning strategy based on meta-learning to tackle the knowledge forgetting issue
RA, In this work, we systematically investigate self-supervised learning under dataset imbalance
RA, This paper proposes a general self-supervised regression learning (SSRL) framework that enables learning regression neural networks with only input data (but without ground-truth target data), by using a designable operator that encapsulates domain knowledge of a specific application
RA, Based on this result, we propose surface smoothing adversarial training (SSAT)
RA, In this paper, we propose a challenging and untouched problem: Open-Set Single Domain Generalization (OS-SDG), where target domains include unseen categories out of source label space
RA, To bypass this fundamental hardness, this paper proposes a novel method, named Self-Organized Polynomial-time Coordination Graphs (SOP-CG), which uses structured graph classes to guarantee the optimality of the induced DCOPs with sufficient function expressiveness
RA,We introduce reducible held-out loss selection (RHOLS), a technique for faster model training which selects a sequence of training points that are “just right”
RA, In this work, instead of selecting a set of hand-picked default augmentations alongside the searched data augmentations, we propose a fully automated approach for data augmentation search called Deep AutoAugment (DeepAA)
RA, In this paper, we address these shortcomings by introducing a new class of concatenated kernel learning methods that use the kernels from the reproducing kernel Banach spaces(RKBSs)
RA, We then assess a variety of generative models of embedding spaces by these criteria, and conclude that incremental insertion processes based on the Barabási-Albert network generation process best model the observed phenomenon on language and network data
RA, We then further investigate why pre-training alleviates forgetting in this setting
RA, To enable better performance, we investigate the offline-online setting: The agent has access to a batch of data to train on but is also allowed to learn during the evaluation phase in an online manner
RA, Therefore, naturally, the generative model needs to produce not only relevant samples, but also those that are sufficiently rich for classifier training purposes, which is handled by various heuristics in existing works
RA, We introduce weakly supervised signals to learn disentangled representation and use contrastive methods to enforce invariant representation
RA, In this work, we propose FrugalMCT, a principled framework that adaptively selects the APIs to use for different data in an online fashion while respecting the user’s budget
RA, In this paper, we introduce a relational graph generative process to model how the observed edges are generated by aggregating the node interactions over multiple overlapping node communities, each of which represents a particular type of relation that contributes to the edges via a logical OR mechanism
RA, However, we found that the current approach of jointly training experts and the sparse gate introduces a negative impact on model accuracy, diminishing the efficiency of expensive large-scale model training
RA, We propose a new framework, Bayes augmented with memory (BAM), that takes advantage of past experience by allowing the agent to choose which past observations to remember and which to forget
RA, In this paper, we present evidences that this argument is actually inaccurate because of using improperly small finetuning learning rates
RA, We show that this asymmetry in the number of positive and negative quantization levels can result in significant quantization error and performance degradation at low precision
RA, We explore a simple modification to supervised contrastive loss that prevents class collapse by uniformly pulling apart individual points from the same class
RA, In this work, we present a domain adaptation approach that improves generalization of the identification module by leveraging prior knowledge in physics
RA, To address these limitations, we introduce the Maximum Relative Error (MRE) between the target distribution and the transported distribution computed by an SDOT map
RA, In this work, we investigate methods for predicting the target domain accuracy using only labeled source data and unlabeled target data
RA, In this work, we propose the Intervention-based Recurrent Casual Model (IRCM) for non-stationary video casual discovery
RA, In this work, we propose an algorithm that trains an intermediary policy in the learner environment and uses it as a surrogate expert for the learner
RA, Following this, we introduce DisTop, a new model that simultaneously learns diverse skills and focuses on improving rewarding skills
RA,We propose SEAL which utilizes this energy network as a trainable loss function for a simple feedfoward network
RA, In this paper, we explicitly capture the aleatoric uncertainty from a distributional perspective and propose an information-theoretic exploration method named Optimistic Value Distribution Explorer (OVD-Explorer)
RA, In this work, we investigate the origins of the less studied epoch-wise double descent in which the test error undergoes two non-monotonous transitions, or descents as the training time increases
RA, We propose six rules as a guideline for experimental design and execution to conduct robust continual learning evaluation for better understanding of the methods
RA, We investigate these effects by studying adversarial robustness--a local generalization property--to reveal hard, model-specific instances and spurious features
RA, In this paper, we introduce a new general framework for improving coordination and performance of multi-agent reinforcement learners (MARL)
RA,In this paper we develop a novel regularization method for deep neural networks by penalizing the trace of Hessian
RA, We formulate this as a generalization of optimal transport with an entropy bottleneck to account for the rate constraint due to compression
RA, However, we propose that forgetting can in fact be favorable to learning
RA, We explored inductive biases that allowed artificial neural networks to learn these transformations in pixel space in a way that could generalize out-of-distribution (OOD)
RA, In this paper, we propose a new approach for hierarchical forecasting which consists of two components
RA, This results in generating generic summaries that do not cater to the users needs or preferences
RA, In this study, we propose a hybrid method to take the best of both worlds
RA, This architecture results in computationally efficient rapid scene exploration
RA, We study the generalization properties of the maximum entropy method for solving the inverse reinforcement learning problem for both exact and approximate formulations and demonstrate that by applying an instantiation of the invariant risk minimization principle, we can recover reward functions which induce better performing policies across domains in the transfer setting
RA, We argue that current SGMs employ overly simplistic diffusions, leading to unnecessarily complex denoising processes, which limit generative modeling performance
RA, This method is however limited to strictily models, where each prediction is associated with a small receptive field
RA,  We propose the Structured State Space (S3) model based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths
RA, In this paper we present a novel method that learns meaningful representations from videos, titles and comments, which are abundant on the internet
RA, In this work, we mitigate these issues by finding a scalar, domain-agnostic, and scalable metric for evaluating and ranking GGMs
RA,We show how to derive state-of-the-art unsupervised neural machine translation systems from generatively pre-trained language models
RA, In this paper, we address the setting where the target domain has only limited labeled data from a distribution that is expected to change frequently
RA, We use TorchGeo to create reproducible benchmark results on existing datasets, benchmark our proposed method for preprocessing geospatial imagery on-the-fly, and investigate the differences between ImageNet pre-training and in-domain self-supervised pre-training on model performance across several datasets
RA, In this paper, we propose Feature-Augmented Hypergraph Neural Networks (FAHGNN) focusing on hypergraph structures
RA,  In this paper, we model the dynamics change as the variation of unobserved environment-specified factors Z across environments
RA, We term this kind of imitation learning ``imitation-learning-with-extraneousness'' and introduce Extraneousness-Aware Imitation Learning (EIL), a self-supervised approach that learns visuomotor policies from third-person demonstrations where extraneous subsequences exist
RA, To demonstrate this, we first present formal definitions for consistency and coherence, and a proposed Dynamic Comparator relation-decoder model designed around these principles
RA, This builds on -- and is a stronger version of -- the observation in Nakkiran&Bansal 20, which requires the runs to be on separate training sets
RA, In this work, we revisit GNNs that allow using positional features of nodes given by positional encoding (PE) techniques such as Laplacian Eigenmap, Deepwalk, etc
RA, In this paper, we revisit the idea of kernel pruning (to only prune one or several k \times k kernels out of a 3D-filter), a heavily overlooked approach under the context of structured pruning due to it will naturally introduce sparsity to filters within the same convolutional layer—thus, making the remaining network no longer dense
RA, In addition to these measures, we show that balancing constraints on predicted class labels and decision boundaries are beneficial
RA, To address this, we propose a rigorous technique to select subsets of data points that when augmented, closely capture the training dynamics of full data augmentation
RA,We present a generic method for recurrently using the same parameters for many different convolution layers to build a deep network
RA, We analyze the pruned network in the context of the properties of Ramanujan expander graphs
RA, In this work, we design an agent's curriculum by focusing on the aspect of goal reachability, and introduce the idea of a reachability trace, which is used as a basis to determine a sequence of intermediate subgoals to guide the agent towards its primary goal
RA, We find a critical piece missing in the current understanding of information retrieval (IR) systems: as interventions, recommendation not only affects the already observed data, but it also interferes with the target domain (distribution) of interest
RA, The key in the latter problem, which we call UserRec, is to recommend users in their temporal contexts on behalf of the item providers
RA, To provide detailed change assessment, we model MLAPI shifts as confusion matrix differences, and propose a principled algorithmic framework, MASA, to provably assess these shifts efficiently given a sample budget constraint
RA, To learn the distribution over the global prototypes, we minimize its OT distance to the set empirical distribution over data points, providing a natural unsupervised way to improve the summary network
RA, First, we design algorithms for learning an \epsilon-Coarse Correlated Equilibrium (CCE) in \mathcal{O(H^5S\max_{i\le m A_i / \epsilon^2) episodes, and an \epsilon-Correlated Equilibrium (CE) in \mathcal{O(H^6S\max_{i\le m A_i^2 / \epsilon^2) episodes
RA, We then propose Cold Brew, a new method that generalizes GNNs better in the SCS setting compared to pointwise and graph-based models, via a distillation approach
RA, In this work, we formalize the notion of user-specific cost functions and introduce a new method for identifying actionable recourses for users
RA, Under this interpretation, we can derive the probability P(x_0 \cap x_1) that a pair of independent features are both present in the stimulus from their logits
RA, Our key insight is that systematic suboptimality can be modeled by predicting policies, which couple action choices over time, instead of trajectories
RA, We explore the relevance of the feature kernel for Knowledge Distillation (KD), using a mechanistic understanding of an NN's optimisation process
RA, We propose an attention encoder-LSTM decoder hybrid model, in which the decoder's hidden state can represent the sequence of actions made
RA, To solve this challenge, a reconstruction network is built before the public pre-trained classifiers to offer certified robustness and defend against adversarial examples through input perturbation
RA, To improve RL's efficiency and generalization to varying environments, we study how to automatically generate a curriculum of tasks with coupled environments for RL
RA, In this paper, we propose a novel transformer-based Updater-Extractor architecture that can work with sequences of arbitrary length and refine its long-term knowledge about the world based on inputs at application time
RA, In this work, we propose a unified evaluation suite, Sneakoscope, to revisit the problem with in-depth exploration of unsupervised OOD detection
RA, For this latter setting, we propose Correct-N-Contrast (CNC), a contrastive learning method to train models more robust to spurious correlations
RA, In this paper, we propose a multi-layer perceptron (MLP) architecture, namely speech-MLP, useful for extracting information from speech signals
RA, We design a novel pairwise-independent SE(3)-equivariant graph matching network to predict the rotation and translation to place one of the proteins at the right location and the right orientation relative to the second protein
RA, To properly connect the prediction loss with the optimization goal, in this paper we propose a total group preorder (TGP) loss and its differential version called approximated total group preorder (ATGP) loss for predict-then-optimize (PTO) problems with strong ranking property
RA, We present LARC, the Language-complete ARC: a collection of natural language descriptions by a group of human participants  who instruct each other on how to solve ARC tasks using language alone, which contains successful instructions for 88% of the ARC tasks
RA, We propose a novel adversarial training framework that learns to reweight the loss associated with individual training samples based on a notion of class-conditioned margin, with the goal of improving robust generalization
RA, In this work, we present methods to combine the benefits of full and lightweight finetuning, achieving strong performance both ID and OOD
RA, In this work, we propose an interpretable neural network that leverages metric and prototype learning for classification tasks
RA, Here we show that temporal coding such as rank coding (RC) inspired by SNNs can also be applied to conventional ANNs such as LSTMs, and leads to computational savings and speedups
RA, In this paper, we discuss several key issues and a new procedure for obtaining efficient networks that minimize total number of parameters and computation requirement
RA, We propose in this paper RankedDrop a novel method with a spatial-aware dropping-edge selection
RA, In this paper, we explore a relaxed-smoothness assumption of the loss landscape which LSTM was shown to satisfy in previous works, and design a communication-efficient gradient clipping algorithm
RA, Specifically, we first propose a strategy to measure the data quality based on the learning behaviors of the data during adversarial training and find that low-quality data may not be useful and even detrimental to the adversarial robustness
RA, To understand reward hacking, we construct four RL environments with different misspecified rewards
RA, In this work, we introduce the Fairness Calibration (FairCal) method, a post-training approach that simultaneously: (i) increases model accuracy (improving the state-of-the-art), (ii) produces fairly-calibrated probabilities, (iii) significantly reduces the gap in the false positive rates, (iv) does not require knowledge of the sensitive attribute, and (v) does not require retraining, training an additional model or retuning
RA, Our key observations are that (1) such performance improvements are well-approximated by power laws (linear log-log plots) as the training set size increases, (2) this applies to both cases of target data coming from either the same or from a different domain (i,e, new classes) as the training data, and (3) few-shot performance on new classes converges at a faster rate than the standard classification performance on previously seen classes
RA, To address this challenge, we investigate the effect of network architecture on the propensity of learning algorithms to make use of these relationships in human-compatible ways
RA, To handle this imbalanced problem, we propose a novel image sequence retrieval framework that utilizes scene graph similarities of the images and a dual learning scheme
RA, To ensure high inference accuracy in the new environment, it is indispensable to adapt the compact model to the target data
RA, We show that multi-agent deep reinforcement learning (RL) can discover stable solutions that are \epsilon-Nash equilibria for a meta-game over agent types, in economic simulations with many agents, through the use of structured learning curricula and efficient GPU-only simulation and training
RA, We first define the Neural Fisher Kernel (NFK), which is the Fisher Kernel applied to neural networks
RA, Specifically, we consider the framework of generalized policy evaluation and improvement, in which the rewards for all tasks of interest are assumed to be expressible as a linear combination of a fixed set of features
RA, Adding NormFormer on top of the GPT3-Medium architecture can reach the SOTA perplexity 22% faster, or converge 0.33 perplexity better in the same compute budget
RA, In this paper, we propose a Gesture2Vec model using representation learning methods to learn the relationship between semantic features and corresponding gestures
RA, To address this deficiency, we present a simple and effective architecture modification to ViT's input layer by adding discrete tokens produced by a vector-quantized encoder
RA, To this end, the Gromov-Wasserstein (GW) distance, based on Optimal Transport (OT), has proven to be successful in handling the specific nature of the associated objects
RA, Concretely, we propose a novel federated learning framework that explicitly decouples a model's dual duties with two prediction tasks
RA, Our proposition consisting of a simple deep learning architecture that learns with novel RL training techniques exploits two main ideas
RA, In this work, we present SHAC, a short-horizon actor-critic method that successfully leverages parallel differentiable simulation to accelerate policy learning
RA, This paper revisits the approach from a matrix approximation perspective
RA, In this work, we propose a framework for disentangling these performance gaps
RA, To tackle the combinatorial nature of composing features, we propose a compositional approach to steering music transformers, building on lightweight fine-tuning methods such as prefix tuning and bias tuning
RA, In this work, we propose a query-based framework that trains a query neural network to generate informative input-output examples automatically and interactively
RA, To resolve this, we propose FedFB, a private fair learning algorithm on decentralized data with a modified FedAvg protocol
RA, In this work, we present an in-depth analysis of popular NAS algorithms and performance prediction methods across 25 different combinations of search spaces and datasets, finding that many conclusions drawn from a few NAS benchmarks do not generalize to other benchmarks
RA,We introduce Contrastive Intrinsic Control (CIC) - an algorithm for unsupervised skill discovery that maximizes the mutual information between skills and state transitions
RA, In this paper, we observe a new issue in deeper GNNs, i,e, feature overcorrelation, and perform a thorough study to deepen our understanding on this issue
RA,We propose a reinforcement learning based approach to the problem of query object localization, where an agent is trained to localize objects of interest specified by a small exemplary set
RA, This differs from human decision-making, where gains and losses are valued differently and outlying outcomes are given increased consideration
RA, To this end, we formulate and study the directional domain generalization (DDG) scenario, which exploits not only the source data but also their evolving pattern to generate a model for the unseen task
RA, This distance sensitivity with respect to the data aids in tasks such as uncertainty calibration and out-of-distribution (OOD) detection
RA, Here, we propose an unsupervised variational framework using multiple interacting networks called cpl-mixVAE that significantly outperforms state-of-the-art in high-dimensional discrete settings
RA,  In this work we propose a novel network for unsupervised disentanglement that combines the stable training of the VAE with the interpretability offered by GANs without the training instabilities
RA, In this work, we leverage the unique characteristics of GNNs to overcome these overheads, creating efficient ensemble GNNs that are faster than even single models at inference time
RA, In this work, we leverage the range image representation and propose a novel deep delta encoding model to compress lidar data
RA,  We show that in the underparameterized regime the network learns eigenfunctions of an integral operator T_K determined by the Neural Tangent Kernel at rates corresponding to their eigenvalues
RA, Hence our methods can be applied to study the generalization error of infinitely wide neural networks
RA, To address it, we propose to calibrate the similarity for probabilistic embeddings
RA, In this work, we introduce the multi-label box model (MBM), a multi-label classification method that combines the encoding power of neural networks with the inductive bias and probabilistic semantics of box embeddings (Vilnis, et al 2018)
RA, Based on this discrepancy, we propose a novel DR and visualization method called Space-based Manifold Approximation and Projection (SpaceMAP)
RA, In this paper, we investigate the role of dataset properties and augmentation strategies on the success of GCL and reconstruction-based approaches
RA, Our proposed algorithm improves upon the robustness certificate of this algorithm by up to 8\times while with no additional computational cost
RA, In this paper, we take a first step in developing a gradient-based learning framework named DAG-Shared Federated Causal Discovery (DS-FCD), which can learn the causal graph without directly touching local data and naturally handle the data heterogeneity
RA,We introduce a memory-driven semi-parametric approach to text-to-image generation, which is based on both parametric and non-parametric techniques
RA,We introduce Explanatory Learning (EL), an explanation-driven machine learning framework to use existing knowledge buried in symbolic sequences expressed in an unknown language
RA, Inspired by the diffusion process in classical non-equilibrium thermodynamics where heated particles will diffuse from original states to a noise distribution, in this paper, we propose a novel generative model named GeoDiff for molecular conformation prediction
RA, In this paper, we benchmark popular temporal LVMs against state-of-the-art deterministic models on speech
RA, This significantly reduces the amount of computation required and leads to large speedups in training and inference, while achieving similar accuracy as the baseline MLP
RA, To address this challenge, we propose a contact point discovery approach (CPDeform) that guides the stand-alone differentiable physics solver to deform various soft-body Plasticine
RA, To this end, we leverage the second-order uncertainty representation provided by evidential models and we introduce KLoS, a Kullback–Leibler divergence criterion defined on the class-probability simplex
RA, This paper proposes a method by which a neural network is represented in terms of an embedding of the neurons rather than explicit weights
RA, To confront this challenge, we study the FSL problem from a geometric point of view in this paper
RA, This makes it challenging to leverage features like loops and motifs associated with network formation mechanisms
RA, To this end, we propose a novel policy architecture that consists of an input graph composed of available actions and a graph attention network to learn the action interdependence
RA,We present a framework for learning compositional, rational skill models (RatSkills) that support efficient planning and inverse planning for achieving novel goals and recognizing activities
RA, In this work, we introduce the use of knowledge-driven scene priors in the semantic audio-visual embodied navigation task: we combine semantic information from our novel knowledge graph that encodes object-region relations, spatial knowledge from dual Graph Convolutional Networks, and background knowledge from a series of pre-training tasks|all within a reinforcement learning framework for audio-visual navigation
RA, In this work, we address this issue by introducing attention masks to incorporate spatial locality into self-attention heads of transformers
RA, To model such data, we here investigate a deep generative model in the form of a variational autoencoder (VAE) which can learn a sparse, binary code for its latents
RA, In this paper, we introduce a natural multi-agent version of this framework, where multiple decision makers try to predict the same outcome
RA,We propose a novel prediction interval (PI) method for uncertainty quantification, which addresses three major issues with the state-of-the-art PI methods
RA, We additionally propose a new metric to better measure the performance of the continual learning methods subject to inference queries at any moment
RA, We characterize the implicit bias of unregularized non-convex gradient flow as convex regularization of an equivalent convex model
RA,We hypothesize that due to the greedy nature of learning in multi-modal deep neural networks (DNNs), these models tend to rely on just one modality while under-utilizing the other modalities
RA, In this paper, we examine these defense mechanisms from a principled threat analysis perspective
RA, In this work, we propose a scalable proximal gradient type algorithm for Wasserstein gradient flow
RA, In this work we instead focus on growing the architecture without requiring costly retraining
RA, Our key contribution is a novel mechanism based on computing conditional expectations through joint conditional probabilities for capturing dependencies on other agents actions without increasing the complexity of training or decision making
RA, In this paper, we go beyond mere descriptions of the learning dynamics by taking a graph perspective and investigating the relationship between the graph structure of NNs and their performance
RA, In this work, we first illustrate how simply setting hyperparameters based on non-private training runs can leak private information
RA, In this work, we propose TIME-LAPSE, a spatio-temporal framework for uncertainty scoring that examines the sequence of predictions prior to the current sample to determine its predictive uncertainty
RA, Here, we introduce RAINDROP, a graph-guided network for learning representations of irregularly sampled multivariate time series
RA, However, to deploy a Deep Learning model in resource-constrained devices, low inference energy cost is also required along with low memory cost
RA, We propose a model which consists of a domain-invariant latent representation layer and a domain-specific linear prediction layer with a low-rank tensor structure
RA, In particular, we establish the power of learning community-based representations to understand the ability of a group of control nodes to steer the network to a target state
RA, We posit that a superior encoder for zero-shot generalization in RL can be trained by using solely an auxiliary SSL objective if the training process encourages the encoder to map behaviorally similar observations to similar representations, as reward-based signal can cause overfitting in the encoder (Raileanu et al, 2021)
RA, To address this issue, many relabelling methods like Hindsight Experience Replay have been developed and bring significant improvement
RA, Specifically, we can transfer the pretrained image model to a point-cloud model by inflating 2D convolutional filters to 3D and then finetuning the image-pretrained models (FIP)
RA, This strategy is outperformed by a more costly but state-of-the-art method---fine-tuning all parameters of the source model to the target domain---possibly because fine-tuning allows the model to leverage useful information from intermediate layers which is otherwise discarded by the later pretrained layers
RA, This paper proposes a model that reconciles language grounding and object detection with two main contributions: i) Architectures that exhibit iterative attention across the language stream, the pixel stream, and object detection proposals
RA, We propose a goal randomization method that uses random basic goals to train a policy in the absence of the reward of environments
RA, We start by examining the rank characteristics of the subspace spanned by gradients (i,e, the gradient-space) in centralized model training, and observe that the gradient-space often consists of a few leading principal components accounting for an overwhelming majority (95-99%) of the explained variance
RA, In this work, we focus on enhancing language model pre-training by leveraging definitions of the rare words in dictionaries (e,g,, Wiktionary)
RA, Under this form, transfer between related tasks now only requires training the reward component
RA,We present LSeg, a novel model for language-driven semantic image segmentation
RA, We learn such a dequantisation scheme q(z | x), using variational inference with TRUncated FLows (TRUFL) --- a novel flow-based model that allows the dequantiser to have a learnable truncated support
RA, Deviating from such models, we here introduce Latent Image Animator (LIA), a self-supervised auto-encoder that evades need for structure representation
RA, We instead propose a flexible editing paradigm that generates molecules using learned molecular fragments---meaningful substructures of molecules
RA,We propose data-driven one-pass streaming algorithms for estimating the number of triangles and four cycles, two fundamental problems in graph analytics that are widely studied in the graph data stream literature
RA,  To this end, we propose a generalized framework for combining boosted trees and more general model ensembling techniques, with graph propagation layers that share  node/sample information across edges connecting related samples
RA, In this paper, we boost the performance of deep metric learning (DML) models with adversarial examples generated by attacking two new objective functions: intra-class alignment and hyperspherical uniformity
RA, We present Multi-Task Distribution Learning, highlighting the similarities between Multi-Task Learning and preparing for Distribution Shift
RA, In this paper, we revisit binary hashing from RFF, and propose SignRFF, a new and simple strategy to extract RFF-based binary codes
RA, We frame supernet NAS as a two-stage search, decoupling the training of the supernet from the extraction of a final design from the supernet
RA, In this work, we introduce an active intervention-targeting mechanism which enables quick identification of the underlying causal structure of the data-generating process
RA, This definition, however, fails to recognize unpruned parameters that detached from input or output layers of underlying subnetworks, potentially underestimating actual effective sparsity: the fraction of inactivated connections
RA, We propose a novel algorithm for learning the linear mixture SSP, which can attain a \tilde O(dB_{\star^{1.5\sqrt{K/c_{\min) regret
RA, This paper introduces algorithms that ensure the calibration of any model while maintaining sharpness
RA, Taking inspiration from them, we exploit the specific relation of each query element with the targeted image and derive light-weight attention mechanisms which enable to mediate between the two complementary modalities
RA,We present a meta-learning framework for learning new visual concepts quickly, from just one or a few examples, guided by multiple naturally occurring data streams: simultaneously looking at images, reading sentences that describe the objects in the scene, and interpreting supplemental sentences that relate the novel concept with other concepts
RA, To solve the OSR problem based on pre-trained Softmax classifiers, previous studies investigated offline analyses, e,g,, distance-based sample rejection, which can limit the feature space of known-class data items
RA,This paper proposes a new and simple way of training sparse neural networks
RA, First, we provide a constrained-optimization perspective showing that CE and Dice share a much deeper connection than previously thought: They both decompose into label-marginal penalties and closely related ground-truth matching penalties
RA, To address these issues, we propose a representation-first approach to molecular graph generation
RA, In particular, we decompose a sample x to be its variational auto-encoder (VAE) reconstruction G(x) plus the residual R(x)=x-G(x), where R(x) retains most identity-distinctive information due to an information-theoretic interpretation of the VAE objective
RA, Based on the observation, we propose a new face restoration model that improves both generation and reconstruction by learning a stochastic model and enhancing the latent features respectively
RA, To address this, we introduce a new neural stochastic processes, Decoupled Kernel Neural Processes (DKNPs), which explicitly learn a separate mean and kernel function to directly model the covariance between output variables in a data-driven manner
RA, In this paper, we present a novel supervised learning framework of learning from open-ended data, which is modeled as data implicitly sampled from multiple domains with the data in each domain obeying a domain-specific target function
RA,This paper presents a framework for analyzing the expressiveness and learning of relational models applied to hypergraph reasoning tasks
RA,We present a memory-efficient neural ODE framework PNODE based on high-level adjoint algorithmic differentiation
RA, Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace and results in over-smoothing
RA, In this paper, we develop faster no-regret learning dynamics for extensive-form correlated equilibrium (EFCE) in multiplayer general-sum imperfect-information extensive-form games
RA,We introduce Self-GenomeNet, a novel contrastive self-supervised learning method for nucleotide-level genomic data, which substantially improves the quality of the learned representations and performance compared to the current state-of-the-art deep learning frameworks
RA, This paper proposes a two-stage method to distill multiple word senses from a pre-trained language model (BERT) by using attention over the senses of a word in a context and transferring this sense information to fit multi-sense embeddings in a skip-gram-like framework
RA, In this work, we introduce MIDI-DDSP a hierarchical model of musical instruments that enables both realistic neural audio synthesis and detailed user control
RA, To address these challenges, we present Adversarial Retriever-Ranker (AR2), which consists of a dual-encoder retriever plus a cross-encoder ranker
RA, What if one wants to predict error for a specific test sample? To answer this, we propose the novel paradigm of input-conditioned generalization error bounds
RA, In contrast, our proposed Poisson Quasimetric Embedding (PQE) is the first quasimetric embed-ding formulation learnable with gradient-based optimization, and enjoys strong performance guarantees
RA, In this work, we present Guided-TTS, a high-quality TTS model that learns to generate speech from untranscribed speech data
RA, To this end, we present the method of lower-dimensional embedding of log-signature (LORD), where we define an NRDE-based autoencoder to implant the higher-depth log-signature knowledge into the lower-depth log-signature
RA, We show that mixing examples that either have a large data or label distance may have an increasingly-negative effect on model performance
RA, In this work, we propose PLISA (Provable Learning-based Iterative Sparse recovery Algorithm) to learn algorithms automatically from data
RA, We propose an Euler discretization for these first-order finite-time flows, and provide convergence guarantees, in the deterministic and the stochastic setting
RA, We propose to enhance the practical applicability of online 3D-BPP via learning on a novel hierarchical representation – packing configuration tree (PCT)
RA, To take the advantages of both models, some combined methods have been proposed
RA, In this paper, we analyze the role of SA in Transformer-based ASR models for improving efficiency
RA, In this work, we replace the widely used random weight initialization with a fully deterministic initialization scheme ZerO, which initializes residual networks with only zeros and ones
RA, Specifically, they flag inputs for which local robustness checks fail, but yet that are not adversarial; i,e, they are classified consistently with all valid inputs within a distance of \epsilon
RA, In this paper, we introduce a simple variational objective whose optima are exactly the set of representations on which risk minimizers are guaranteed to be robust to any distribution shift that preserves the Bayes predictor, e,g,, covariate shifts
RA, In this work, we inject HJ reachability theory into the constrained Markov decision process (CMDP) framework, as a control-theoretical approach for safety analysis via model-free updates on state-action pairs
RA, We analyze the robustness of multiple fair machine learning algorithms that satisfy equalized odds (and equal opportunity) notion of fairness
RA, Based on this connection, the GCN architecture, shaped by stacking graph convolution layers, shares a close relationship with stacking GPCA
RA,We introduce Latent Temporal Flows (LatTe-Flows), a method for probabilistic multivariate time-series analysis tailored for high dimensional systems whose temporal dynamics are driven by variations in a lower-dimensional discriminative subspace
RA, Inspired by this, we design a simple yet effective visual pretext task, coined Patch-Aware Self-Supervision (PASS), for learning better patch-level representations
RA,   In this work, we propose a unified, systematic approach to learning N:M sparsity and integer quantization for pre-trained Transformers using the Alternating Directions Method of Multipliers (ADMM)
RA, To keep a knowledge graph up-to-date, it is required of an extractor to possess not only the ability to recall the triples encountered during training, but also the triples it has never seen before
RA, We propose a neural networks approach SegTime that finds precise breakpoints, obviates sliding windows, handles long-term dependencies, and it is insensitive to the label changing frequency
RA, However, we find that SCO algorithms are impractical for training GNNs on large graphs because they need to store the moving averages of the aggregated features of all nodes in the graph
RA, This shifts the burden of reward specification to the optimal design of the queries
RA, In this paper, we develop a training framework underlain by a novel superclass conditional Gaussian mixture model (SCGM)
RA, However, we find that existing approaches of this type underperform their potential, and can be overly complicated besides
RA, Specifically, we exhibit a pathological example where under common assumptions, _no_ useful importance sampling estimates of the partition function can guarantee to have variance bounded below a rational number
RA, We propose a Crystal Diffusion Variational Autoencoder (CDVAE) that captures the physical inductive bias of material stability
RA, In this paper, we provide the first study of low-precision Stochastic Gradient Langevin Dynamics (SGLD), arguing that it is particularly suited to low-bit arithmetic due to its intrinsic ability to handle system noise
RA,We introduce a structured latent variable model that learns the underlying data-generating process for a dataset of scenes
RA, Our conjecture has implications for lottery ticket hypothesis, distributed training and ensemble methods
RA, Using this formalism, we are able to determine in what regimes existing approaches fall short of fairness and provide variations that are fair in more situations, and handle counterfactual information
RA, In this paper, we present orthogonality preserving pruning (OPP), a regularization-based structured pruning method that maintains the dynamical isometry during pruning
RA, We show how to design learned sketches for the Hessian in the context of second order methods
RA, Our framework shows that a feedforward SNN with one neuron per layer and skip-layer connections can approximate the mapping function between any arbitrary pairs of input and output spike train on a compact domain
RA,  We present experiments showing that when applied to modern pretrained models, active learning offers inconsistent and often poor performance
RA, In this paper, we introduce a new framework to improve machine learning fairness
RA, In contrast, in this work, we investigate the viability of backdoor injection attacks in real-life deployments of DNNs on hardware and address such practical issues in hardware implementation from a novel optimization perspective
RA, We propose an algorithm that tackles this problem by letting the meta-learner teach itself
RA, We introduce a simulated benchmark EARL based on this framework, containing a set of diverse and challenging simulated tasks reflective of the hurdles introduced to learning when only a minimal reliance on extrinsic intervention can be assumed
RA,We propose StyleNeRF, a 3D-aware generative model for photo-realistic high-resolution image synthesis with high multi-view  consistency, which can be trained on unstructured 2D images
RA, In this paper, we propose Synchromesh: a framework for substantially improving the reliability of pre-trained models for code generation
RA, In this paper, we are the first to evaluate state-of-the-art DML methods trained on imbalanced data, and to show the negative impact these representations have on minority subgroup performance when used for downstream tasks
RA, Thus, it is critical to uncover their underlying connections to tackle one based on the other
RA, However, this reasoning is not applicable in our setting, since adversarial perturbations are believed not to change the label
RA, To this end, we show that perceptual sensitivity is correlated with the probability of an image in its close neighborhood
RA, In this paper, we propose object-centric actionable visual priors as a novel perception-interaction handshaking point that the perception system outputs more actionable guidance than kinematic structure estimation, by predicting dense geometry-aware, interaction-aware, and task-aware visual action affordance and trajectory proposals
RA, To understand this, we provide an analysis on the relationship between the location of NAS samples in the feature space and the performance of distance- and confidence-based OOD detection methods
RA, In this work, we introduce U-WILDS, which augments the WILDS benchmark of in-the-wild distribution shifts with curated unlabeled data that would be realistically obtainable in deployment
RA,We introduce a new task, unsupervised vision-language (VL) grammar induction
RA, In this work, we propose Gradient Assisted Learning (GAL), a new method for various entities to assist each other in supervised learning tasks without sharing data, models, and objective functions
RA, In this work, we recast AlphaZero with Hindsight Experience Replay to tackle complex goal-directed planning tasks
RA, In particular, for several pretrained models, we investigate the intermediate activations from networks that perform the Markov step of the reverse diffusion process
RA, We then present an alternative framework for density ratio estimation that is motivated by the scaled-Bregman divergence
RA, We apply Dynamo to both RNNs and CNNs, and find that the resulting model embedding manifolds enable novel applications: clustering of neural networks on the basis of their high-level computational processes in a manner that is less sensitive to reparameterization; model averaging of several neural networks trained on the same task to arrive at a new, operable neural network with similar task performance; and semi-supervised learning via optimization on the model embedding manifold
RA,We introduce environment predictive coding, a self-supervised approach to learn environment-level representations for embodied agents
RA, In this paper, we show the possibility of unsupervised FL whose model is still a classifier for predicting class labels, if there is sufficient class-prior shift among the unlabeled data owned by the clients and the class priors are known to the clients
RA, In this work, we analyze a popular variant of SGD where gradients are truncated above a fixed threshold
RA,We study efficient algorithms for reinforcement learning in Markov decision processes, whose complexity is independent of the number of states
RA, We find that learning from more complex tasks tend to give better representations for few-shot classification, and thus we propose the use of representations learned from multiple tasks for few-shot classification
RA, In this case, one can also use behavioral cloning (BC) algorithms, which mimic a subset of the dataset via supervised learning
RA, In this paper, we propose an information-theoretic approach for domain generalization
RA, In this work, we investigate the properties of magnitude on individual images, with each image forming its own metric space
RA, We propose to measure an RL agent's capacity to  generalize by evaluating it in a contextual decision process that combines a  tabular environment with observations from a supervised learning dataset
RA, We propose Latent Variable Sequential Set Transformers which are encoder-decoder architectures that generate scene-consistent multi-agent trajectories
RA, Toward this end, we first propose a policy called ``UCB-Filtering-with-Delayed-Feedback'' (UCB-FDF) policy for this new MAB framework
RA, In this work, we propose an approach to learn a central (global) model from the federation of (local) models which are trained on user-devices, without disclosing the local data or model parameters to the server
RA, We hypothesize that this is due to the manual construction of rules in past attempts
RA, In this work, we propose ContrastiveCrop, which could effectively generate better crops for Siamese representation learning
RA, Furthermore, to collect more data, we propose to imitate the example of standardized exams rather than designing them from scratch
RA, Particularly, we highlight the glaring disparities between models trained on CIFAR-10 and ImageNet-derived datasets
RA, Inspired by this observation, we propose Transformer with a Mixture of Gaussian Keys (Transformer-MGK), a novel transformer architecture that replaces redundant heads in transformers with a mixture of keys at each head
RA, To balance diversity and purity in the memory, we propose to combine a novel memory management strategy and robust learning
RA, Here we hypothesize that such an enhancement is enabled by recurrent neuronal computations in early visual areas
RA,We reveal an intriguing connection between adversarial attacks and cycle monotone maps, also known as optimal transport maps
RA, In this paper we propose a novel agnostic constrained learning formulation to tackle the class imbalance problem in FL, without requiring further information beyond the standard FL objective
RA, This approach becomes challenging in generic situations where the trained energy is nonconvex, due to the need to sample the Gibbs distribution associated with this energy
RA, This so-called ‘backfill’ phenomenon and its effect on model performance have been barely addressed in the prior literature
RA, We also provide a convergence proof of SAM for non-convex objectives when used with stochastic gradients and empirically discuss the convergence and generalization behavior of SAM for deep networks
RA, We design experiments to investigate this folk wisdom, and ﬁnd that representations learned from long-tailed data distributions substantially differ from the representations learned from ”normal” data distributions
RA, But to fill the performance gap that still exists in the large batch optimization, we study a method to directly control the flatness of local minima
RA, We start from the observation that popular methods for reducing overconfidence by regularizing the distribution of outputs or intermediate variables achieve better calibration by sacrificing the separability of correct and incorrect predictions, another important facet of uncertainty estimation
RA, The proposed method addresses this issue by using a high step-size at the beginning of the algorithm to search the main surface of the point cloud fast and effectively
RA, To tackle this challenge, we propose a Neural SLAM approach that, for the first time, utilizes several modalities for exploration, predicts an affordance-aware semantic map, and plans over it at the same time
RA, We use a form of covariance shrinkage to provide robustness against singular covariances due to overparameterized features or small datasets
RA, We investigate two distinct such inductive biases: feature-level bias (differences in which features are more readily learned) and exemplar-vs-rule bias (differences in how these learned features are used for generalization)
RA, Therefore, in this work, we develop a novel additive learning algorithm based on reinforcement learning (RL) for few-shot natural language generation (NLG) tasks
RA, We instead implement an automatic debiasing procedure based on automatically learning the Riesz representation of the linear functional using Neural Nets and Random Forests
RA, We meet the first requirement by designing recurrent neural networks (RNNs) with specific architectures that segregate motif-dependent parameters (as customary in continual learning works), and try a standard method to address the second by training with random initial states
RA, In this work, we consider the adversarial examples distribution as a tiny shift of the original distribution
RA, In this work, we consider a quantile approach to generative modelling using optimal transport with provable guarantees
RA, In this paper, we propose GARNET, a scalable spectral method to boost the adversarial robustness of  GNN models for both homophilic and heterophilic graphs
RA, We shed new light on this question by showing that applying ER causes the newly added classes’ representations to overlap significantly with the previous classes, leading to highly disruptive parameter updates
RA, More concretely, we design a dynamic attention-based multi-head token selector, which is a lightweight module for adaptive instance-wise token selection
RA, These benefits require no algorithmic changes other than the perspective and hold across a variety of learning algorithms, experimental settings, and distribution shifts, albeit only when hand-centric observability is sufficient
RA, To enhance the expressiveness of speech, we propose DPP-TTS: a text-to-speech model based on a determinantal point process
RA, Therefore, with standard state-dependent baselines, the policy gradient methods may still suffer high variance, causing low sample efficiency during the training of DR
RA, Specifically, we assume that while noise varies significantly between individuals, true responses to stimuli will share common, low-dimensional features between subjects which are jointly discoverable
RA, However, this approach relies on the simulator being a sufficiently accurate reflection of the target environment, which is difficult to achieve in practice, resulting in the need to bridge sim2real gap
RA, To address the challenge that available tasks may not densely sample the space of tasks, we propose to augment the task set through interpolation
RA, In this work, we instead revisit the concept of leave-one-out error to measure the generalization ability of deep and wide models
RA, This paradigm relies on direct supervision examples, which is not applicable to many emerging applications, such as generating adversarial attacks or generating prompts to control language models
RA, To overcome these limitations, in this work we propose to combine neural implicit representations for appearance modeling with neural ordinary differential equations (ODEs) in order to obtain interpretable physical models directly from visual observations
RA, This task is termed open-set recognition (OSR) and has received significant attention in recent years
RA, We propose symmetric support difference as a divergence measure to quantify the mismatch between supports
RA, In this work, to establish a rigorous definition of “context representation”, we formalize this intuition using a category theory framework, which indicates the necessity of including the information from both tokens and how transitions happen among different tokens in a given context
RA,We propose a framework to continuously learn object-centric representations for visual learning and understanding
RA, In this paper, we decompose mixup into two sub-tasks of mixup generation and classification and formulate it for discriminative representations as class- and instance-level mixup
RA, To improve generalization, Adam is typically used in tandem with a squared \ell_2 regularizer (referred to as Adam-\ell_2)
RA, We apply the simplification approach to a wide range of scenarios: conventional training, dataset condensation and post-hoc explanations
RA, We propose a new Federated Learning framework referred to as SemiFL to address Semi-Supervised Federated Learning (SSFL)
RA,  We propose a new minimax optimization framework,GDA-AM, that views the GDA dynamics as a fixed-point iteration and solves it using Anderson Mixing to converge to the local minimax
RA, This enables us to directly parameterize filters in terms of a band-limited basis on the base space, but also to easily implement steerable CNNs equivariant to a large number of groups
RA, In this paper, we use equivariant transition models as an inductive bias to learn symmetric latent representations in a self-supervised manner
RA, This is suboptimal since better nonlinear classifiers could exist in the same embedding vector space
RA, We present aleatoric mapping agents (AMAs), a neuroscience inspired solution modeled on the cholinergic system of the mammalian brain
RA, Therefore, when an RS designer chooses which system to deploy, they are implicitly choosing how to shift or influence user preferences
RA, However, is random optimal? We show that this depends heavily on what functions we are optimizing, and the convergence gap between optimal and random permutations can vary from exponential to nonexistent
RA, Specifically, we can successfully locate the LIP subnetworks at the sparsity range of 20%-86.58% in setting i; and those at sparsity range of 5%-36% in setting ii
RA, In this work, we present a novel complete verifier which combines the strengths of both paradigms: it leverages multi-neuron relaxations and an efficient, GPU-based dual optimizer to drastically reduce the number of subproblems generated during the BaB process
RA, We propose using an additional guide retriever that is allowed to use the target output and ``in hindsight’’ retrieve relevant passages during training
RA, This paper bridges this critical gap by proposing and exploring an unknown-aware RL framework, which improves the safety and reliability of deep Q-learning
RA, Here, we present Variational Predictive Routing (VPR) – a neural probabilistic inference system that organizes latent representations of video features in a temporal hierarchy, based on their rates of change, thus modeling continuous data as a hierarchical renewal process
RA,In this paper, we introduce the online and streaming MAP inference and learning problems for Non-symmetric Determinantal Point Processes (NDPPs) where data points arrive in an arbitrary order and the algorithms are constrained to use a single-pass over the data as well as sub-linear memory
RA,  In this paper, we propose a novel evaluation framework for OOD detection that tests a detector over a larger, unexplored space of outliers
RA, To mitigate this discrepancy in a computationally efficient manner, we propose a method for mapping actuation torques to subjective effort without simulating muscles and their energy expenditure
RA, We show that by relaxing the inductive biases of these models, we can match or exceed performance on energy-conserving systems while dramatically improving performance on practical, non-conservative systems
RA, We introduce Flashlight, an open source library built to spur innovation in machine learning tools and systems by prioritizing open, modular, customizable internals and state-of-the-art, research-ready models and training setups across a variety of domains
RA, In this work, we propose a new family of distance metrics, called augmented sliced Wasserstein distances (ASWDs), constructed by first mapping samples to higher-dimensional hypersurfaces parameterized by neural networks
RA, In this paper, we introduce a model-based framework to solve such exploration-exploitation problem during its execution
RA, To address the two aforementioned problems, we wish to learn a distance metric only over fewer temporal graphs, which metric could not only help accurately categorize seen temporal graphs but also be adapted smoothly to unseen temporal graphs
RA, To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and model utility for legitimate users
RA, In this paper, we compare Lasso with 5 other methods: Elastic net, SCAD, forward selection, thresholded Lasso, and forward backward selection
RA, In this paper, we reveal important insights that reliance on unimportant weights and units can directly attribute to the brittleness of OOD detection
RA,In this work we propose a HyperTransformer, a transformer based model that generates all weights of a CNN model directly from the support samples
RA, In particular, we train transformers to perform multi-step computations by asking them to emit intermediate computation steps into a "scratchpad"
RA, However, in this paper, we show that when trained on one type of poisoned data, adversarial training can also be fooled to have catastrophic behavior, e,g,, <1% robust test accuracy with >90% robust training accuracy on CIFAR-10 dataset
RA, In this paper, we instead propose to compute distance between black-box models by comparing their Local Interpretable Model-Agnostic Explanations (LIME)
RA,  Specifically, we solve the partial alignment problem where the label only matches a middle part of the sequence
RA, We propose to overcome this limitation by introducing the new setting of language-guided image clustering
RA, We propose a novel SF mechanism, \xi-learning, based on learning the cumulative discounted probability of successor features
RA,   In this paper, we propose an alternative to the CRF layer — the Prior Knowledge Layer (PKL), that allows one to obtain probability distributions of each token and also takes into account prior knowledge concerned the structure of label sequences
RA, In this work, we propose an Object-Oriented Text Dynamics (OOTD) model that enables planning algorithms to solve decision-making problems in text domains
RA,This paper demonstrates how time-constrained multi-robot task allocation (MRTA) problems can be modeled as a Markov Decision Process (MDP) over graphs, such that approximate solutions can be modeled as a policy using Reinforcement Learning (RL) methods
RA, In this paper, we introduce NetVec, a novel multi-level framework for scalable unsupervised hypergraph embedding, which outperforms state-of-the-art hypergraph embedding systems by up to 15% in accuracy
RA, To measure the implicit bias of architecture, new technical tools are developed to both analytically bound and consistently estimate the average test error of the neural network--Gaussian process (NNGP) posterior
RA, Motivated by these observations, we introduce two simple but effective methods: (1) Computation Redistribution (CR), which reallocates the computation between the backbone, neck and head of the model; and (2) Sample Redistribution (SR), which augments training samples for the most needed stages
RA,We introduce DictFormer with efficient shared dictionary to provide a compact, fast, and accurate transformer model
RA,We present the ﬁrst framework of Certifying Robust Policies for reinforcement learning (CROP) against adversarial state perturbations
RA, To overcome these issues, we propose a communication-efficient distributed GNN training technique named \text{Learn Locally, Correct Globally (LLCG)
RA, Different from these works, we propose a general framework for nonlinear ICA, in which the mixing function is assumed to be a volume-preserving transformation, and meanwhile the conditions on the sources can be much looser
RA, In this paper, we propose Eigencurve, the first family of learning rate schedules that can achieve minimax optimal convergence rates (up to a constant) for SGD on quadratic objectives when the eigenvalue distribution of the underlying Hessian matrix is skewed
RA, In this work, we formally pinpoint reasons for NCE’s poor performance when an inappropriate noise distribution is used
RA, In this paper, we propose a novel GNN called Count-GNN for subgraph isomorphic counting, to deal with the above challenges at two levels
RA, Thus, in visually complex scenes they learn representations that model lots of task-irrelevant details and hence lead to slower downstream task learning
RA, We propose FCause, a new flow-based causal discovery method that addresses these drawbacks
RA, This increases the malleability of the trained global model to poisoning attacks and exposes the sensitive local datasets of parties to inference attacks
RA, Our PolyLoss allows the importance of different polynomial bases to be easily adjusted depending on the targeting tasks and datasets, while naturally subsuming the aforementioned cross-entropy loss and focal loss as special cases
RA, These methods coerce suboptimal data into a form that allows supervised learning methods to acquire optimal policies
RA, We propose a novel neural network modification to mitigate the impacts of missing data
RA, This fine-tuning is formally similar to the natural evolution of genetic codes in response to shifting environment
RA, Our approach essentially split up example strings into multiple parts using a neural network trained to group similar substrings from positive strings
RA, In this paper, we introduce several ways to perturb SARS-CoV-2 spike protein sequences in ways that mimic the error profiles of common sequencing platforms such as Illumina and PacBio
RA, This leaves little guidance for a practitioner or a researcher using these methods in their decision process
RA, Instead, we focus on more expressive transforms that result in a better rate-distortion-computation trade-off
RA, In this paper, we point out several key facets of how to train MAML to excel in few-shot classification
RA, In this work, we use vision transformers (ViTs) as our base model for visual reasoning and incorporate concepts defined as object entities and their relations as a source of weak supervision
RA,This paper explores a simple method for improving the zero-shot learning abilities of language models
RA, To derive the tight offline learning bound, we design the variance-aware pessimistic value iteration (VAPVI), which adopts the conditional variance information of the value function for time-inhomogeneous episodic linear Markov decision processes (MDPs)
RA, We highlight a bias introduced by this common practice: we prove that the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples
RA, This work proposes a novel training paradigm, Data efficient CLIP (DeCLIP), to alleviate this limitation
RA, To this end, we present CrowdPlay -- a complete crowdsourcing pipeline for OpenAI Gym and Gym-like MDP environments, a large-scale publicly available crowdsourced dataset of human gameplay demonstrations on single and multi-player Atari 2600 games, a collection of imitation and offline reinforcement learning benchmarks, and a detailed discussion of crowdsourcing and incentivization methodology
RA, Here, we present GrID-Net, a framework based on graph neural networks with lagged message passing for Granger causal inference on DAG-structured systems
RA,We investigate the robustness of vision transformers (ViTs) through the lens of their special patch-based architectural structure, i,e, they process an image as a sequence of image patches
RA, In this paper, we present Neural Circuit Architectural Priors (NCAP), a set of reusable architectural components and design principles for deriving network architectures for embodied control from biological neural circuits
RA, In this paper, we propose to use a recently reported strong pre-trained feature extractor called CLIP and also propose a novel and yet simple pseudo-replay method to deal with CF
RA,In this paper, we develop a new approach to conformal prediction in which we aim to output a precise set of promising prediction candidates that is guaranteed to contain a limited number of incorrect answers
RA,In this paper, we propose a f-divergence Thermodynamic Variational Objective (f-TVO)
RA,In this work, we analyze the effectiveness of the Bellman equation as a proxy objective for value prediction accuracy in off-policy evaluation
RA, Moreover, applying these methods to belief state modeling in multi-agent settings would require passing policies into the belief model---at the time of writing, there have been no successful demonstrations of this
RA, To alleviate this issue, we propose a series of techniques, including a gradient projection algorithm, a switchable layer scaling design, and a simplified data augmentation and regularization training recipe
RA, We propose a novel Pooling Network (PoNet) for token mixing in long sequences with linear complexity
RA, We propose a self-supervised and personalized federated learning framework, named SSFL, and a series of algorithms under this framework which work towards addressing these challenges
RA, In this work, we present TrMRL (Transformers for Meta-Reinforcement Learning), a meta-RL agent that mimics the memory reinstatement mechanism using the transformer architecture
RA, To combat this inherent pessimism towards exploration, we derive an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement
RA, In this work, we propose uncertainty-targeted attacks (UTA), where the perturbations are obtained by maximizing the model's estimated uncertainty
RA, To address this, we propose a Wasserstein distributionally robust optimization scheme called WAFL
RA, We propose a novel approach, DISSECT, that jointly trains a generator, a discriminator, and a concept disentangler to overcome such challenges using little supervision
RA, With these factors set right, we obtain private NLP models that outperform state-of-the-art private training approaches and strong non-private baselines---by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora
RA, In this work, we introduce augmentation-augmented variational autoencoders (AAVAE), yet another alternative to self-supervised learning, based on autoencoding
RA,   In this work,  we go from a complementary direction and articulate the Dual Lottery Ticket Hypothesis (DLTH): Any randomly selected subnetwork of a randomly initialized dense network can be transformed into an appropriate condition with admirable train-ability— any ticket in a given lottery pool can be transformed into a winning ticket
RA,We revisit the incremental autonomous exploration problem proposed by Lim and Auer (2012)
RA, In this paper, we propose a new guarantee on the downstream performance without resort to the conditional independence assumption that is widely adopted in previous work but hardly holds in practice
RA, Our RIRL framework generalizes and is more flexible than prior work by allowing for multi-timestep dynamics and information channels with heterogeneous processing costs
RA,We introduce a new constrained optimization method for policy gradient reinforcement learning, which uses two trust regions to regulate each policy update
RA, This more transparent process encourages trust in the biomedical community for automatic hypothesis generation systems
RA, In this work, we present an approach of meta-imitation learning by watching video demonstrations from humans
RA, In this work, we exploit the relation between the singular values of the convolution layer's  Jacobian and the structure of the convolution kernel
RA, Here, we show that rather than using only invariance, pre-training that encourages non-trivial equivariance to some transformations, while maintaining invariance to other transformations, can be used to improve the semantic quality of representations
RA,We propose Compressed Vertical Federated Learning (C-VFL) for communication-efficient training on vertically partitioned data
RA, This block structure has two seemingly contradictory properties: on the one hand, its constituent layers have highly similar dominant first principal components (PCs), but on the other hand, their representations, and their common first PC, are highly dissimilar across different random seeds
RA, We present a learn-able hierarchy of parameterized and "physics-explainable" SPH informed fluid simulators using both physics based parameters and Neural Networks as universal function approximators
RA, This paper introduces a closed-loop continual learning framework, which obtains a real-time feedback learning signal via an additional test memory and then adapts the replay dynamics accordingly
RA, In this work, we propose two simple yet effective metrics, Combined Error Variance (CEV) and Symmetric Distance Error (SDE), to quantitatively evaluate the class-wise bias of two models in comparison to one another
RA, We hypothesize that humans explore new environments efficiently by inferring the structure of unobserved spaces using spatial information collected from previously explored spaces
RA, We find that the fraction of separable dichotomies is determined by the dimension of the space that is fixed by the group action
RA, This paper presents a new Bayesian meta-learning approach called Neural Variational Dropout Processes (NVDPs)
RA, We use a pre-trained GPT-2 LM to initialize an interactive policy, which we fine-tune via imitation learning to perform interactive tasks in a simulated household environment featuring partial observability, large action spaces, and long time horizons
RA, In this paper, we propose a novel Split-Mix FL strategy for heterogeneous participants that, once training is done, provides in-situ customization of model sizes and robustness
RA, In this work, we address both of these challenges by extending prior works in three main directions
RA, To address this, we developed a simple and robust method to train a Deep Neural Network (DNN) through self-supervised learning for solving a goal-predefined combinatorial problem
RA, In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them
RA, In this work, we introduce Plug-In Inversion, which relies on a simple set of augmentations and does not require excessive hyper-parameter tuning
RA, In this paper, we show that standard cryptographic assumptions imply that this stronger condition is still insufficient
RA, To this end, we show its policy evaluation error on the distribution of transitions decomposes into: a Bellman error, a bias from policy mismatch, and a variance term from sampling
RA, This urges the practitioner to develop fair systems whose performances are uniform among individuals
RA, Here we present  a drawing agent that operates on stroke-level representation of images
RA, We also present and analyze the Story-Critique Dataset, a new corpora composed of 1.3 million aligned story-critique pairs derived from over 80,000 stories
RA, However, target accuracy is the goal, and so we argue for optimizing as much as possible on target
RA, This paper explores a theoretical framework for value factorisation with interpretability
RA, In this paper, we rethink adversarial transferability from a data distribution perspective and further enhance transferability by score matching based optimization
RA, To this end, we propose an efficient approximate median aggregation with MPC privacy guarantees on the multi-silo setting, e,g,, across hospitals, with two semi-honest non-colluding servers
RA, This paper takes a step in this direction by establishing contraction properties of broad classes of nonlinear recurrent networks and neural ODEs, and showing how these quantified properties allow in turn to recursively construct stable networks of networks in a systematic fashion
RA,We present a differentiable approach to learn the probabilistic factors used for inference by a nonparametric belief propagation algorithm
RA, To achieve this, a common approach is to record neural populations in behaving animals, and model these data as emanating from a latent dynamical system whose state trajectories can then be related back to behavioural observations via some form of decoding
RA, Besides optimizing the GNN model architecture, we propose to simultaneously optimize the input graph topology, via a set of parameterized data augmentation operators
RA, We consider public training data sets that are from the *same distribution* as the private training data set
RA, We propose a principled inference framework to improve summarization models on these two aspects
RA, To bypass this limitation, unsupervised neural machine translation techniques have been proposed to learn code translation using only monolingual corpora
RA, Next, we propose a meta-algorithm that transforms an existing noisy label defense algorithm to one that protects against backdoor attacks
RA, In this paper, we propose LSSAMP that uses the multi-scale VQ-VAE to learn the positional latent spaces modeling the secondary structure
RA, In light of both device-to-device (D2D) and device-to-server (D2E) cooperation opportunities in modern communication networks, this paper proposes a new federated optimization algorithm dubbed hybrid local SGD (HL-SGD) in FL settings where devices are grouped into a set of disjoint clusters with high D2D communication bandwidth
RA, Our approach relies on two key concepts: successor features (SF), a value function representation that decouples the dynamics of the environment from the rewards, and an actor-critic framework that incorporates the learned SF representations
RA,In this paper, we present a novel method to learn a Bayesian neural network robust against adversarial attacks
RA, In this paper, we provide improved generalization analyses for almost all existing generalization measures of minimax problems, which enables the minimax problems to establish sharper bounds of order \mathcal{O\left( 1/n \right), significantly, with high probability
RA,This work presents a probabilistic channel pruning method to accelerate Convolutional Neural Networks (CNNs)
RA, This suggests that the generalization gap between Adam and SGD is fundamentally tied to the nonconvex landscape of deep learning optimization, which cannot be covered by the recent neural tangent kernel (NTK) based analysis
RA, This paper presents Conditional Contrastive Learning with Kernel (CCL-K) that converts existing conditional contrastive objectives into alternative forms that mitigate the insufficient data problem
RA, To move towards a more guided process, the impact of design decisions on information processing must be understood better
RA, Our work carefully characterizes these conditions and provides supporting theoretical understanding and empirical observations
RA, In this paper, we adopt a more difficult setting, incorporating background distractors, as a first step towards addressing this challenge
RA, Specifically, Muller et al, [1] claim that LS erases relative information in the logits; therefore a LS-trained teacher can hurt KD
RA, To solve this problem, we explore the optical flow between video frames for the automatic portrait video matting
RA,  To this end, we propose FedDrop, which introduces channel-wise weighted dropout layers between convolutions to accelerate training while minimizing their impact on convergence
RA, To this end, we present Hierarchical Affordance Learning (HAL), a method that learns a model of hierarchical affordances in order to prune impossible subtasks for more effective learning
RA, In this paradigm, typed edges may have several key-value pairs known as qualifiers that provide fine-grained context for facts
RA, In this study, we introduced adversarial one-class classification into the classification-based IRL framework, and consequently developed a novel IRL method that requires only expert trajectories
RA, In this work, we consider a generalized setting where given samples from multiple distributions p_1, \ldots, p_k (for k > 2), we aim to efficiently estimate the density ratios between all pairs of distributions
RA, Specifically, we show that over any sequence of convex loss functions, any low-regret algorithm can be adapted to optimize the parameters of a neural network such that it competes with the best net in hindsight
RA, To enhance the quality of representation, we design an RL framework that combines three different self-supervised learning methods; 1) Adversarial Representation, 2) Forward Dynamics, and 3) Inverse Dynamics
RA, In this study, we derive a novel closed-form gradient sampling solution for Differentialble Gradient Sampling (DGS) that enables backpropagation of the loss of the spatial gradient back to the feature map pixels, thus allowing the imposition of the loss efficiently on the spatial gradient
RA, In this regard, we propose the input convex graph neural networks (ICGNN) whose inputs and outputs are related via convex functions
RA, Using precise high-dimensional asymptotics, we characterize the dynamics of the fitted model in two “worlds”: in the Oracle World the model is trained on the population distribution and in the Empirical World the model is trained on an i.i.d finite dataset
RA, Our key innovations are: i) relaxed queries where a domain expert (oracle) only judges the correctness of the predicted labels (a binary question) rather than identifying the exact class (a multi-class question), and ii) new criteria of maximizing information gain propagation for active learner with relaxed queries and soft labels
RA, We formulate a hierarchical reinforcement learning framework for learning to decide when to request additional information from humans and what type of information would be helpful to request
RA, However, this scales quadratically with the number of pixels, which becomes infeasible for high-resolution inputs
RA, Our proof relies on a specific decomposition of the network into a multilinear (up to sign) function and another ReLU network whose weights are constant under a certain parameter directional convergence
RA,   To this end, we propose a new SGD algorithm with delayed averaging, namely DaSGD, which can fully parallelize SGD and forward/backward propagations to hide 100% of gradient communication
RA, We propose CAGE, a framework for inferring the cause-effect relationships governing deep generative models
RA, To this end, we propose a new scheme named federated learning via plurality vote (FedVote)
RA, Hence, we explore the practical setting called single positive setting, where each data instance is annotated by only one positive label with no explicit negative labels
RA, In this work, we extend hindsight relabelling mechanisms to guide exploration along task-specific distributions implied by a small set of successful demonstrations
RA, In this sense, models based on Self-Organizing Maps models with relevance learning (SOMRL) were considered as they perform well in clustering besides being able to create a map that learns the relevance of each input dimension for each cluster, preserving the original relations and topology of the data
RA, In this work, we address this issue by introducing a novel self-supervised loss that encourages the discriminator to approximate a richer reward function
RA, This is because that training a deep neural network from scratch by maximizing an AUC loss usually does not yield a satisfactory performance
RA, Using Instance-adaptive Batch Normalization (IaBN), we modify normalization layers by combining the feature statistics acquired at training time with those of the test sample
RA, In this work, we aim to study and devise methods that make no assumptions about the model training process and are broadly applicable at test time
RA, We save on average 58.49% computational time to train a CNN model
RA,In this paper, we present a novel approach to learn texture mapping for a 3D surface and apply it to document image unwarping
RA,In this paper, we establish the first finite-time convergence result of the actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward
RA, Instead, we define a surrogate gap, a measure equivalent to the dominant eigenvalue of Hessian at a local minimum when the radius of neighborhood (to derive the perturbed loss) is small
RA, In this work, we take a deep look at the prioritized ER
RA, In this paper, we first investigate the relationship between them by analyzing Federated Averaging at the client level and determine that a better federated global model performance does not constantly improve personalization
RA,We present a novel approach to use curricula to identify principles by which a system learns
RA, We then introduce selective ensembles to mitigate such inconsistencies by applying hypothesis testing to the predictions of a set of models trained using randomly-selected starting conditions; importantly, selective ensembles can abstain in cases where a consistent outcome cannot be achieved up to a specified confidence level
RA, To address these concurrently, we propose Federated Supermask Learning (FSL)
RA, Based on these observations, we propose several techniques, stemmed from the rich LTR literature, to improve the MCC performance
RA, In this paper, we present GNP, a L^p norm-like pooling function that is trainable end-to-end for any given task
RA, In this work, we explore the use of an often unused source of auxiliary supervision: language
RA, Specifically, we show that sparse non-physical approximations exist with excellent fitting accuracy, but fail to adequately model the situation
RA, In this paper, we take the first step in introducing a generation of RL solvers that learn to minimise safety violations while maximising the task reward to the extend that can be tolerated by safe policies
RA, We propose a temporal convolution network based embedding that improves on the state-of-the-art by incorporating recent advances in contrastive learning to the time series domain and by adopting a multi-resolution approach
RA, We decompose this problem into three easier subtasks, and provide candidate solutions for each of them
RA, In this work, we show that a simple network based on the multi-layer perceptron (MLP)-mixer enables state-of-the art image reconstruction performance without convolutions and without a multi-resolution architecture
RA, This approach has many constraints, so we propose the Limitless Routing Network (LRN) which removes the constraints through the usage of a transformer-based router and a reevaluation of the state and action space
RA, To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation
RA, In this paper we leverage attribute conditioning in order to address the problem of conflicting information in multi document summarization and show strong gains in performance over the base abstractive multi document summarization methods
RA, To prove our point, we examine dataset transformations used in the literature to adapt machine learning-based methods across domains and show that these dataset transformations are not always beneficial in terms of performance
RA, To solve this task, algorithms must produce features for every pixel that are both semantically meaningful and compact enough to form distinct clusters
RA, To address such composite tasks, we propose a hierarchical modular approach to learn agents that navigate and manipulate objects in a  divide-and-conquer manner for the diverse nature of the entailing tasks
RA,We propose learning via retracing, a novel self-supervised approach for learning the state representation (and the associated dynamics model) for reinforcement learning tasks
RA, This paper proposes an on-the-fly DFQ framework with sub-second quantization time, called SQuant, which can quantize networks on inference-only devices with low computation and memory requirements
RA,  In this paper, we propose a generalization of conformal prediction to multiple learnable parameters, by considering the constrained empirical risk minimization (ERM) problem of finding the most efficient prediction set subject to valid empirical coverage
RA, Here, we introduce a novel open-world semi-supervised learning setting that formalizes the notion that novel classes may appear in the unlabeled test data
RA, We also show the importance of a diverse allocation of domains, as well as room-for-improvement of existing methods on both domain and example selection
RA, To address this issue, we introduce the information bottleneck principle and propose the Self-supervised Variational Information Bottleneck (SVIB) learning framework
RA, Moreover, we study the effect of early stopping on generalization and demonstrate that optimal early stopping can help mitigate ''descent'' in various settings
RA,We advocate for a practical Maximum Likelihood Estimation (MLE) approach towards designing loss functions for regression and forecasting, as an alternative to the typical approach of direct empirical risk minimization on a specific target metric
RA, To allow asynchronous learning and decision-making, we formulate a set of asynchronous multi-agent actor-critic methods that allow agents to directly optimize asynchronous (macro-action-based) policies in three standard training paradigms: decentralized learning, centralized learning, and centralized training for decentralized execution
RA, Thus, it is challenging for these methods to train a suitable global model to effectively induce high-quality personalized models without changing learning objectives
RA, In this work, we show that DQN can indeed diverge and cease to operate in realistic settings
RA, Thereby we propose AdaMomentum as a new optimizer reaching the goal of training fast while generalizing better
RA, We show that using a kernelised generalised linear model (kGLM) as an inner problem in a DDN yields a large class of commonly used DEQ architectures with a closed-form expression for the hidden layer parameters in terms of the kernel
RA, We propose a simple yet effective approach for adding locality information into such models by adding learned parameters that improve the likelihood of retrieving examples from local neighborhoods
RA, We further provide a lower bound on the number of heads for Vision Transformers to express CNNs
RA, This paper proposes an elegant bidirectional candidates generation framework that can serve both purposes all together
RA,We prove new generalization bounds for stochastic gradient descent for both the convex and non-convex cases
RA, In this article, we show that by introducing a properly pre-trained encoder, the latent variable can play a more important role, which decomposes a deep generative model into a supervised learning problem and a much simpler unsupervised learning task
RA, This phenomenon motivates our method named Model Zoo which, inspired from the boosting literature, grows an ensemble of small models, each of which is trained during one episode of continual learning
RA, We present SparRL, the first general and effective reinforcement learning-based framework for graph sparsification
RA, In this paper, we propose a new deep learning framework for probabilistic interpolation of irregularly sampled time series that we call the Heteroscedastic Temporal Variational Autoencoder (HeTVAE)
RA, In this work, we propose a new ETD method, called PER-ETD (i,e, PEriodically Restarted-ETD), which restarts and updates the follow-on trace only for a finite period for each iteration of the evaluation parameter
RA, Specifically, we encode the morphological information in terms of the traversal-based positional embedding and the graph-based relational embedding
RA, We show that in-context learning emerges via Bayesian inference when the pretraining distribution is a mixture of HMMs
RA, In this work, we present a path auxiliary algorithm that uses a composition of local moves to efficiently explore large neighborhoods
RA, To address this limitation, we rather start with strong empirical evidence of the plateau of the sharpness (the top eigenvalue of the Hessian) of the loss function landscape
RA, To enable easy post-hoc editing at scale, we propose Model Editor Networks with Gradient Decomposition (MEND), a collection of small auxiliary editing networks that use a single desired input-output pair to make fast, local edits to a pre-trained model
RA,We address these difficulties by integrating PDE and CNN in a loop, thus shifting the paradigm to unsupervised learning that only requires seismic data
RA, To tackle this issue, we propose to explicitly model analogical structure with an Abelian group
RA, To address this problem, we theoretically derive Stein latent optimization that provides reparameterizable gradient estimations of the latent distribution parameters assuming a Gaussian mixture prior in a continuous latent space
RA, To prevent this, existing methods often use negative pairs (contrastive learning) or ad hoc architecture constructs
RA, To address this issue, continual learning methods use episodic memory, parameter regularization, masking and pruning, or extensible network structures
RA, We incorporate this information into a pre-trained reasoning module, and investigate its role in shaping the discovered representations in diverse self-supervised learning settings from pixels
RA, To accelerate the inference process while keeping the sample quality, we provide a new perspective that DDPMs should be treated as solving differential equations on manifolds
RA, We improve on these methods with MixtureEnsembles, which learns to factorize ensemble members with shared parameters by constructing each layer with a linear combination of templates
RA, This two-stage approach has a potential limitation that the predicted distances may conflict with each other, e,g,, violating the triangle inequality
RA,  We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately
RA, Specifically, we propose a novel loss that improves test-time adaptation by addressing both premature convergence and instability of entropy minimization
RA, This work presents Temporally-Adaptive Convolutions (TAdaConv) for video understanding, which shows that adaptive weight calibration along the temporal dimension is an efﬁcient way to facilitate modelling complex temporal dynamics in videos
RA,We present Path Integral Sampler~(PIS), a novel algorithm to draw samples from unnormalized probability density functions
RA, We propose two approaches: one that models neurons' activation behavior as a graph and examines whether the neurons form meaningful communities, and the other examines the predictability of neurons' behavior using entropy
RA, We derive generalization error bounds for such active learning strategies in terms of Rademacher average and localized discrepancy for general loss functions which satisfy a regularity condition
RA, We compare its computational complexity to the one of BP and delineate the regimes in which TP can be attractive compared to BP
RA, To tackle these problems, our key idea is to incorporate the design procedure of an agent into its decision-making process
RA, In this paper we propose One-Shot Supermasking (OSSuM), a gradient-free, compute-efficient technique to efficiently prune neurons in fully-connected networks
RA,We propose a minimax formulation for removing backdoors from a given poisoned model based on a small set of clean data
RA, In this paper, we extend this to two general flexible classes: kernel exponential families and our new sparse counterpart kernel deformed exponential families
RA,This paper presents a simple MLP-like architecture, CycleMLP, which is a versatile backbone for visual recognition and dense predictions
RA,We consider adversarial attacks to a black-box model when no queries are allowed
RA, We postulate that such hidden time-conditioned properties may be captured by the tools of multipersistence, i.e, a emerging machinery in topological data analysis which allows us to quantify dynamics of the data shape along multiple geometric dimensions
RA, In this paper, we develop a methodology to analyze the robustness of fixed feature extractors, which in turn provide bounds on the robustness of any classifier trained on top of it
RA,We present a method to compute the derivative of a learning task with respect to a dataset
RA,We show how to train a rotation-equivariant representation to extract local keypoints for image matching
RA, In this paper, we take a very different perspective on the problem and ask,  ``Can we instead identify latent properties by leveraging knowledge of the mechanisms that govern their evolution?'' We provide a complete characterization of the sources of non-identifiability as we vary knowledge about a set of possible mechanisms
RA, In this paper, in contrast to strict MV control, we consider learning MV efficient policies that achieve Pareto efficiency regarding MV trade-off
RA, In this paper, we investigate causes for this perceived instability
RA, In this work, we study generalization across three disparate task structures: (a) tasks composed of spatial and temporal compositions of regularly occurring object motions; (b) tasks composed of active perception of and navigation towards regularly occurring 3D objects; and (c) tasks composed of navigating through sequences of regularly occurring object-configurations
RA,We derive an online algorithm for unsupervised learning based on representing every input \mathbf{x_t by a high dimensional vector \mathbf{y_t with pairwise inner products that approximately match input similarities as measured by a kernel function: \mathbf{y_s \cdot \mathbf{y_{t \approx f(\mathbf{x_s, \mathbf{x_{t)
RA,  First, we propose a positive sampling strategy to align one augmented view of instance with the neighbors of another view so that we can avoid the class collision issue caused by the negative examples and hence improve the within-cluster compactness
RA, We found that directly supervising the model with a set of training attributes does not generalize well on the test attributes, whereas self-supervised pre-training brings significant improvement
RA, To overcome this fundamental limitation, we propose a methodology to create cheap NAS surrogate benchmarks for arbitrary search spaces
RA, Thus, standard RL methods easily fail and generate responses diverging from human language, even when fine-tuning a powerful pre-trained language model
RA, To this end, we propose a physics-infused deep neural network based on the Blaschke products for phase retrieval
RA, In this paper, we take the first step in building AI system learning inter-object functional relationships in 3D indoor environments with key technical contributions of modeling prior knowledge by training over large-scale scenes and designing interactive policies for effectively exploring the training scenes and quickly adapting to novel test scenes
RA, To override these problems, we leverage the fundamental solution, boundary integral method and neural networks to develop a new method for computing Green's function with high accuracy in this paper
RA, In this paper, we propose a Layer-wise Abstaining Loss Minimization method (LAM),  a tractable method that breaks the hierarchical learning problem into layer-by-layer learning-to-abstain sub-problems
RA,We introduce Softmax Gradient Tampering, a technique for modifying the gradients in the backward pass of neural networks in order to enhance their accuracy
RA, Motivated by dimension reduction methods for dynamical systems, we develop a *cell-to-state* (cell2state) learning method that, through learning from such multi-modal data, maps single-cell gene expression profiles to low-dimensional state vectors that are predictive of cell dynamics
RA, We put forward a framework that contains both a receiver model and a sender model
RA, Based on our findings we proposed a new,  non-adversarial and non-generative method named \modelName: Augmentation Based Contrastive Disentanglement
RA, In this work, we present Object-Region Video Transformers (ORViT), an object-centric approach that extends video transformer layers with a block that directly incorporates object representations
RA, In this work, we propose a simple yet effective dataset condensation technique that requires significantly lower training cost with comparable performance by matching feature distributions of the synthetic and original training images in sampled embedding spaces
RA, Our method, Differentiable Symbolic Execution (DSE), learns such programs by sampling code paths using symbolic execution, constructing gradients of a worst-case "safety loss" along these paths, and then backpropagating these gradients through program operations using a generalization of the reinforce estimator
RA, To address these pain points, we propose a framework for resource- and parameter-efficient fine-tuning by leveraging the sparsity prior in both weight updates and the final model weights
RA, Under this model, latent correlation maximization is shown to guarantee the extraction of the shared components across views (up to certain ambiguities)
RA, In this paper, we design the collaborative three-stream transformers to model the interactions of objects, and the actions/relations of objects between different modalities
RA, In this paper, we develop label learning flow (LLF), a general framework for weakly supervised learning problems
RA, We provide a systematic analysis of the sources of uncertainty in the noisy supervision that occurs in RL, and introduce inverse-variance RL, a Bayesian framework which combines probabilistic ensembles and Batch Inverse Variance weighting
RA, In this framework, given a query graph and a graph database, the goal is to identify subgraphs of the database graphs that are structurally similar to the query
RA,  We address this tension by introducing a simple and effective method for improving robustness:  ensembling the weights of the zero-shot and fine-tuned models (WiSE-FT)
RA, To achieve this goal, we assume that the perception of a deposition device is limited and can capture the process only qualitatively
RA, To overcome this barrier, we consider a scenario where ``advice'' is provided to help perform clustering
RA,We present a novel framework for Distributing Black-Box Optimization (DiBB)
RA, With this intuition, we present a two-stage weakly-supervised contrastive learning approach
RA,We describe a novel architecture for modeling the cost-to-go function in approximate dynamic programming problems involving country-scale, real-life electrical power generation systems
RA, To tackle these problems, we propose DEGREE (Decomposition based Explanation for GRaph nEural nEtworks) to provide a faithful explanation for GNN predictions
RA, We study the use of deep generative models that generate multimodal data from latent representations
RA, The second stage is a slower reflection stage where we ask the network to reflect on its feed-forward decision by considering and evaluating all available choices
RA, To better understand the utility of deep models in RL we present an analysis of recursive value estimation using overparameterized linear representations that provides useful, transferable findings
RA,We present the first provable Least-Squares Value Iteration (LSVI) algorithm that achieves runtime complexity sublinear in the number of actions
RA, Instead, we leverage the explicit description afforded by the NTK to maximally perturb the output of the model, using solely information about the model structure and the training data
RA,In this paper we propose a new generative model of text, Step-unrolled Denoising Autoencoder (SUNDAE), that does not rely on autoregressive models
RA, Our key insight is that regardless of how bad data are generated, they tend to contribute little to training a model with good prediction performance or more generally, to some utility function of the data analyst
RA, In this work, we introduce a set of experiments to deepen our understanding of shortcut learning and its implications
RA, In this paper, we propose to solve the distant goal-reaching task by using search at training time to generate a curriculum of intermediate states
RA,We analyze neural networks composed of bijective flows and injective expansive elements
RA, Inspired by nativism, we directly model very basic human innate priors in abstract visual tasks like character or doodle recognition
RA, Second, we propose a new pre-training task, non-contrastive region-matching, which allows the model to capture fine-grained region dependencies and as a result significantly improves the quality of the learned vision representations
RA,We present AutoOED, an Automated Optimal Experimental Design platform powered by machine learning to accelerate discovering solutions with optimal objective trade-offs
RA, To this end, we explore a generic program embedding approach that aim at solving multiple program analysis tasks
RA, We point out a major challenge in this problem setting: that common mechanisms for enforcing DP in deep learning, which require injecting real-valued noise, are fundamentally incompatible with MPC, which exchanges finite-field integers among the participants
RA, To address this issue, Few-Shot NAS reduces the level of weight-sharing by splitting the One-Shot supernet into multiple separated sub-supernets via edge-wise (layer-wise) exhaustive partitioning
RA, We present a novel method, CrossMatch, for semi-supervised object detection
RA, In this work, we design an acoustically complex environment in which, besides the target sound, there exists a sound attacker playing a zero-sum game with the agent
RA, In this paper, we propose a systematic approach, conceptual counterfactual explanations (CCE), that explains why a classifier makes a mistake on a particular test sample(s) in terms of human-understandable concepts (e,g, this zebra is misclassified as a dog because of faint stripes)
RA, To address this, we propose the HD-cos network that uses 1) cosine as activation function, 2) the Hadamard-Diagonal transformation to replace the unstructured linear transformations
RA,Drawing inspiration from gradient-based meta-learning methods with infinitely small gradient steps, we introduce Continuous-Time Meta-Learning (COMLN), a meta-learning algorithm where adaptation follows the dynamics of a gradient vector field
RA, We find that a careful architecture and hyperparameter decisions yield a recurrent model-free implementation that performs on par with (and occasionally substantially better than) more sophisticated recent techniques in their respective domains
RA, This paper introduces a novel attacking method to find the optimal attacks through collaboration between a designed function named "actor" and an RL-based learner named "director'"
RA, We particularly investigate the effect of curriculum learning, which, inspired by human curricula, leverages a guided learning regime to improve model generalization and has been found to improve predictive performance in the aforementioned cases
RA,We present Sequential Neural Variational Inference (SNVI), an approach to perform Bayesian inference in models with intractable likelihoods
RA, We then propose a new learning paradigm, asymmetry learning, that identifies which symmetries the classifier must break in order to correctly predict Y in both train and test
RA, This paper presents a different perspective to evaluate how confidence and uncertainty estimators behave under distribution shifts, focusing on the biomedical imaging domain
RA, We propose a novel architecture for synthetically generating time-series data with the use of Variational Auto-Encoders (VAEs)
RA,In this paper, we propose THOMAS, a joint multi-agent trajectory prediction framework allowing for efficient and consistent prediction of multi-agent multi-modal trajectories
RA, In this work, we decompose input features into low-frequency and high-frequency signals and analyze the performance of different frequencies on GNNs as the depth increases
RA, This ambiguity biases models towards a single prediction, which could result in the suppression of classes that tend to co-occur in the data
RA,  To tackle this challenge, we formalize the RED problem and identify a set of principles crucial to the RED approach design
RA, To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices
RA, To reflect such demands, we introduce the first-occupancy representation (FR), which measures the expected temporal discount to the first time a state is accessed
RA,  In this paper, we combine some aspects of classic techniques for data-association filtering with modern attention-based neural networks to construct object-based memory systems that consume and produce high-dimensional observations and hypotheses
RA, To handle the limited-data problem in few-shot regimes, recent methods tend to collectively use a set of local features to densely represent an image instead of using a mixed global feature
RA, We find that as more protected attributes are introduced to a task, it becomes more important to leverage the protected attribute labels during training to promote fairness
RA,  In this work, we address this challenging problem by developing an algorithm that exploits the offline demonstration data generated by {a sub-optimal behavior policy for faster and efficient online RL in such sparse reward settings
RA, Motivated by the recent success of these approaches, we present SURF, a semi-supervised reward learning framework that utilizes a large amount of unlabeled samples with data augmentation
RA, However, the proposed strategy (named FLSD-53) is based on simple heuristics which, when selecting the \gamma, does not take into account any knowledge of whether the network is under or over confident about such samples and by how much
RA, In this paper, we first establish an analytic framework to investigate ViT from the spectrum domain
RA, This type of compositional reasoning permits reuse of the subproblem solutions when tackling future tasks that share part of the underlying compositional structure
RA, In this work, we introduce Partial G-CNNs: a family of equivariant networks able to learn partial and full equivariances from data at every layer end-to-end
RA, To facilitate the learning of useful control flow, we propose two modifications to the Transformer architecture, copy gate and geometric attention
RA, In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model's decision boundary during training
RA, In others our results are SOTA relative to published methods similar to or identical to our own -- in some cases by wide margins, but below SOTA absolute
RA, Specifically, we introduce a loss combining three terms: the summed squared error, the focal loss, and a regularization penalty
RA, This is possible because it utilizes the dataset of text-image pairs where the text provides the source of compositionality
RA, We report an amortized approach to generate synthetic pathways as a Markov decision process conditioned on a target molecular embedding
RA, In this paper, we tackle this issue by proposing to learn a group of parameterized synperiodic filter banks
RA, In this paper, we present and adapt to mini-batch training on GPUs a family of BP-based message-passing algorithms with a reinforcement field that biases distributions towards locally entropic solutions
RA,We present Reward-Switching Policy Optimization (RSPO), a paradigm to discover diverse strategies in complex RL environments by iteratively finding novel policies that are both locally optimal and sufficiently different from existing ones
RA, To tackle this problem, we propose a novel self-supervised learning technique for AD in time series, namely DeepFIB
RA, We address these challenges by introducing a novel approach, Scale-Invariant Teaching (SIT), which is a simple yet effective end-to-end knowledge distillation framework robust to large object size variance and class imbalance
RA, We define a partial order on the space of weight-data alignments and prove that generalization performance improves in response to stronger alignment
RA,We present a new framework AMOS that pretrains text encoders with an Adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators
RA, To avoid these points, directions of negative curvature can be utilized, which requires computing the second-derivative matrix
RA, To address this challenge, we leverage valuable information from the past: in particular, data collected in past traversals of the same scene
RA,We explore a new perspective on video understanding by casting the video recognition problem as an image recognition task
RA, To overcome such problems, we establish a new class of tractable lossless compression models that permit efficient encoding and decoding: Probabilistic Circuits (PCs)
RA, To accelerate inference, recent work has been exploring non-autoregressive (NAR) approaches that translate blocks of tokens in parallel
RA, We introduce a minimal space of datasets with systematic and non-systematic features in both the input and output
RA, Our contribution lies in carefully re-designing those heterogeneous technologies and proposing our unified framework
RA, To understand the power of GIA, we compare it with GMA and find that GIA can be provably more harmful than GMA due to its relatively high flexibility
RA, We posit that a suitable state abstraction should depend on the capabilities of the available lower-level policies
RA, We present an efficient procedure, designed specifically to defend against an adaptive RL adversary, that can directly certify the total reward without requiring the policy to be robust at each time-step
RA, On six distribution shift datasets, we find that simply ensembling the standard and robust models is a strong baseline---we match the ID accuracy of a standard model with only a small drop in OOD accuracy compared to the robust model
RA, We introduce Neural Parameter Allocation Search (NPAS), a novel task where the goal is to train a neural network given an arbitrary, fixed parameter budget
RA, To gain insight into the utility of this operation, we studied the effects on AlexNet of a local divisive normalization between features, with learned parameters
RA, We posit that safety is a key reason behind the demand for explainability
RA, Second, we show that, for analytic kernels, like Gaussian and inverse multiquadric, target kernel KT admits maximum mean discrepancy (MMD) guarantees comparable to square-root KT without the need for an explicit square-root kernel
RA, This weight function adds the flexibility of putting more emphasis on different parts of the observation-conditioned class probability (i,e, P(Y|X)) range during training
RA, In this work, we propose a novel way to establish such a link by corpus transfer, i,e, pretraining on a corpus of emergent language for downstream natural language tasks, which is in contrast to prior work that directly transfers speaker and listener parameters
RA, To address this, we propose LG2AR, Learning Graph Augmentations to Learn Graph Representations, which is an end-to-end automatic graph augmentation framework that helps encoders learn generalizable representations on both node and graph levels
RA, We compare several representation learning methods that can be applied to this setting, using the latent space of the generator to generate multiple "views" of the same semantic content
RA, The proposed two methods (FCL, ICL) can be combined synthetically, called Zero-CL, where ``Zero'' means negative samples are zero relevant, which allows Zero-CL to completely discard negative pairs i,e, with zero negative samples
RA, Based on such observations, we propose the spherical message passing (SMP) as a novel and powerful scheme for 3D molecular learning
RA, Motivated by the success of MSG, we investigate whether efficient approximations to ensembles can be as effective
RA, Particularly, we leverage the gradient of the energy function w,r,t, the discrete data space to approximately construct the provable optimal proposal distribution, which is subsequently used by importance sampling to efficiently estimate the original ratio matching objective
RA, Second, we design a joint-training scheme with self-supervision methods for the GAN-Inversion encoder and the generator
RA, We find that the decision boundary lies close to input samples in a large subspace, where the distance to the boundary grows smoothly and sub-linearly as one increases the dimensionality of the subspace
RA, 2) To encourage rare class training, whose model is low-recall but high-precision that discards too many pseudo-labeled data, we propose Class-Aware Imputation (CAI) that dynamically decreases (or increases) the pseudo-label assignment threshold for rare (or frequent) classes
RA, Then, we propose our ZO-RL algorithm, i,e, using deep deterministic policy gradient, an actor-critic RL algorithm to  learn a  sampling policy which can guide the generation of perturbed vectors in getting ZO gradients as accurate as possible
RA, To address this problem, we introduce a novel generative model for behavior cloning, in a mode-separating manner
RA,  We found that the discriminability of robust representation and texture model images decreased to near chance performance as stimuli were presented farther in the periphery
RA, We showed that the introduction of DPO into DNN modules and Attention modules can respectively benefit two main tasks in CTR prediction, enhancing the adaptiveness of feature-based modeling and improving user behavior modeling with the instance-wise locality
RA, To reduce the additional computation caused by recursive operation while maintaining the superior accuracy, we propose an approximating method through multiple sliced group self-attentions across recursive layers which can reduce the cost consumption by 10~30% with minimal performance loss
RA, We propose two approaches for selecting the class attributes: external attributes from the dataset annotations and internal attributes from the clustered latent space of the encoder
RA, Specifically, the proposed method concatenates a set of latent labels (instead of actual labels) to the text tokens, inputs them to BERT, then maps the contextual encodings of these latent labels to actual labels cooperatively
RA, We evaluate Shifty-ttest, an implementation of Shifty based on Student’s 𝑡-test, and, using a real-world data set of university entrance exams and subsequent student success, show that the models output by our algorithm avoid unfair bias under demo-graphic shift, unlike existing methods
RA, In the experimental results, we explore the effects of the data missing probability, the number of agents, and the number of pre-training episodes for GAIN on the performance of IA-MARL
RA,  We propose a Deep Neural Network-based recommender that uses the knowledge graph embeddings to recommend health nudges for maximizing engagement by combating the cold-start and sparsity problems
RA, Our methods are particularly suitable for evaluating MI in deep generative models, since explicit forms for the marginal or joint densities are often available
RA, Specifically, instead of using data augmentation approach, SynCLR employs data synthesis for multi-view generation
RA, This yields a unified perspective on how negative samples and SimSiam predictor alleviate collapse and promote dimensional de-correlation
RA, Motivated by this finding, we propose Separable Reweighted Adversarial Training (SRAT) to facilitate adversarial training under imbalanced scenarios, by learning more separable features for different classes
RA, We observe that a mostly unified encoder for vision and language signals outperforms all other variations that separate more parameters
RA, Specifically, small gradients have larger relative errors than large ones, thus having a higher probability to be pruned
RA, Specifically, we propose a novel technique for identifying the model parameters that are mainly relevant to the restricted classes
RA, In order to overcome these shortcomings, we propose a novel defense, Oracle-Aligned Adversarial Training (OA-AT), to align the predictions of the network with that of an Oracle during adversarial training
RA, However, we found these techniques hurt the performance of tiny neural networks
RA, First, we construct a constraint set and derive a high probability bound for the group assignment to belong to the set
RA, Based on this characteristic, we develop a simple but effective algorithm GLATE to dynamically adjust the temperature value in the training phase
RA, The proposed method can achieve the same convergence rate as the SGD method for a single task when the catastrophic forgetting term which we define in the paper is suppressed at each iteration
RA, In this paper, we present a framework that can explain the implicit assumptions and also the simplifications made in the prior work
RA, This is accomplished without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss demonstrating a desirable degree of generality and flexibility
RA, We propose invariant Causal Representation Learning (iCaRL), an approach that enables out-of-distribution (OOD) generalization in the nonlinear setting (i,e, nonlinear representations and nonlinear classifiers)
RA, This attack formulation can be used to tackle hard instances where none of the existing adversarial attacks can succeed
RA, We first formulate the framework of Multi-Objective Online Convex Optimization, which encompasses a novel multi-objective dynamic regret in the unconstrained max-min form
RA, To this end, we propose ModeRNN, which introduces a novel method to learn structured hidden representations between recurrent states
RA, We initiate the passage node embedding from the FiD encoder and then use graph neural network (GNN) to update the representation for reranking
RA, We compare our method with baselines on 19 time-series and regression datasets, and our method achieves approximately 2x reduction in calibration error, comparable sharpness, and improved downstream decision utility
RA, The input to our method is trained GAN that can produce images in domain A and a single reference image I_B from domain B
RA, We show that this framework encompasses existing curriculum learning approaches such as difficulty-based data sub-sampling, data pruning, and loss re-weighting
RA, Second, we outline general `wrapper' multiclass-to-binary (M2B) algorithms that can be used to achieve confidence, top-label, and class-wise calibration, using underlying binary calibration routines
RA, To resolve this issue, we introduce several novel regularization techniques for training GANs with ViTs
RA, We propose Generalized Gaussian Diffusion Processes (GGDP), a family of non-Markovian samplers for diffusion models, and we show how to improve the generated samples of pre-trained DDPMs by optimizing the degrees of freedom of the GGDP sampler family with respect to a perceptual loss
RA, We present empirical results of our framework on both simulated and real-world datasets to demonstrate the benefits of our approach
RA, Interestingly, we find that quantization, when properly engineered, can enhance the effectiveness of contrastive learning
RA, The proposed method demonstrates superior performance than the state-of-the-art algorithms in offline MARL
RA, Furthermore, we develop a smoothing method to further improve PipeGCN's convergence
RA, To obtain strong task-specific generative models, we either fine-tune a large language model (LLM) on inputs from specific tasks, or prompt a LLM with a few input examples to generate more unlabeled examples
RA, We first define the nuisance-varying family, a set of distributions that differ only in the nuisance-label relationship
RA, Specifically, our modifications in Fast AdvProp are guided by the hypothesis that disentangled learning with adversarial examples is the key for performance improvements, while other recipes (e,g,, paired clean and adversarial training samples, multi-step adversarial attackers) could be largely simplified
RA, Our method, named the HST initialization, can also be easily extended to the setting of differential privacy (DP) to generate private initial centers
RA, In particular, we propose adversarial invertible transformation, that can be viewed as a mapping from image to image, to encrypt data samples so that they become ''unlearnable'' by machine learning models with negligible loss of visual features
RA, This has a two-fold benefit: 1) it only requires 1 backward pass and 2) the cross-gradient information between the models promotes robustness against transferable attacks
RA, Our derived bounds bear an intimate connection with the spectrum of the Hessian at the optimum, and importantly, exhibit a double descent behaviour at the interpolation threshold
RA, Especially, we maximize the regularizer in the inner-loop to encourage the adapted-model to be more sensitive to the new task, and minimize the regularizer in the outer-loop to resist overfitting of the meta-model
RA, We treat objects as latent causes whose function to an agent is to facilitate efficient prediction of the coherent motion of their parts in visual input
RA, Additionally, we let the temporal latent state to 'reconstruct' the cluster assignment of the observation, thereby relieving the world model from modeling low-level details
RA, To make future work in real-world settings possible, we create new benchmarks for three large-scale settings
RA, In this paper, we refine this idea by estimating the density of these state-action pairs to distinguish neighbourhoods
RA, To enable learning long-horizon behaviors, recent works have explored leveraging prior experience in the form of offline datasets without reward or task annotations
RA, We also introduce a method to flexibly control the focus of training APPLE between global and local objectives
RA, We focus on a class of kernel learning problems (which includes the popular neural tangent kernel (NTK) learning as a special case) and propose a novel  multi-agent kernel approximation technique that allows the agents to distributedly estimate the full kernel function, and subsequently perform decentralized optimization, without directly exchanging any local data or parameters
RA, Specifically, we conduct controlled interpolation in latent space during training and ’reuse’ the encoder to help form a ’perfect disentanglement’ regularization
RA, Specifically, we propose a Data-Dependent GCN framework dubbed D^2-GCN which integrates data-dependent dynamic skipping at multiple granularities: (1) node-wise skipping to bypass aggregating features of unimportant neighbor nodes and their corresponding combinations; (2) edge-wise skipping to prune the unimportant edge connections of each node; and (3) bit-wise skipping to dynamically adapt the bit-precision of both the node features and weights
RA, We establish upper and lower bounds for both methods, which improves upon concurrently developed upper bounds for this problem via a fast rate generalization bound
RA, We find that just tuning LayerNorm parameters is a surprisingly effective baseline across the board
RA, Specifically,  \method directly models drug molecules in the geometric~(3D) space and performs geometric editing with the  knowledge guidance by self-training and simulated annealing in a purely training data free fashion
RA, To address this issue, we propose LOCo, an efficient deep Bayesian optimization framework which employs a novel regularizer to reduce the collision in the learned latent space and encourage the mapping from the latent space to the objective value to be Lipschitz continuous
RA, To solve this problem, we first propose a metric to quantify the sharpness of the model output
RA, We approach the condensation problem by imitating the GNN training trajectory  on the original graph through the optimization of a gradient matching loss and design a strategy to condense node futures and structural information simultaneously
RA, We explore different mechanisms to modulate the layers depending on the underlying architecture of a target network and the structure of the conditioning variable
RA, We used the framework of variational autoencoders to fit parameters of the mechanistic simulation constituting the generative model of the LVM to calcium imaging observations
RA, To address the information leakage malaise resulting from correlations among different tasks, we generalize the training process by incorporating an MTK decoupling process with a controllable trade-off between the protective efficacy and the model performance
RA, Our token reorganization method is integrated into ViT during training
RA, We discover that even a very simple graph neural counter can outperform all the existing GNN modules on CommonsenseQA and OpenBookQA, two popular QA benchmark datasets which heavily rely on knowledge-aware reasoning
RA, This approach overcomes the main drawback in the standard Taylor model arithmetic, i,e, its inability to handle functions that cannot be well approximated by Taylor polynomials, and significantly improve the accuracy and efficiency of reachable states computation for NNCSs
RA, This observation motivates us to adapt a concentration estimation algorithm that accounts for label uncertainty, resulting in more accurate intrinsic robustness measures for benchmark image classification problems
RA, We further develop a variant with improved communication efficiency and enhanced privacy
RA, We also proposed using a gradient origin network, a deep generative model that learns inferences without using an inference network, thereby reducing the need for additional parameters by introducing the surrogate posterior
RA, Our proposed data refinement approach is based on an ensemble of one-class classifiers (OCCs), each of which is trained on a disjoint subset of training data
RA, We also observe that our improvement over other methods is higher for smaller target datasets making it an effective tool for small data applications that may benefit from transfer learning
RA, This two-step hybrid policy presents human-friendly interpretations and achieves better performance in terms of generalization and robustness
RA, We further propose a series of sample encoding, decoding, and training loss functions applicable in this domain and demonstrate that our generic regression approach results in lower error than direct regression and specialized approaches while being suitable for various regression problems, network architectures, and evaluation metrics
RA, Further, we jointly model the set cardinality and output by listing the set size as the first element and taking advantage of the autoregressive factorization used by seq2seq models
RA, To address this obstacle, we make use of a key principle of many real-world graphs, i,e, homophily, or the principle that ``like attracts like,'' as the guidance to effectively search various self-supervised pretext tasks
RA, Our method leverages ideas from saddle-point optimization to derive stable first-order updates to solve a specific constrained optimization problem
RA, We then find the conditions under which this neural reparameterization could speed up convergence rates during gradient descent
RA, Specifically, we maintain a local knowledge memory based on the current context, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint to guide decoding via reinforcement learning
RA, By leveraging these insights, we deduce that a design principle identified to significantly improve predictive performance under heterophily—separate aggregators for ego- and neighbor-embeddings—can also inherently offer increased robustness to GNNs
RA, Specifically, we introduce a novel contrastive version of Multi-View Information Bottleneck (MIB) objective for temporal data
RA, Leveraging the structure of neural networks, we further propose two novel algorithms that change the exponent of the compute and memory requirements of the finite width NTK, dramatically improving efficiency
RA, We conduct extensive experiments on several high-dimensional datasets and show that our proposed CARD achieves the state-of-the-art certified robustness
RA, We provide (i) an autoencoder-decoder architecture that implements the M-mode SVD and (ii) a generalized autoencoder that employs a kernel activation and implements the doubly nonlinear Kernel-MPCA
RA, Thus, we derive a novel generalization bound for the prototypical network and show that focusing on the variance of the norm of a feature vector can improve performance
RA, We also provide a surprisingly simple method for constructing the orthogonal classifier (a classifier utilizing directions other than the given classifier)
RA, We devise a two-stage optimization problem that leads to a deterministic and interpretable solution for the optimal label smoothing
RA, We start by establishing several negative results, by highlighting failure cases of prior representation learning based approaches
RA, We propose an acquisition function that quantifies how much information a state-action pair would provide about the optimal solution to a Markov decision process
RA, Our modular method achieves SOTA performance (24.46%) with a substantial (8.17 % absolute) gap from previous work all while using less data by eschewing both expert trajectories and low-level instructions
RA, Our model consists of a multimodal Transformer encoder that jointly encodes UI images and structures, and performs UI object detection when the UI structures are absent in the input
RA, To deal with the exponentially large input space of combinatorial optimization problems, we designed WeaveNet to be highly parameter efficient while characterizing edges through stacked set-encoder with cross-concatenation operations
RA, In order to show attack potential by HYPOCRITE, this paper implemented a framework that makes homoglyph adversarial examples for natural language web services in the physical world and evaluated the performance under various conditions
RA, We maintain a fixed set of memory slots in our memory network and explore different designs to input new information into the memory, combine the information in different memory slots and decide how to discard old information
RA, We distill the policies learned with this reward signal on several tasks to produce one goal text conditioned policy
RA, Our method, Fixed Neural Network Steganography (FNNS),  achieves 0% error reliably for hiding up to 3 bits per pixel (bpp) of secret information in images and yields significantly lower error rates when compared to prior state of the art methods for hiding more than 3 bpp
RA,   We propose novel architectural modifications to the self-supervised feature learning step, that enable such compact ID distributions to be learned
RA, Our framework first approximates the distribution of the clients' aggregated data through cooperative multi-agent coordination
RA, We design effective attacks to degrade the MARL agent's performance by adversarially perturbing the states of agent(s) and solving an optimization problem
RA, Our method simply pulls the embedding of an instance closer to its nearest neighbors in a search space that is constrained using the additional knowledge
RA, Based on this observation, we propose Spectral Dynamics Embedding (SPEDE), which breaks the trade-off and completes optimistic exploration for representation learning by exploiting the structure of the noise
RA, Our core idea is to re-cast the DNN training as an explicit pruning-aware process: that is formulated with an auxiliary K-sparse polytope constraint, to encourage network weights to lie in a convex hull spanned by K-sparse vectors, potentially resulting in more sparse weight matrices
RA, We use these mechanisms to enable privacy-preserving multi-label learning by extending the canonical single-label technique: PATE
RA, Specifically, we first learn an invariant style classifier that takes out nuisance variation, and then introduce an orthogonal classifier that highlights the confounding cues
RA, We explore a novel hierarchical EM management strategy to address the forgetting issue
RA, Although the factorized matrices from our approach do not result in smaller reconstruction errors, we find that our resulting task accuracy is much closer to the original model's performance
RA, We apply our approach to two-sample tests, and on various benchmarks, we achieve superior test power compared to competing methods
RA, Our key insight is that by seeking approximate correspondence between elements of different sets, we learn strong representations that exclude the inactive factors of variation and isolate the active factors which vary within all sets
RA, We introduce a two-stage neural framework, which we label Neural Structure Mapping (NSM),  to learn visual analogies from Raven's Progressive Matrices, an abstract visual reasoning test of fluid intelligence
RA, Our implementation is available at https://anonymous.4open.science/r/firth_bias_reduction-2B60
RA, Specifically, we perform self-distillation on masked patch tokens and take the teacher network as the online tokenizer, along with self-distillation on the class token to acquire visual semantics
RA, By deriving a variational representation, we show that this framework is a fairness-aware objective and can be easily optimized by solving a joint minimization problem over the model parameters and a dual variable
RA, That is, this approach injects topics into these NLMs and trains them via topics behind these dependencies over segments, introducing both topic alignment (TA) and training tasks (TDM and TEM), while previous Transformer based NLMs are better at learning from the predefined segment length such as the context
RA, To address the problem, we propose a sample level weighting approach called Multi-variation Cosine Margin (MvCoM) which orthogonally enhances the conventional cosine loss function to incorporate the importance of training samples
RA, This is done by baking in a number of key control baselines in the evaluation method, particularly the blind guess (quantifying the dataset bias), the scratch model (quantifying the architectural contribution), and the gold standard (quantifying the upper-bound)
RA, We use this method across all possible pairings of 101 tasks in the UK Biobank dataset of retinal images, leading to \sim193k different models
RA, In this work, we design simple sequential tools for testing if the difference between source (training) and target (test) distributions leads to a significant drop in a risk function of interest, like accuracy or calibration
RA, Our extensions are supported by strong convergence theory in the smooth nonconvex and also Polyak-Łojasiewicz regimes
RA, We then systematically vary various aspects of the training setup to understand how they impact the data scaling laws
RA, We achieve this by extending local aggregation in MPNNs from star patterns to general subgraph patterns (e,g,, k-egonets): in our framework, each node representation is computed as the encoding of a surrounding induced subgraph rather than encoding of immediate neighbors only (i,e, a star)
RA, We then show how alternative choices for the loss yield a flexible family of acquisition functions for a variety of specialized optimization tasks, including variants of top-k estimation, level set estimation, and multi-valued search
RA,To allow the agent's sufficient exploration in the risky environments while ensuring the training safety, the human expert can take over the control and demonstrate how to avoid probably dangerous situations or trivial behaviors
RA, More generally, we propose a new way to compute fact embeddings and event probabilities for neural Datalog through time (Mei et al, 2020a), a neural-symbolic framework for event sequence modeling that automatically derives deep recurrent neural models from temporal logic programs
RA, While motivated differently, in this paper we carefully elucidate the similarity and differences of these methods, quantifying explicit situations where the solutions they produced may actually be equivalent and others where behavior diverges
RA, On top of the concept of disguised subnetworks, we propose a novel two-stage algorithm that plays a Peek-a-Boo (PaB) game to identify the disguised subnetworks with a combination of two operations: (1) searching efficiently for a subnetwork at random initialization; (2) unmasking the disguise by learning to transform the resulting subnetwork's remaining weights
RA, We form a conceptual framework to describe the likelihood of multi-step edits, and describe neural models that can learn a generative model of sequences based on these multi-step edits
RA, Our algorithm allows policy architectures to be learned with policy parameters via bilevel optimization using efficient policy-gradient methods, and thus does not require a pretrained oracle
RA,  We also present a method for novel classification task based on calibrating the centroid of the few-shot category towards the base classes
RA, We propose to use Gromov-Wasserstein and invariant optimal transport distances for cross-domain imitation, which enables us to align and compare states between the different spaces of the agents and solve the benchmark
RA, Our proposed use of distillation to only handle easy instances allows for a more aggressive trade-off in the student size, thereby reducing the amortized cost of inference and achieving better accuracy than standard distillation
RA, We show that the RKHS consists of additive models of interaction terms between patches, and that its norm encourages spatial similarities between these terms through pooling layers
RA, Thus, we design two types of reblurring loss functions for better deblurring
RA, To handle a large number of candidate matches in a dense correlation map, we develop a light-weight architecture with an effective positional encoding technique for matching
RA, Our proposed method inherits advantages of the original SDCA, including (i) exponential convergence (with respect to the outer iteration steps), and (ii) better dependency on the sample size and condition number than the full-batch gradient method
RA, We pack several environment instances, a local learner and buffer, and a carefully designed multi-queue manager which avoids blocking into a container
RA, To limit the privacy loss, we design an efficient knowledge distillation to reduce the time of distilling from the private data
RA, This makes the problem solvable via our augmented knapsack solver, enabling HALP to surpass prior work in pruning efficacy and accuracy-efficiency trade-off
RA, Specifically, we leverage a parsimonious graphical representation that characterizes structural relationships over variables in the RL system
RA, This is done in a two-step process: first, we develop a method that encodes unstructured image-like modality into a structured representation bifurcated by sensitive and non-sensitive representation
RA, This makes self-learning techniques highly attractive for any practitioner who applies machine learning algorithms in the real world
RA, Based on extensive experiments, we find that ViTs are weaker learners compared with CNNs against our Patch-Fool and the results from Sparse Patch-Fool, a sparse variant of our Patch-Fool, indicate that the perturbation density on each patch seems to be the key factor that influences the robustness ranking between ViTs and CNNs
RA, We apply it to multiple real-world unsupervised and supervised learning problems involving  domain shift
RA, The key to our approach is to ensure that with a low-rank feature subspace, a small number of attacked samples, and other mild assumptions, RVFR recovers the underlying uncorrupted features with guarantees, thus sanitizes the model against a vast range of backdoor attacks
RA, We establish the convergence and analyze the generalization performance
RA, Inspired by these discoveries, we develop a simple but strong baseline ``Meta-Vote Pruning (MVP)'' that significantly reduces the pruning iterations for a new task by initializing a sub-network from the pruned models for tasks similar to it
RA, Specifically, we standardize the base neural network architectures and benchmarking datasets, while also exploring more realistic model selection approaches that can work with no labeled data or few labeled samples
RA, We find that most often, a small portion of confusion neurons are able to effectively contaminate the pre-trained model
RA, We study different data augmentation strategies and model variances in the DM-CT framework
RA, We propose two novel EI-based algorithms for this problem, one when the reward function is assumed to be linear and the other when no assumption is made about the reward function other than it being bounded
RA, Under our proposed framework, we show a principled way to design integrated contrastive losses that simultaneously achieve good accuracy and robustness on downstream tasks
RA, We show that previous fairness learning algorithms designed for in-distribution fairness fail to meet the new robust fairness goal
RA, We realize this strategy with contrastive attraction and contrastive repulsion (CACR), which makes the query not only exert a greater force to attract more distant positive samples but also do so to repel closer negative samples
RA, This bypasses the introduction of explicit additional input of policies and mitigates the scalability issue meanwhile
RA, Specifically, we propose to enhance sparse point clouds generated from LiDAR with dense pseudo point clouds generated from depth completion
RA, Then, using this transformation, we propose a method that successfully estimate a parametric or nonparametric functions defined under the conditional moment restrictions
RA, This alignment compensates for both imbalance in the data as well as the eventual distributional shift present during evaluation
RA,  We fine-tune a pretrained encoder-decoder model (Raffel et al, 2020; Lester et al, 2021) on this multitask mixture covering a wide variety of tasks
RA, Inspired by neural implicit representations in 3D modeling, e,g, NeRF, we propose a method to represent objects as neural implicit functions upon which we can define and jointly train interaction features
RA, We further propose a layer-wise modification for adapting LookSAM to the large-batch training setting (Look-LayerSAM)
RA, After that, we treat the synthesized examples as fair samples from the energy-based model and update the model parameters with the maximum likelihood learning gradient, while the normalizing flow directly learns from the synthesized examples by maximizing the tractable likelihood
RA, We show that LAQ can recover value functions that have high correlation with value functions learned using ground truth actions
RA, We use self-training in conjunction with meta-learning for re-weighting noisy pseudo-prompt labels
RA, Such decomposition enables both the global and local components to make efficient use of interaction data and independently generalize
RA, For this purpose, we develop an efficient algorithm that solves multiple bi-objective optimization problems with distinct constraints defined by reference vectors targeting diverse regions of the Pareto front
RA, Our novel Tmean(·) rules are derived from Mean(·) by appropriately trimming some of the values before averaging them
RA, In contrast, we propose to co-design the sequence and 3D structure of CDRs as graphs
RA, We formulate a budget-constrained, end-to-end optimization framework, targeting jointly learning model weights, layer-wise pruning ratios/masks, and skip configurations, under a distillation loss
RA, We assume that the generative mechanisms underlying the temporal point processes are governed by a set of first-order temporal logic rules, as a compact representation of domain knowledge
RA, Our method, called ModInv or model invariance, argues for learning using multiple predictors and a single  representation, creating a bottleneck architecture
RA,  We find appropriate bases for positional encoding on manifolds through generalizations of Fourier series
RA, Inspired by these observations, we propose Sparse and Local Neural Logic Machines (SpaLoc), a structured neural network for hypergraph reasoning
RA, Along this line of finding, we further propose two low-rank, closed-form solutions, derived from carefully generalizing Frobenius-norm based regularizers
RA, To test our proposal, we show in a few-shot image generation task, that having a prototype memory during attention can improve image synthesis quality, learn interpretable visual concept clusters, as well as improve the robustness of the model. Interestingly, we also find that our attentional memory mechanism can implicitly modify the horizontal connections by updating the transformation into the prototype embedding space for self-attention. 
RA, Our framework also provides a tuning hyper-parameter that nicely trades off generation quality and control satisfaction, enabling practitioners to easily adjust it to meet their needs
RA, We propose a new measure of connectivity ---the relative connection strengths between same and different classes across domains---that governs the success of contrastive pre-training for domain adaptation in a simple example and strongly correlates with our results on benchmark datasets
RA, Our key insight is to leverage recent advances in generative modeling to capture the set of similar individuals in the generative latent space
RA, Our framework is based on the novel integration of neural process,  deep sequence model and active learning
RA, Motivated by this observation, we propose an Information Fusion (IF) approach to fine-tune the NTM based on estimated PTM
RA, In addition, we compared the proposed method with the existing methods through the intersection over union (IoU) metric commonly used in segmentation tasks and demonstrated the superiority of our method for anomaly segmentation
RA, Besides, we propose D^2ULO as a solution that solves both issues, which leverages the idea of domain adaptation (DA) to train a data utility model that can effectively predict the utility for any given unlabeled data in the target domain once labeled
RA, Built on the celebrated Douglas-Rachford splitting, our method tackles the original OT problem directly instead of solving approximate regularized problems, as many state-of-the-art techniques do
RA, Based on ALD, we construct a new deep latent variable model named the Langevin autoencoder (LAE)
RA, Although simple, we show that this idea leads to surprising improvements on a variety of adaptation scenarios without access to task reward, including changes in camera poses from the challenging distractor control suite
RA, Of independent interest, we develop a new algorithm for solving L_p regression for arbitrary matrices, which is significantly faster in practice for every p\ge4
RA, Moreover, we propose to learn action representations to effectively reduce the influence of payoff functions' estimation errors on graph construction
RA, Our method is generator-agnostic, producing strong segmentation results with a wide range of different GAN architectures
RA, Utilizing the existing reward-free RL solvers, our framework provides sharp sample complexity results for constrained RL in the tabular MDP setting, matching the best existing results up to a factor of horizon dependence; our framework directly extends to a setting of tabular two-player Markov games, and gives a new result for constrained RL with linear function approximation
RA, Next we use proxy distributions to significantly improve the performance of adversarial training on five different datasets
RA, Further, we extend the decentralized approach to sequential decision-making problems where we show in 13 continuous control benchmark environments that it matches or outperforms the state-of-the-art CEM algorithms in most cases, under the same budget of the total number of samples for planning
RA, In this work, we address these challenges by first designing a principled evaluation framework that enables a quantitative comparison of SDMs across 1,235 slice discovery settings in three input domains (natural images, medical images, and time-series data)
RA, We find both static and dynamic sparse methods to yield win-win: substantially shrinking the robust generalization gap and alleviating the robust overfitting, meanwhile significantly saving training and inference FLOPs
RA, To mitigate this problem, we propose the tessellated 2d convolution network, a novel divide-and-conquer based approach, which first independently learns the abstract representations of non-overlapping regions within an image, and then learns how to combine these representations to infer its class
RA, This policy decomposition allows us to combine global decisions about which parts of the game space to return to with curiosity-based local exploration in that space, motivated by how a human may approach these games
RA, Specifically, we assess the robustness of EEGNet which is the current state-of-the-art network for embedded BCIs
RA, To further understand the phenomena, we study the cost distributions of deep planners and found  planning instances can have heavy-tailed runtime distributions, with tails both on the right-hand and left-hand sides
RA, We propose new experimental protocols to support the analysis and guide the practice of training Transformers, built around the rich theory of learning sparse Boolean functions
RA, To quantify this we introduce a decoding bottleneck: information must be captured by simple predictors, mapping concepts to clusters of data formed in representation space
RA, In this work, we propose ImaginE, an imagination-based automatic evaluation metric for natural language generation
RA, We further propose a variant of CLA termed Adaptive Cross-Layer Attention (ACLA)
RA, We propose a novel approach that intertwines model aggregations with permutations of local models
RA, In this paper, we introduce a novel task, pairwise 3D geometric shape assembly, and propose Neural Shape Mating (NSM) to tackle this problem
RA, Specifically, we propose the Equivariant Vector Field Network (EVFN), which is built on a novel tuple of equivariant basis and the associated scalarization and vectorization layers
RA, To mitigate this issue, we perform an eigendecomposition of the graph and propose to learn multiple adaptive polynomial filters acting on different subsets of the spectrum
RA, Such property results in a new attack surface of the NMT system—an adversary can slightly changing inputs to incur a significant amount of redundant computations in NMT systems
RA, To address them, we present density based approaches to perform alignments, and we complement them with our validation criteria accounting for downstream task performances
RA, Specifically, we train a score-based generative model on medical images to capture their prior distribution
RA, To tighten the certification, we also propose different partition and aggregation protocols to train robust policies
RA, Based on these observations, we propose SWARM Parallelism (Stochastically Wired Adaptively Rebalanced Model Parallelism) — a model-parallel training algorithm designed for swarms of poorly connected, heterogeneous unreliable devices
RA, We show that SRG can be extended to combine the benefits of both importance-sampling-based preconditioning and variance reduction
RA, This formulation unifies various machine learning settings: the weak beliefs can come in the form of noisy or incomplete labels, likelihoods given by a different prediction mechanism on auxiliary input, or common-sense priors reflecting knowledge about the structure of the problem at hand
RA, Specifically, we first use graphs within the same class to estimate a graphon
RA, We then develop a structured sequential Variational Auto-Encoder to estimate the environment model and extract ASRs
RA, This paper proposes a stochastic attention mechanism for NPs to capture appropriate context information
RA, This approach provides effective solutions to both model verification and authorization
RA, This property makes NeRFs hard to generalize to new scenarios, including new lighting or new arrangements of objects
RA, In particular, we first obtain short-horizon skills for using each individual tool from a gradient-based optimizer and then learn a neural skill abstractor from the demonstration videos; Finally, we plan over the skills to solve the long-horizon task
RA, We observe that in most cases, we need both a suitable domain generalization algorithm and a strong GNN backbone model to optimize out-of-distribution test performance
RA, We first present Distributional Decision Transformer (DDT) and its two practical instantiations, Categorical and Gaussian DTs, and show that these simple modifications to DT can enable effective offline state-marginal matching that generalizes well to unseen, even synthetic multi-modal, reward or state-feature distributions
RA, In contrast to prior work in this direction, our framework is trained end-to-end and thus avoids instabilities and constraints associated with the commonly-used Laplace-Beltrami basis or sequential optimization schemes
RA, To fully exploit the immense design space via an accurate predictor, we identify the importance of carrying feature engineering on searchable features to improve neural architecture representations and propose a Wide & Deep Predictor to unify dense and sparse neural architecture representations for lower error in performance prediction
RA, We introduce the Representation Topology Divergence (RTD) score measuring the dissimilarity in multi-scale topology between two point clouds of equal size with a one-to-one correspondence between points
RA, We also analyze a more convenient C-MinHash variant which reduces two permutations to just one, with extensive numerical results to validate that it achieves essentially the same estimation accuracy as using two permutations with rigorous theory
RA, We derive the TE error bounds that enable representations balanced for treatment groups conditioned on individualized features
RA, We propose a Federated Expectation-Maximization algorithm enhanced by Temporal priors of the shifting distribution (FedTEM), which jointly learns a mixture model to infer the mode of each client, while training a network with multiple light-weight branches specializing at different modes
RA, We then apply the proposed node distance to define a graph Wasserstein distance on tree edit embedding space exploiting Optimal Transport framework
RA, Along the way, we identify a vanishing-gradient issue with all existing formulations of AWP and we propose Weighted Truncated AWP (WT-AWP) to alleviate this issue
RA, Our method, which we call Equivariant representations for RL (EqR), outperforms many previous methods in a similar setting by achieving a median human-normalized score of 0.418, and surpassing human-level performance on 8 out of the 26 games
RA, We generalize the existing adversarial learning framework with a novel graph discriminator using encoding-conditioned graph embeddings
RA, We prove upper bounds on these quantities for shallow and deep neural networks, drastically lessening the curse of dimensionality
RA, To achieve that, we propose Momentum adversarial Domain Invariant Representation learning (MoDIR), which introduces a momentum method in the DR training process to train a domain classifier on the source versus target, and then adversarially updates the DR encoder to learn domain invariant representations
RA, This makes it possible to use inheritance to share and reuse research, as developing a method for a given setting also makes it directly applicable onto any of its children
RA, Our algorithm converges even in settings where VOGN does not converge
RA, In this framework, the prompt pool explicitly manages task-invariant and task-specific knowledge while maintaining model plasticity
RA, Specifically, we present a family of team games whose induced utility is non-multilinear with non-attractive per-se mixed Nash Equilibria, as strict saddle points of the underlying optimization landscape
RA, Second, towards understanding the robustness of SSL, we hypothesize that SSL learns richer features from frequent data: it may learn label-irrelevant-but-transferable features that help classify the rare classes and downstream tasks
RA, Specifically, much stronger smoothness is used on the perturbed examples farther away from the decision boundary to achieve better robustness, while weaker smoothness is on those closer to the decision boundary to avoid incorrect classification on clean samples
RA, We design a CrossMatch approach to improve the performance of SDG methods on identifying unknown classes by leveraging a multi-binary classifier
RA, We extend the graph topology to be state-dependent, formulate the graph selection as an imaginary agent, and finally derive an end-to-end learning paradigm from the unified Bellman optimality equation
RA, We propose a tractable information-theoretic acquisition function—the reducible heldout loss—to efficiently choose training points that maximize information about a holdout set
RA, On CIFAR-10 and CIFAR-100, we achieve accuracies of 97.56% and 84.02% with Wide-ResNet-28-10 which are 0.1% and 0.5% better than the previous state-of-the-art
RA, We propose a framework of construction for these Deep RKBS models and then provide a representer theorem for regularized learning problems
RA, We study this phenomenon by analyzing the loss landscape, finding that pre-trained weights appear to ease forgetting by leading to wider minima
RA, This is an extension to batch RL, allowing the agent to adapt to new situations without having to precommit to a policy
RA, In this paper, we introduce a principled approach for training generative models directly for training data generation purposes
RA, We run systematic experiments using ML APIs from Google, Microsoft, Amazon, IBM, Tencent, and other providers for tasks including multi-label image classification, scene text recognition, and named entity recognition
RA, Based on this relational generative model, we partition each edge into the summation of multiple relation-specific weighted edges, and use the weighted edges in each community to define a relation-specific GCN
RA, In this work, we proposed Dense-To-Sparse gate (DTS-Gate) for MoE training
RA, To deeply understand why finetuning learning rate holds such a critical role, we examine the theoretical reason behind through the lens of dynamical isometry, a nice property of networks that can make the gradient signals preserve norm during propagation
RA, We propose and analyze a quantizer called centered symmetric quantizer (CSQ), which preserves the symmetry of latent distribution by providing equal representations to the negative and positive sides of the distribution
RA, Our loss produces embeddings that achieve lift on three downstream applications by distinguishing strata: 4.4 points on coarse-to-fine transfer learning, 2.5 points on worst-group robustness, and 1.0 points on minimal coreset construction
RA, Our proposed algorithm, UC-DiffOSI, combines a UC trained on a wide range of environments with an Online System Identification module based on a differentiable physics engine (DiffOSI)
RA, We propose a statistical method to compute the lower and upper bounds of the MRE given a confidence threshold and a precision
RA, We propose Average Thresholded Confidence (ATC), a practical method that learns a threshold on the model's confidence, predicting accuracy as the fraction of unlabeled examples for which model confidence exceeds that threshold
RA, First, we extend the existing intervention-based casual discovery framework for videos to formulate the instantaneous change of the casual structure in a principled manner
RA, To derive a practical and scalable algorithm, we employ concepts from prior work on estimating the support of a probability distribution
RA, Using this topology, a state-independent hierarchical policy can select where the agent has to keep discovering skills in the state space
RA, We find that rather than using SPEN as a prediction network, using it as a trainable loss function is not only computationally efficient but also results in higher performance
RA, Furthermore, to make OVD-Explorer tractable for continuous RL, we derive a closed form solution, and integrate it with SAC, which, to our knowledge, for the first time alleviates the negative impact on exploration caused by aleatoric uncertainty for continuous RL
RA, In this setting, we derive closed-form analytical expressions for the evolution of generalization error over training
RA, Using these rules, we demonstrate the importance of experimental choices regarding the sequence of incoming data and the sequence of the task oracle
RA, For this purpose, we derive perturbation models for SAT and TSP
RA, Our framework, named Learnable Intrinsic-Reward Generation Selection algorithm (LIGS) introduces an adaptive learner, Generator that observes the agents and learns to construct intrinsic rewards online that coordinate the agents’ joint exploration and joint behaviour
RA, Our regularization method is also orthogonal to data augmentation methods, achieving the best performance when our method is combined with data augmentation
RA, We provide expressions for the tradeoff between  compression rate and the achievable distortion with and without shared common randomness between the encoder and decoder and demonstrate using the example of a binary source that shared randomness can strictly improve the tradeoff
RA, We introduce forget-and-relearn as a powerful paradigm for shaping the learning trajectories of artificial neural networks
RA, To remedy this we show that two more principle components are needed 1) iterative training where outputs are fed back as inputs 2) applying convolutions after conversion to log-polar space
RA, To address this issue we introduce HydraSum, a new summarization architecture that extends the single decoder framework of current models, e,g, BART, to a mixture-of-experts version consisting of multiple decoders
RA, Our method utilizes the black-box variational inference framework so that it can be applied to a wide variety of modern machine learning models, including the variational autoencoders
RA, We propose foveated Transformer (FoveaTer) model, which uses pooling regions and eye movements to perform object classification tasks using a vision Transformer architecture
RA, Based on connections to statistical mechanics, we propose a novel critically-damped Langevin diffusion (CLD) and show that CLD-based SGMs achieve superior performance
RA, We propose a more general collective certificate for the larger class of softly local models, where each output is dependent on the entire input but assigns different levels of importance to different input regions (e,g, based on their proximity in the image)
RA,  Our technique involves conditioning \( A \) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel
RA, Due to the nature of user comments, we introduce an attention-based mechanism that allows the model to disregard text with irrelevant content
RA, To this end, we study existing GGM metrics and neural-network-based metrics emerging from generative models of images that use embeddings extracted from a task-specific network
RA, Our method consists of three steps: few-shot amplification, distillation, and backtranslation
RA, We first propose a fast and light-weight method for adapting a Gaussian mixture density network (MDN) using only a small set of target domain samples
RA, Specifically, for a node, the common features contain the shared information with other nodes in hyperedges, while the personal features represent its special information
RA,  To tackle this problem, we introduce an interventional prediction module to find  Z belonging to the same environment
RA, Our method allows agents to learn from extraneousness-rich demonstrations by intelligently ignoring irrelevant components
RA, We then perform a proposed Partial Relation Transfer learning task on a novel data set, using a neural-symbolic autoencoder architecture that combines sub-symbolic representations with modular relation-decoders
RA,  Here, we study these issues in a principled way and propose a provable solution, a class of GNN layers termed PEG with rigorous mathematical analysis
RA, We address this problem by proposing a versatile grouped pruning framework where we first cluster filters from each convolutional layer into equal-sized groups, prune the grouped kernels we deem unimportant from each filter group, then permute the remaining filters to form a densely grouped convolutional architecture (which also enables the parallel computing capability) for fine-tuning
RA, We propose a novel formulation of these constraints using matroids, an algebraic structure that generalizes linear independence in vector spaces, and present an efficient greedy algorithm with constant approximation guarantees
RA, We first show that data augmentation, modeled as additive perturbations, speeds up learning by enlarging the smaller singular values of the network Jacobian
RA, Specifically, for a network, we create a recurrent parameter generator (RPG), from which the parameters of each convolution layer are generated
RA, This characterizes an absence of resilient connectivity in the pruned sub-network
RA, We discuss several properties of the trace function, and in addition, validate our proposed approach empirically in a range of environments, while comparing its performance against appropriate baselines
RA, We then rephrase optimizing recommendation as finding an intervention that best transports the patterns it learns from the observed domain to its intervention domain
RA, Our main challenge is to compare these item-consumption sequences of variable lengths and duration
RA, Our algorithm for learning CE integrates an adversarial bandit subroutine which minimizes a weighted swap regret, along with several novel designs in the outer loop
RA, We propose an objective function, Expected Minimum Cost (EMC), based on two key ideas: (1) when presenting a set of options to a user, it is vital that there is at least one low-cost solution the user could adopt; (2) when we do not know the user's true cost function, we can approximately optimize for user satisfaction by first sampling plausible cost functions, then finding a set that achieves a good cost for the user in expectation
RA, By converting the resulting probability back into a logit, we obtain a logit-space equivalent of the AND operation
RA, We introduce the Boltzmann policy distribution (BPD), which serves as a prior over human policies and adapts via Bayesian inference to capture systematic deviations by observing human actions during a single episode
RA, This enables us to prove that KD using only pairwise feature kernel comparisons can improve NN performance in such settings, with both single & ensemble teacher models, whereas standard training without KD fails to generalise
RA, To prevent this leakage, differential privacy (DP) is applied to offer a provable privacy guarantee on training data through gradient perturbation
RA, To this end, we train two curriculum policies together with RL: (1) a co-operative planning policy recursively decomposing a hard task into coarse-to-fine sub-task sequences as a tree; and (2) an adversarial policy modifying the environment (e,g,, position/size of obstacles) in each sub-task
RA, We explicitly train the model to incorporate incoming information into its world state representation, obtaining strong inductive generalization and the ability to handle extremely long-range dependencies
RA, Our surprising discovery shows that (1) model architectures play a significant role in unsupervised OOD detection performance; (2) unsupervised approaches applied on large-scale pre-trained models can achieve competitive performance compared to their supervised counterparts; and (3) unsupervised OOD detection based on Mahalanobis Distance with the support of a pre-trained model consistently outperforms other unsupervised methods by a large margin and compares favorably with results from state-of-the-art supervised OOD detection methods reported in the literature
RA, Our motivating observation is that worst-group performance is related to a representation alignment loss, which measures the distance in feature space between different groups within each class
RA, Our model approximates the binding pocket and predicts the docking pose using keypoint matching and alignment through optimal transport and a differentiable Kabsch algorithm
RA, We also propose an automatic searching algorithm that adapts the ATGP loss to PTO problems with different combinatorial structures
RA, We analyze the collected instructions as `natural programs', finding that while they resemble computer programs, they are distinct in two ways: First, they contain a wide range of primitives; Second, they frequently leverage communicative strategies beyond directly executable codes
RA, Inspired by MAML-based approaches, we formulate weighted adversarial training as a bilevel optimization problem where the upper-level task corresponds to learning a robust classifier, and the lower-level task corresponds to learning a parametric function that maps from a sample's multi-class margin to an importance weight
RA, Our first contribution is identifying and analyzing several key components in training efficient networks with the backpropagation (BP) algorithm: 1) softmax normalization in output layers may be one major cause of parameter explosion; 2) using log likelihood ratio (LLR) representation in output layers can reduce overfitting; 3) weight decaying and structural regularization can effectively reduce overfitting when ReLU activation is used
RA, This algorithm can be run on multiple machines, where each machine employs a gradient clipping scheme and communicate with other machines after multiple steps of gradient-based updates
RA, We then design controlled experiments to investigate the interconnections between data quality and problems in adversarial training
RA, We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, and observation space noise
RA, We apply it to the task of Face Verification, and obtain state-of-the-art results with all the above advantages
RA, Our findings shed new light on the relationship between scale and generalization
RA, We find that attention-based architectures that jointly process a featurized representation of the observation and the action, have a better inductive bias for exploiting semantic relationships for zero-shot coordination
RA, We set a video captioning as a dual learning task that reconstructs the input story from the sampled image sequence
RA, To this end, a scheme called source-target unified knowledge distillation (STU-KD) is developed in this paper
RA, Our GPU implementation enables training and analyzing economies with a large number of agents within reasonable time frames, e,g,, training completes within a day
RA, We show that NFK can be computed for both supervised and unsupervised learning models, which can serve as a unified tool for representation extraction
RA, For masked language modeling, NormFormer improves fine-tuned GLUE performance by 1.9% on average
RA, We propose a vector-quantized variational autoencoder structure as well as training techniques to learn a rigorous representation of gesture sequences
RA, Aside from immediate computational benefits, we discuss its properties, and show that it can lead to an efficient graph dictionary learning algorithm
RA, On the other hand, we formulate the personalized predictor as a lightweight adaptive module that is learned to minimize each client's empirical risk on top of the generic predictor
RA, Second, we interleave efficient local search heuristics with the usual RL training to smooth the value landscape
RA, Our method alleviates problems with local minima through a smooth critic function, avoids vanishing/exploding gradients through a truncated learning window, and allows many physical environments to be run in parallel
RA, We identify two issues in the existing layer-wise sampling methods: sub-optimal sampling probabilities and the approximation bias induced by sampling without replacement
RA, Using this framework, we observe and explain differences in behavior across natural and synthetic federated datasets, indicating that dataset synthesis strategy can be important for realistic simulations of generalization in federated learning
RA, We introduce a novel contrastive loss function that enables us to steer compositional models over logical features using supervised learning
RA, To optimize the framework, we propose the functional space (F-space) to represent input-output examples and the programs and model the similarity between them in a continuous and differentiable way
RA, To help remedy this problem, we introduce NAS-Bench-Suite, a comprehensive and extensible collection of NAS benchmarks, accessible through a unified interface, created with the aim to facilitate reproducible, generalizable, and rapid NAS research
RA, We derive a novel lower bound estimate for the mutual information which combines a particle estimator for state entropy to generate diverse behaviors and contrastive learning to distill these behaviors into distinct skills
RA, Therefore, we propose the DeCorr, a general framework to effectively reduce feature correlation for deeper GNNs
RA, We learn a transferable reward signal formulated using the exemplary set by ordinal metric learning
RA, We propose a more direct approach, whereby the distribution of full-episode outcomes is optimized to maximize a chosen function of its cumulative distribution function (CDF)
RA, In practice, our analysis also suggest solving the DDG problem in a meta-learning manner, which leads to directional prototypical network, the first method for the DDG problem
RA, With the use of an attentive set encoder, we propose to meta learn either diagonal or diagonal plus low-rank factors to efficiently construct task specific covariance matrices
RA, We observe that during message passing, nodes that are incorrectly classified (error nodes) also end up adversely affecting the representations of other nodes in their neighborhood
RA, Our deep model takes in local range image patches and predicts the next pixel value in a raster-scanning manner
RA, In addition, we further evaluate three pre-training tasks of language models, which is important for cross-modal but seldom investigated in previous studies
RA, Our method establishes a quantitative space transformation to address the ``crowding problem" in DR; with the proposed equivalent extended distance (EED) and function distortion (FD) theory, we are able to match the capacity of high-dimensional and low-dimensional space, in a principled manner
RA, Next, we introduce a synthetic data generation process that systematically controls the amount of style vs  content in each sample- i,e, information that is irrelevant vs  content in each sample- i,e, information that is irrelevant vs relevant to the downstream task- to elucidate how graph representation learning methods perform under different dataset conditions
RA, In addition, we perform a case study on incorporating the probabilistic robustness verification during training for the first time
RA, We also incorporate the content information into the discriminator, together with semantic features, allowing the discriminator to make a more reliable prediction
RA, We formulate the EL problem as a simple binary classification task, so that common end-to-end approaches aligned with the dominant empiricist view of machine learning could, in principle, solve it
RA, We report the likelihood, which is a much used metric in the image domain but rarely, and often incomparably, reported for speech models
RA, This approach thus aims at first learning functions that are a decomposition of the original signal and then fusing them to generate the learned signal
RA, The key idea of our approach is to integrate optimal transport-based contact points discovery into the differentiable physics solver to overcome the local minima from initial contact points or contact switching
RA, We further design an auxiliary neural network, KLoSNet, to learn a refined criterion directly aligned with the evidential training objective
RA, This allows us to transplant individual neurons and layers into another network and still maintain their functionality
RA, We retrofit it by making use of a recent advance in computational geometry called Cluster-induced Voronoi Diagram (CIVD)
RA, We propose a link prediction algorithm based on a new pooling scheme called WalkPool
RA, We define a new audio-visual navigation sub-task, where agents are evaluated on novel sounding objects, as opposed to unheard clips of known objects
RA, Here, we divert much more strongly from conventional VAE training: We ask if it is also possible to keep the discrete nature of the latents fully intact by applying a direct, discrete optimization for the encoding model
RA,  We showcase that such competition can result in interesting phenomena by proving the possibility of phase transitions from stability to instability and eventually chaos
RA, Our PI3NN method calculates PIs from linear combinations of three NNs, each of which is independently trained using the standard mean squared error loss
RA, To address the challenging setup and evaluation protocol, we propose an effective method that employs a new memory management scheme and novel learning techniques
RA, We then show that the limit points of non-convex subgradient flows can be identified via primal-dual correspondence in this convex optimization problem
RA, We thus propose a training algorithm, balanced multi-modal learning, and demonstrate that it indeed addresses the issue of greedy learning
RA, We formulate and analyze threat models for transductive-learning based defenses, and point out important subtleties
RA, The key of our method is a variational formulation of the objective function, which makes it possible to realize the JKO proximal map through a primal-dual optimization
RA, We present a method that adds new neurons during training without impacting what is already learned, while improving the training dynamics
RA, We show that our new approach, Conditional Expectation based Value Decomposition (CEVD) outperforms NeurADP by up to 9.76% in terms of overall requests served, which is a significant improvement on a city wide benchmark taxi dataset
RA, Specifically, we propose (1) representing the neural network learning process as a time-evolving graph (i,e, a series of static graph snapshots over epochs), (2) capturing the structural changes of the NN during the training phase in a simple temporal summary, and (3) leveraging the structural summary to predict the accuracy of the underlying NN in a classification or regression task
RA, Motivated by this observation, we then provide privacy guarantees for hyperparameter search procedures within the framework of Renyi Differential Privacy
RA, Our key insight is that in-distribution samples will be more “similar” to each other compared to OOD samples, not just over the encoding latent-space but also across time
RA,  We use RAINDROP to classify time series and interpret temporal dynamics of three healthcare and human activity datasets
RA, To show the vulnerability of Neural ODEs against adversarial energy-based attack, we propose NODEAttack
RA, We are motivated by abundant real-world examples, ranging from power and water systems to brain networks, in which practitioners do not have access to fine-scale knowledge of the network
RA, We propose Cross-Trajectory Representation Learning (CTRL), a method that runs within an RL agent and conditions its encoder to recognize behavioral similarity in observations by applying a novel SSL objective to pairs of trajectories from the agent's policies
RA, To this end, we propose a novel meta-algorithm Self-Imitation Policy Learning through Iterative Distillation (SPLID)  which relies on the concept of \delta-distilled policy to iteratively level up the quality of the target data and agent mimics from the relabeled target data
RA, We discover that, surprisingly, models with minimal finetuning efforts --- only on input, output, and optionally batch normalization layers, can achieve competitive performance on 3D point-cloud classification, beating a wide range of point-cloud models that adopt task-specific architectures and use a variety of tricks
RA, We propose a method, Head-to-Toe probing (Head2Toe), that selects features from all layers of the source model to train a classification head for the target-domain
RA,In this way, the model learns to condition on easy-to-detect objects (e,g,, “table”) and language hints (e,g,  “on the table”) to detect harder objects (e,g,, “mugs”)mentioned in the utterance
RA, Specifically, through simple but effective goal generation, our method learns to continuously propose challenging -- yet temporal and achievable -- goals that allow the agent to learn general skills for acting in a new environment, independent of the task to be solved
RA, Motivated by this, we propose the "Look-back Gradient Multiplier" (LBGM) algorithm, which utilizes this low-rank property of the gradient-space in federated learning
RA, To incorporate a rare word definition as a part of input, we fetch its definition from the dictionary and append it to the end of the input text sequence
RA, In this paper, we propose a novel extension to the successor feature framework resulting in a natural second-order variant
RA, This allows LSeg to generalize to previously unseen categories at test time, without retraining or even requiring a single additional training sample
RA, We derive a lower bound for training, and propose a rejection sampling scheme to account for the invalid samples during generation
RA, Specifically, motion in generated video is constructed by linear displacement of codes in the latent space
RA, To do so, we train a variational autoencoder (VAE) to encode molecular fragments in a coherent latent space, which we then utilize as a vocabulary for editing molecules to explore the complex chemical property space
RA, We analyze our algorithms under several noise models, showing that the algorithms perform well even when the oracle errs
RA, Based on our new understanding on the DML models, we propose Adversarial Deep Metric Learning model with adversarial samples generated by Alignment or Uniformity objective (ADML+A or U)
RA, Similarly, we show how training with a data distribution aids with performance on that data distribution
RA, We show the locality-sensitivity of SignRFF, and propose a new measure, called ranking efficiency, to theoretically compare different Locality-Sensitive Hashing (LSH) methods with practical implications
RA, We propose a set of metrics which utilize benchmark data sets to evaluate each stage of the search process independently
RA, Our method significantly reduces the required number of interactions compared with random intervention targeting and is applicable for both discrete and continuous optimization formulations of learning the underlying directed acyclic graph (DAG) from data
RA, In this work, we adopt the lens of effective sparsity to reevaluate several recent pruning algorithms on common benchmark architectures (e,g,, LeNet-300-100, VGG-19, ResNet-18) and discover that their absolute and relative performance changes dramatically in this new, and as we argue, more appropriate framework
RA, Our algorithm also applies to the case when c_{\min = 0, where a \tilde O(K^{2/3) regret is guaranteed
RA, Using our algorithms, machine learning models can under some assumptions be calibrated without sacrificing accuracy: in a sense, calibration can be a free lunch
RA, Our model, namely FALCON, represents individual visual concepts, such as colors and shapes, as axis-aligned boxes in a high-dimensional space (the box embedding space)
RA, In this paper, we propose a novel distance-based BCR method suitable for OSR, which limits the feature space of known-class data in a class-wise manner and then makes background-class samples located far away from the limited feature space
RA, Our method is based on a differentiation of the forward and backward paths: the weights in the forward path are a thresholded version of the weights maintained in the backward path
RA, Then, we provide bound relationships and an information-theoretic analysis, which uncover hidden label-marginal biases: Dice has an intrinsic bias towards specific extremely imbalanced solutions, whereas CE implicitly encourages the ground-truth region proportions
RA, We  guide the latent representation of an autoencoder by capturing graph structure information with the geometric scattering transform and apply penalties that structure the representation also by molecular properties
RA, We then adversarially perturb G(x) in the VAE's bottleneck space and adds it back to the original R(x) as an augmentation, which is therefore sufficiently challenging for contrastive learning and meanwhile preserves the sample identity intact
RA, Furthermore, we adapt the number of skip connections for a better balance between the two sub-tasks
RA, To address this issue, we devise an Open-ended Supervised Learning (OSL) framework, of which the key component is a subjective function that allocates the data among multiple candidate models to resolve the "conflict'' between the data from different domains, exhibiting a natural hierarchy
RA, We start with a general framework that unifies several relational neural network architectures: graph neural networks, neural logical machines, and transformers
RA, We also show PNODE enables the application of implicit time integration methods that are desired for stiff dynamical systems
RA, To alleviate the over-smoothing problem, we consider hierarchical fusion strategies, which combine the representations from different layers adaptively to make the output more diverse
RA, One of our conceptual contributions is to connect predictive (that is, optimistic) regret minimization with the framework of \Phi-regret
RA, Our proposed method learns and parametrizes the latent space by leveraging the reverse-complement of genomic sequences
RA, This creates a 3-level hierarchy (notes, performance, synthesis) that affords individuals the option to intervene at each level, or utilize trained priors (performance given notes, synthesis given performance) for creative assistance
RA, The two models are jointly optimized according to a minimax adversarial objective: the retriever learns to retrieve negative documents to cheat the ranker, while the ranker learns to rank a collection of candidates including both the ground-truth and the retrieved ones, as well as providing progressive direct feedback to the dual-encoder retriever
RA, By modeling the unconditional distribution for speech, our model can utilize the untranscribed data for training
RA, Hence, we use the stricter assumption that linearity only holds within certain data or label distances for regression where the degree may vary by each example
RA, We then apply the proposed algorithms to academic examples, as well as deep neural networks training, where we empirically test their performances on the SVHN dataset
RA, We propose a Deep Latent Space Model (DLSM) for directed graphs to incorporate the traditional latent space random graph model into deep learning frameworks via a hierarchical variational auto-encoder architecture
RA, We reveal that SA performs two distinct roles: Phonetic and linguistic localization
RA, Our objective has two components: First, a representation must remain discriminative for the task, i,e, one predictor can simultaneously minimize the source and target risk;  Second, the representation's marginal support needs to be the same across source and target
RA, Our approach has significantly fewer constraint violations in comparison to other constrained RL baselines, and achieve the new state-of-the-art results on the L2R benchmark task
RA, In addition, we extend GPCA to the (semi-)supervised setting and show that it is equivalent to GPCA on a graph extended with “ghost” edges between nodes of the same label
RA, We perform indirect learning from hidden traits of observed sequences by assuming that the random vector representing the data is generated from an unobserved low-dimensional latent vector
RA, To be specific, we enforce invariance against each patch and its neighbors, i,e, each patch treats similar neighboring patches as positive samples
RA, In this paper, we show that although existing extraction models are able to memorize and recall already seen triples, they cannot generalize effectively for unseen triples
RA, In this work, we propose a variant of SCO algorithms with sparse moving averages for GNN training
RA, Our framework models rewards and policies as nonparametric functions belonging to subsets of Reproducing Kernel Hilbert Spaces (RKHSs)
RA, Here, we provide a short and simple algorithm which achieves state-of-the-art results by attempting iterative ascent in the direction of the surrogate's loss gradient, and, only when that fails to proceed, sampling alternative search directions from the coimage of the surrogate's local linearisation, i,e, its Jacobian's row space
RA, Our model also explicitly encodes interactions across periodic boundaries and respects permutation, translation, rotation, and periodic invariances
RA, To remedy this issue, we develop a new quantization function that preserves the correct variance in each update step
RA, To make inference tractable, we take inspiration from visual topic models and introduce an interpretable hierarchy of scene-level and object-level latent variables (i,e, slots)
RA, We also introduce a fast kernel-based method for estimating Shapley-Taylor indices that require orders of magnitude fewer function evaluations to converge
RA, Specifically, OPP regularizes the gram matrix of convolutional kernels to encourage kernel orthogonality among the important filters meanwhile driving the unimportant weights towards zero
RA, We also design a new objective to learn the sketch, whereby we optimize the subspace embedding property of the sketch
RA, Consequently, we propose SNN architectures incorporating the preceding constructs that are trained using supervised backpropagation-through-time (BPTT) and unsupervised spiking-timing-dependent plasticity (STDP) algorithms for classification of spatiotemporal data
RA, The goal of our model is to minimize the influence of sensitive feature from the perspectives of both data input and predictive model
RA, Consequently, we propose a novel network training algorithm based on constrained optimization for realistic backdoor injection attack in hardware
RA, Finally, we explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an epsilon-greedy Q-learning agent - without backpropagating through the update rule
RA, We show that standard approaches to episodic RL and existing approaches struggle as interventions are minimized, underscoring the need for developing new algorithms for reinforcement learning with a greater focus on autonomy
RA, To address these issues, StyleNeRF integrates the neural radiance field (NeRF) into a style-based generator to tackle the aforementioned challenges, i,e, improving rendering efficiency and 3D consistency for high-resolution image generation
RA, We evaluate our methods by synthesizing code from natural language descriptions using GPT-3 and Codex in three real-world languages: SQL queries, Vega-Lite visualizations and SMCalFlow programs
RA, In this work, we first define fairness in DML through an analysis of three properties of the representation space -- inter-class alignment, intra-class alignment, and uniformity -- and propose finDML, the fairness in non-balanced DML benchmark to characterize representation fairness
RA, Going beyond label flipping noise, we propose to measure the mismatch between the assigned and (unknown) true label distributions, denoted as implicit label noise
RA, We also explore the relation between distances induced by autoencoders and the probability distribution of the training data, as well as how these induced distances are correlated with human perception
RA, We design an interaction-for-perception framework VAT-Mart to learn such actionable visual representations by simultaneously training a curiosity-driven reinforcement learning policy exploring diverse interaction trajectories and a perception module summarizing and generalizing the explored knowledge for pointwise predictions among diverse shapes
RA, Based on the analysis, we split NAS samples into three categories and further suggest a simple modification to the training objective to obtain an improved OOD detection method that is capable of detecting samples from all NAS categories
RA, We systematically benchmark contemporary methods that leverage unlabeled data, including domain-invariant, self-training, and self-supervised methods, and show that their success on the shifts in U-WILDS is limited
RA, To address these challenges, we propose a novel method, CLIORA, which constructs a shared vision-language constituency tree structure with context-dependent semantics for all possible phrases in different levels of the tree
RA, In this framework, all participants collaboratively optimize the aggregate of local loss functions, and each participant autonomously builds its own model by iteratively fitting the gradients of the objective function
RA, Based on these observations, we describe a simple semantic segmentation method, which can work even if only a few labeled images are provided
RA, Our proposal is to scale the densities p and q by another density m and estimate \log p/q as \log p/m - \log q/m
RA, We learn these representations via a masked-zone prediction task, which segments an agent’s trajectory into zones and then predicts features of randomly masked zones, conditioned on the agent’s camera poses
RA, We propose federation of unsupervised learning (FedUL), where the unlabeled data are transformed into surrogate labeled data for each of the clients, a modified model is trained by supervised FL, and the wanted model is recovered from the modified model
RA, We characterize the dynamics of truncated SGD driven by heavy-tailed noises
RA, This formulation succinctly captures large scale problems, but is also known to be computationally hard in its general form
RA,  In particular, we characterize the properties of environments that allow offline RL methods to perform better than BC methods even when only provided with expert data
RA, We first establish the domain transformation model, mapping a domain-free latent image into a domain
RA, Putting this method to work, we combine  the MNIST dataset with various gridworld environments to rigorously evaluate  generalization of DQN and QR-DQN in state, observation and action spaces for  both online and offline learning
RA, For the single-agent prediction case, our model achieves top results on the global NuScenes vehicle motion prediction leaderboard and produces strong results on the Argoverse vehicle prediction challenge
RA, This further implies that logarithmic regret and incentive cost growth rates are achievable under this new MAB framework
RA, We propose a federation mechanism for the problems with natural similarity metric between the labels which commonly appear in natural language understanding (NLU) tasks
RA, In this work, we ask how we can build a rule-based system that can reason with natural language input but without the manual construction of rules
RA, This guides us to generate contrastive views which could avoid most false positives (i,e, object vs  background)
RA, Utilizing this framework, we analyze many intriguing properties of training robust models with frequency constraints, and propose a frequency-based explanation for the commonly observed accuracy vs  robustness trade-off
RA, These mixtures of keys follow a Gaussian mixture model and allow each attention head to focus on different parts of the input sequence efficiently
RA, Specifically, we propose a metric to balance the trade-off between diversity and purity in the episodic memory with noisy labels
RA, Specifically, we explore a setting involving a low-resolution dynamical sensor that moves with respect to a static scene, with drift-like tiny steps
RA, Based on this finding, we developed a novel method named source fiction for semi-supervised optimal transport-based domain adaptation
RA, Using general Fenchel duality results, we derive variational principles dual to maximum likelihood EBMs with shallow overparametrized neural network energies, both in the active (aka feature-learning) and lazy regimes
RA, In this paper, we introduce the multi-variate backfill problem using COVID-19 as the motivating example
RA, Finally, we draw parallels between overfitting observed in learning with noisy labels and in adversarial training where SAM also improves generalization
RA, We provide an explanation for why data augmentation helps long-tailed classiﬁcation despite leaving the dataset imbalance unchanged — it promotes inter-class separation, intra-class compactness, and improves localization of tail classes w.r.t to the true data distribution
RA, Toward this, we define yet another sharpness measure called Connectivity sharpness, a reparameterization invariant, structurally separable sharpness measure
RA, To circumvent this, we propose a novel method that builds upon the distributional alignment of the variational information bottleneck and encourages assigning lower confidence to samples from the latent prior
RA, Then, in order to converge to the desired output, the step-size is gradually decreased
RA, Our method works with arbitrary off-the-shelf feature extractors and outperforms existing state-of-the-art on miniImagenet, CUB and Stanford Dogs datasets by 3% to 5% on 5way-1shot and 5way-5shot tasks
RA, The measures we propose characterize changes in extrapolation behavior when feature coverage is manipulated in a combinatorial setting
RA, In particular, we propose to use a selective token generation between the transformer-based PLM and the task-specific adapter during both training and inference
RA, Our method solely requires value query oracle access to the linear functional
RA, We find that these standard RNNs are very unreliable during zero-shot transfer to motif chaining
RA, We thus propose to address the problem of adversarial training (AT) within the framework of distributional robustness optimization (DRO)
RA, We suggest and prove that by optimizing smooth functions with respect to the dual of the correlation maximization problem, the optimum is convex almost surely and hence construct a Brenier map as our generative quantile network
RA,  Based on this empirical analysis, we propose a new method which mitigates this issue by shielding the learned representations from drastic adaptation to accommodate new classes
RA, We further introduce a soft pruning technique to package the pruned tokens, which integrate the less informative tokens generated by the selector module into a package token, and participates in subsequent calculations rather than being discarded completely
RA, To mitigate this, we propose to regularize the third-person information stream via a variational information bottleneck
RA, We further propose a variance reduced domain randomization (VRDR) approach for policy gradient methods, to strike a tradeoff between the variance reduction and computational complexity in practice
RA, In contrast, we propose a neural network called MRMD-AE (manifold-regularized multiple-decoder, autoencoder), that learns a common embedding from multiple subjects in an experiment while retaining the ability to decode to individual raw fMRI signals
RA, We propose to combine the two paradigms: offline datasets and synthetic simulators, to reduce the sim2real gap by using limited offline data to train realistic simulators
RA, In this paper, we introduce a new RL formulation for text generation from the soft Q-learning (SQL) perspective
RA, Our proposed model combines several unique advantages: (i) It is trained from a single video, and thus overcomes the need for large training datasets
RA, In this paper, we first demonstrate that the ability of a classifier to make the 'none-of-above' decision is highly correlated with its accuracy on the closed-set classes
RA, We show that select discriminators (e,g, discriminator trained for Jensen-Shannon divergence) are able to map support differences as support differences in their one-dimensional output space
RA, To mitigate the annotation burden and relax the constraints on the statistical complexity of the data, our method leverages interactions to effectively sample diverse variations of an object and the corresponding training signals while learning the object-centric representations
RA, We first analyze and summarize the properties of instance-level mixup as local smoothness and global discrimination
RA, In this paper, we tackle this question from both an optimization and an empirical point of view
RA, In this way, we analyze what simplified inputs tell us about the decisions made by classification networks
RA, Our general framework allows us to build E(3) and SE(3)-steerable CNNs like previous works, but also CNNs with arbitrary G\leq O(3)-steerable kernels
RA, This allows us to train non-equivariant networks to encode input data, for which the underlying symmetry may be non-obvious, into a latent space where symmetries may be used to reason about outcomes of actions in a data-efficient manner
RA, We advocate a nonlinear kernelized classification layer for deep networks to tackle this problem
RA, This is achieved by generating separate forward predictions for the mean and aleatoric uncertainty of future states with reducing intrinsic rewards for those states that are unpredictable
RA, These steps involve two challenging ingredients: estimation requires the ability to anticipate how hypothetical policies would influence user preferences if deployed -- we do this by training a user predictive model that implicitly contains their preference dynamics from historical user interaction data; evaluation and optimization additionally require metrics to assess whether such influences are manipulative or otherwise unwanted -- we introduce the notion of “safe shifts”, that define a trust region within which behavior is believed to be safe
RA, We first show that for 1-dimensional strongly convex functions, with smooth second derivatives, there exist optimal permutations that offer exponentially faster convergence compared to random
RA, We model the guide retriever after the posterior distribution Q of passages given the input and the target output and train it jointly with the standard retriever and the generator by maximizing the evidence lower bound (ELBo) in expectation over Q
RA, Our key idea is to regularize the training of Q-learning so that OOD states will have higher OOD uncertainty, while in-distribution states will have lower OOD uncertainty; therefore making them distinguishable
RA,  Using several video datasets, we show that VPR is able to detect event boundaries, disentangle spatiotemporal features across its hierarchy, adapt to the dynamics of the data, and produce accurate time-agnostic rollouts of the future
RA, For solving these new problems, we propose algorithms with theoretical guarantees, evaluate them on several real-world datasets, and show that they give comparable performance to state-of-the-art offline algorithms that store the entire data in memory and take multiple passes over it
RA,  In our framework, a detector is evaluated with samples from its adversarial distribution, which generates diverse outlier samples that are likely to be misclassified as in-distribution by the detector
RA, Our approach is based on the Three Compartment Controller model, in which the relationships of variables such as maximum voluntary joint torques, recovery, and cumulative fatigue are present
RA, We extend this approach to constructing transition models for common Mujoco environments, showing that our model can appropriately balance inductive biases with the flexibility required for model-based control
RA, We see Flashlight as a tool enabling research that can benefit widely-used libraries downstream and bring machine learning and systems researchers closer together
RA, We show that the hypersurfaces can be optimized by gradient ascent efficiently
RA, In this paper, we first propose the streaming-snapshot model to describe temporal graphs on different time scales
RA, Instead, we propose to impede model extraction by requiring users to complete a proof-of-work before they can read the model's predictions
RA, To mitigate the issue, we propose a sparsification-based OOD detection framework termed DICE
RA, Specifically, we show that for very small target models, our method can generate significantly better performing models than traditional few-shot learning methods
RA, Therefore, we aim to design a new type of inducing noise, named ADVIN, which is an irremovable poisoning of training data
RA, To compare two models, we take a reference dataset, and locally approximate the models on each reference point with linear models trained by LIME
RA,  This paper proposes the wild-card CTC (W-CTC) to address this issue, by padding wild-cards at both ends of the labels
RA, In this setting, the model is provided with an exhaustive list of phrases describing all the possible values of a specific attribute,  together with a shared image-language embedding (e,g, CLIP)
RA, We introduce two \xi-learning variations, prove its convergence, and provide a guarantee on its transfer performance
RA,  To facilitate the robustness of dynamics, our OOTD model identifies the objects influenced by input actions and predicts the belief of object states with independently parameterized transition layers
RA, The resulting learning architecture is called Covariant Attention-based Mechanism or CAM, which comprises: 1) an encoder: CCN-based embedding model to represent the task space as learnable feature vectors, 2) a decoder: an attention-based model to facilitate sequential decision outputs, and 3) context: to represent the state of the mission and the robots
RA, Going beyond this result, this paper finds that test performance can be substantially improved by selecting a function with much larger margin than is typical under the NNGP posterior
RA, The proposed Sample and Computation Redistribution for Face Detection (SCRFD) is implemented by a random search in a meticulously designed search space
RA, We adopt a sparse-constraint training with l_1\,\,norm relaxation to learn coefficients and indices in DictFormer
RA, We propose two particular types of robustness certiﬁcation criteria: robustness of per-state actions and lower bound of cumulative rewards
RA, To reduce the communication and memory overhead, each local machine in LLCG first trains a GNN on its local data by ignoring the dependency between nodes among different machines, then sends the locally trained model to the server for periodic model averaging
RA, We implement the framework by volume-preserving Flow-based models, and verify our theory by experiments on artificial data and synthesized images
RA, To address this, we introduce a variant of NCE called eNCE which uses an exponential loss and for which normalized gradient descent addresses the landscape issues provably when the target and noise distributions are in a given exponential family
RA, Our insight: to determine which parts of the scene are important and should be modeled, we can exploit task information, such as rewards or demonstrations, from previous tasks
RA, We propose Cronus, a robust collaborative learning framework that supports heterogeneous model architectures
RA, These methods, which we collectively refer to as reinforcement learning via supervised learning (RvS), involve a number of design decisions, such as policy architectures and how the conditioning variable is constructed
RA, The approach is inspired by neuromodulation that is performed by biological neural networks
RA, Here, we formalize this similarity in the framework of Fisher Geometric model and extreme value theory and present a set of tricks used by naturally evolving organisms to accelerate their adaptation, applicable to model fine-tuning
RA, We propose an effective regex synthesis framework called `SplitRegex' that synthesizes subregexes from `split' positive substrings and produces the final regex by concatenating the synthesized subregexes
RA, To address this gap, we investigate whether three classes of post hoc explanations–feature attribution, concept activation, and training point ranking–can alert a practitioner to a model’s reliance on unknown spurious signals
RA, Specifically, we show that nonlinear transforms built on Swin-transformers can achieve better compression efficiency than transforms built on convolutional neural networks (ConvNets), while requiring fewer parameters and shorter decoding time
RA, First, we find that MAML needs a large number of gradient steps in its inner loop update, which contradicts its common usage in few-shot classification
RA, Specifically, we introduce a novel concept-feature dictionary to allow flexible image feature retrieval at training time with concept keys
RA, We show that instruction tuning—finetuning language models on a collection of tasks described via instructions—substantially boosts zero-shot performance on unseen tasks
RA, First, it formalizes the motivation behind a broad line of recent successful NLM training heuristics, proposed for the pretraining and fine-tuning stages, which do not necessarily appear related at first glance
RA, Our motivating application is the analysis of single-cell multimodal data to identify genomic loci that mediate the regulation of specific genes
RA, We find that ViTs are surprisingly insensitive to patch-based transformations, even when the transformation largely destroys the original semantics and makes the image unrecognizable by humans
RA, We apply this method to control a simulated agent performing a locomotion task and show that the NCAP architecture achieves comparable asymptotic performance with fully connected MLP architectures while dramatically improving data efficiency and requiring far fewer parameters
RA, We propose to trade coverage for precision by enforcing that the presence of incorrect candidates in the predicted conformal sets (i,e, the total number of false discoveries) is bounded according to a user-specified tolerance
RA, This eliminates any guarantees relating Bellman error to the accuracy of the value function
RA, Toward addressing these shortcomings, we propose an inference-time improvement algorithm for parametric sequential generative modeling methods called belief fine-tuning (BFT)
RA, The proposed techniques significantly improve the convergence and the performance of all sub-networks
RA, We design multi-granularity pooling and pooling fusion to capture different levels of contextual information and combine their interactions with tokens
RA, Moreover, to address the data heterogeneity at the edge devices in this framework, we have innovated a series of algorithms that broaden existing supervised personalization algorithms into the setting of self-supervised learning including perFedAvg, Ditto, and local fine-tuning, among others
RA, This memory works as a proxy to the current task, and we condition a policy head on it
RA, Our objective directly estimates the epistemic uncertainty that comes from the discriminator not having seen enough training examples, thus providing an intrinsic reward more tailored to the true objective compared to pseudocount-based methods (Burda et al, 2019)
RA, Leveraging its duality, we frame WAFL as an empirical surrogate risk minimization problem, and solve it using a novel local SGD-based algorithm with convergence guarantees
RA, We show that DISSECT produces CTs that (1) disentangle several concepts, (2) are influential to a classifier's decision and are coupled to its reasoning due to joint training (3), are realistic, (4) preserve relevant information, and (5) are stable across similar inputs
RA, To address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any layer in the model
RA, We derive AAVAE starting from the conventional variational autoencoder (VAE), by replacing the KL divergence regularization, which is agnostic to the input domain, with data augmentations that explicitly encourage the internal representations to encode domain-specific invariances and equivariances
RA, We propose a simple sparse network training strategy, Random Sparse Net-work Transformation (RST), to substantiate our DLTH
RA, We introduce three new algorithms with stronger sample complexity bounds than existing ones
RA, Our new theory hinges on the insight that different samples from the same class could be bridged together with aggressive data augmentations, thus simply aligning the positive samples (augmented views of the same sample) could make contrastive learning cluster intra-class samples together
RA, We evaluate RIRL in Principal-Agent (specifically manager-employee relations) problem settings of varying complexity where RI models information asymmetry (e,g, it may be costly for the manager to observe certain information about the employees)
RA, We then enforce the new policy to stay closer to the virtual policy, which is beneficial in case the old policy performs badly
RA, We use a reinforcement learning strategy to formulate the HG problem as a guided node-pair embedding-based link prediction problem via a directed graph walk
RA, In comparison to prior works, our approach is able to translate human videos into practical robot demonstrations and train the meta-policy with adaptive loss based on the quality of the translated data
RA,  To achieve the orthogonality, we explicitly construct the convolution kernel for enforcing all singular values of the convolution layer's Jacobian to be 1s
RA, Specifically, we extend popular SSL methods to a more general fremework which we name Equivariant Self-Supervised Learning (E-SSL)
RA, We provide specific requirements for convergence with common compression techniques, such as quantization and top-k sparsification
RA, By analyzing properties of the dominant PCs, we find that the block structure arises from dominant datapoints — a small group of examples that share similar image statistics (e,g, background color)
RA, Our learning algorithm develops a mixed mode approach, mixing forward and reverse mode automatic differentiation with forward and adjoint based sensitivity analyses to efficiently perform gradient based optimization
RA, More specifically, we propose a reinforcement learning-based method to dynamically adjust replay hyperparameters online to balance the stability and plasticity trade-off in continual learning
RA, We show how this relation extends to operations such as convolutions, element-wise nonlinearities, and global and local pooling
RA, NVDPs model the conditional posterior distribution based on a task-specific dropout; a low-rank product of Bernoulli experts meta-model is utilized for a memory-efficient mapping of dropout rates from a few observed contexts
RA, To leverage pre-training, we first encode observations, goals, and history information as templated English strings, and train the policy to predict the next action
RA, Specifically, we achieve customization by learning a set of base sub-networks of different sizes and robustness levels, which are later aggregated on-demand according to inference requirements
RA, First, we propose a new training method that jointly maximizes robust accuracy and minimizes robust inaccuracy
RA, The proposed method may apply to other goal-predefined combinatorial problems, though it has a few constraints
RA, Specifically, we re-frame them as modifications to specific hidden states in pretrained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification
RA, We illustrate the practicality of our approach by inverting Vision Transformers (ViTs) and Multi-Layer Perceptrons (MLPs) trained on the ImageNet dataset, tasks which to the best of our knowledge have not been successfully accomplished by any previous works
RA, Namely, we show that if local pseudorandom generators (PRGs) exist, then for a large family of natural target distributions, there are ReLU network generators of constant depth and poly size which take Gaussian random seeds so that (i) the output is far in Wasserstein distance from the target distribution, but (ii) no polynomially large Lipschitz discriminator ReLU network can detect this
RA, By comparing the magnitude of bias and variance, we explain the success of the Emphasizing Recent Experience sampling and 1/age weighted sampling
RA, In this work, we propose a post-processing method designed to mitigate bias of state-of-the-art models
RA, We train our agent through reinforcement learning on MNIST and Omniglot  datasets for unconditional generation and parsing (reconstruction) tasks
RA, Moreover, extending from the Bellman operator we propose Shapley-Q operator that is proved to converge to the optimal generalised Shapley value
RA, We identify that some samples with injecting small Gaussian noise can fool different target models, and their adversarial examples under different source models have much stronger transferability
RA, The proposed method protects the confidentiality of client gradient updates against both semi-honest clients and servers
RA, In this work, we replace each crafted factor with a differentiable neural network enabling the factors to be learned using an efficient optimization routine from labeled data
RA, Here, we introduce iLQR-VAE, a novel control-based approach to variational inference in nonlinear dynamical systems, capable of learning both latent dynamics, initial conditions, and ongoing external inputs
RA, Specifically, we apply Mirror Descent with the loss generated by the public data as the *mirror map*, and using DP gradients of the loss generated by the private (sensitive) data
RA, Our framework assumes a hierarchical latent structure of a document where the top-level captures the long range dependency at a coarser time scale and the bottom token level preserves the details
RA, In this work, we propose to use document similarity methods to create noisy parallel datasets of code, thus enabling supervised techniques to be applied for automated code translation without having to rely on the availability or expensive curation of parallel code datasets
RA, We analyze the convergence of HL-SGD in the presence of heterogeneous data for general nonconvex settings
RA, We propose a practical implementation of successor features in continuous action spaces
RA, Instead, we propose approximating the multi-modal posterior of a BNN to prevent mode collapse and encourage diversity over learned posterior distributions of models to develop a novel adversarial training method for BNNs
RA, We then use the improved learning bounds to establish \mathcal{O\left(1/n \right) high probability generalization bounds for classical empirical saddle point (ESP) solution and several popular gradient-based optimization algorithms, including gradient descent ascent (GDA), stochastic gradient descent ascent (SGDA), proximal point method (PPM), extra-gradient (EG), and optimistic gradient descent ascent (OGDA)
RA, To address this problem, we develop a probability-based pruning algorithm, called batch whitening channel pruning (BWCP), which can stochastically discard unimportant channels by modeling the probability of a channel being activated
RA, This work contributes by analyzing the processing of the information in neural architectures with parallel pathways
RA, We present a simple baseline approach that can learn meaningful representations with no metric-based learning, no data augmentations, no world-model learning, and no contrastive learning
RA, We begin by meticulously scrutinizing these contradictory findings under a unified empirical consistency
RA, Specifically, we propose the context motion network to leverage semantic information and motion information
RA, In this work, we bridge this gap and extend the multi-hop reasoning problem to hyper-relational KGs allowing to tackle this new type of complex queries
RA, We then develop a general framework from the perspective of Bregman divergence minimization, where each strictly convex multivariate function induces a proper loss for multi-distribution DRE
RA, Our model also performs well when generalizing to unseen images downloaded directly from the internet (Fig.1)
RA, On the physical heat diffusion, we further apply ICGNN to solve a design optimization problem, which seeks to find the optimal heater allocations while considering the optimal operation of the heaters, by using a gradient-based method
RA, Specifically, in our setting we show that SGD progressively learns more complex functions and that there is a "deep bootstrap" phenomenon: during the second stage, the test error of both worlds remain close despite the empirical training error being much smaller
RA, Our framework extends partially-observed Markov decision processes (POMDPs) by allowing an agent to interact with an assistant to leverage their knowledge in accomplishing tasks
RA, To cope with this challenge, we propose Adaptive Fourier Neural Operator (AFNO) as an efficient token mixer that learns to mix in the Fourier domain
RA, We evaluate the approach on four complex, single and dual arm, robotics manipulation tasks against strong suitable baselines
RA, We hypothesize that this type of model can produce a more intuitive and disentangled representation in the latent space by promoting smoother transitions between cluster points over time
RA, We employ our method to train a graph-based multi-agent actor-critic architecture that learns a centralized policy, conditioned on a learned latent interaction graph
RA, To address this issue, we propose a novel compositional training framework for end-to-end DAM, namely compositional DAM
RA, We next introduce a test-time training (TTT) approach for semantic segmentation, Seg-TTT, which adapts the model parameters to the test sample using a self-supervised loss
RA, We propose a simple approach that can be used in any test setting where the model is probabilistic and adaptable: when presented with a test example, perform different data augmentations on the data point, and then adapt (all of) the model parameters by minimizing the entropy of the model's average, or marginal, output distribution across the augmentations
RA, We propose an efficient method to learn surface parameterization by learning a continuous bijective mapping between 3D surface positions and 2D texture-space coordinates
RA, In this problem, a set of N agents work cooperatively to maximize the global average reward through interacting with their neighbors over a communication network
RA, Based on the above observations, we propose Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), a novel improvement over SAM with negligible computation overhead
RA, Based on the insight, we further point out two limitations of the prioritized ER method: 1) outdated priorities and 2) insufficient coverage of the sample space
RA, To elucidate the cause of this personalization performance degradation problem, we decompose the entire network into the body (i,e, extractor), which is related to universality, and the head (i,e, classifier), which is related to personalization
RA, In particular, we compare the performance of RNNs using target-based learning rules versus those using representational learning rules on three different curricula in the context of two tasks
RA, Inspired by recent advances in transformer-based models, we train agents with an instruction prediction loss that encourages learning temporally extended representations that operate at a high level of abstraction
RA, To fundamentally resolve the data-fitting problem, we propose a physical neural network (PNN) utilizing “Range, Inertia, Symmetry, and Extrapolation” (RISE) constraints
RA, Our approach uses a new two-player framework for safe RL called DESTA
RA, Inspired by the Common Fate Principle of Gestalt Psychology, we first extract (noisy) masks of moving objects via unsupervised motion segmentation
RA, Contrary to the MLP-mixer, we incorporate structure by retaining the relative positions of the image patches
RA, We also provide a simple solution to the module collapse problem and display superior accuracy performance over several MTL benchmarks compared to the original routing network
RA, Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student)
RA, To quantify how different the original dataset is with respect to the transformed one, we compute the dataset distances via Optimal Transport
RA, This observation motivates us to design STEGO (Self-supervised Transformer with Energy-based Graph Optimization), a novel framework that distills unsupervised features into high-quality discrete semantic labels
RA, Specifically, our policy operates at three levels of hierarchy
RA, We introduce Cycle-Consistency World Model (CCWM), a concrete instantiation of learning via retracing implemented under existing model-based reinforcement learning framework
RA, Then, we progressively compose sub-items and propose a novel data-free optimization objective in the discrete domain,  minimizing Constrained Absolute Sum of Error (or CASE in short), which surprisingly does not need any dataset and is even not aware of network architecture
RA, This meta-algorithm generalizes existing conformal prediction algorithms, and we show that it achieves approximate valid population coverage and near-optimal efficiency within class, whenever the function class in the conformalization step is low-capacity in a certain sense
RA, To tackle this challenging problem, we propose ORCA, an end-to-end approach that assigns instances to previously seen classes or  forms novel classes by grouping similar instances without assuming any prior knowledge
RA, Our findings yield the first comprehensive analysis of both existing and novel methods for practitioners faced with multi-domain active learning for natural language tasks
RA, Specifically, We apply Gaussian Mixture Model (GMM) as the parametric approximate posterior distribution to the real feature distribution and introduce the categorical latent variable
RA, We test AFML in an extensive set of these non-IID data scenarios, with both CIFAR-100 and Shakespeare datasets
RA, To overcome these problems, we propose a convergent DQN algorithm (C-DQN) that is guaranteed to converge and can work with large discount factors (0.9998).
RA, We further develop a theory to back up the improvement in optimization and generalization and provide convergence guarantees under both convex and nonconvex settings
RA, Building on existing literature, we interpret DEQs as fine-tuned, unrolled classical algorithms, giving an intuitive justification for why DEQ models are sensible
RA, We also perform an analysis of how and where locality features contribute to improving performance and why the traditionally used contextual similarity metrics alone are not enough to grasp the locality structure
RA, We analyze stability with respect to the normalized version of the loss function used for training
RA, By evaluating on various synthetic and real data sets, we demonstrate that ASL is a stable, efficient, and accurate training framework for deep generative models
RA, We also give a fast version of our algorithm that only queries the evaluation of energy function twice for each proposal via linearization of the energy function
RA, With this observation, we investigate the Hessian through simple and much lower-dimensional matrices
RA, In particular, we use finite difference to approximate the forward modeling of PDE as a differentiable operator (from velocity map to seismic data) and model its inversion by CNN (from seismic data to velocity map)
RA, We construct an Abelian group network using invertible neural networks and show its universal approximation property
RA, Structurally, we introduce an encoder network and novel unsupervised conditional contrastive loss to ensure that data generated from a single mixture component represent a single attribute
RA, Inspired by SimSiam's alternating optimization hypothesis, we propose a novel optimization target, SimMER, for self-supervised learning that explicitly avoids model collapse by balancing consistency (total variance minimization) and entropy of inputs' representations (entropy maximization)
RA, In this paper, we propose a new continual learning framework based on low-rank factorization
RA, Our approach paves the way for a new class of data-efficient representation learning
RA, Specifically, we figure out how to solve differential equations on manifolds and show that DDIMs are simple cases of pseudo numerical methods
RA, This flexibility allows users to control the precise performance-memory cost trade-off without making any changes in the backbone architecture
RA, In this work, we propose a method that directly outputs the coordinates of atoms, so that there is no violation of constraints
RA, In this work, we extend language models with the ability to memorize the internal representations of past inputs
RA, This is achieved by replacing the entropy by a non-saturating surrogate and adding a diversity regularizer based on batch-wise entropy maximization that prevents convergence to trivial collapsed solutions
RA, We construct TAda2D networks by replacing the spatial convolutions in ResNet with TAdaConv, which leads to on par or better performance compared to state-of-the-art approaches on multiple video action recognition and localization benchmarks
RA, By modeling the control as a neural network, we establish a sampling algorithm that can be trained end-to-end
RA, We show how our TP can be used to train recurrent neural networks with long sequences on various sequence modeling problems
RA, Specifically, we learn a conditional policy that, in an episode, first applies a sequence of transform actions to modify an agent's skeletal structure and joint attributes, and then applies control actions under the new design
RA, We also propose various variants of OSSuM that can be used to prune deeper neural networks
RA, This formulation encompasses much of prior work on backdoor removal
RA, We build a family of models which surpass existing MLPs and even state-of-the-art Transformer-based models, e,g, Swin Transformer, while using fewer parameters and FLOPs
RA, In this setting, many methods directly attack surrogate models and transfer the obtained adversarial examples to fool the target model
RA,  We make the first step toward integrating the two rising research directions, that is, time-aware deep learning and multipersistence, and propose a new model, Time-Aware Multipersistence Spatio-Supra Graph Convolutional Network (TAMP-S2GCNets)
RA, In other words, this indicates how robust the representation obtained from that extractor is with respect to a given adversary
RA,  Our method, DIVA (Differentiable Validation) hinges on a closed-form differentiable expression of the leave-one-out cross-validation error around a pre-trained DNN
RA, Therefore, we propose a rotation-invariant keypoint detection method using rotation-equivariant CNNs
RA, We generalize this characterization to settings where we only know some hypothesis class over possible mechanisms, as well as settings where the mechanisms are stochastic
RA, To achieve this purpose, we train an agent to maximize the expected quadratic utility function, a common objective of risk management in finance and economics
RA, To allow for an in-depth analysis, we focus on a specifically popular setup with high variance -- continuous control from pixels with an actor-critic agent
RA, We hypothesize that an agent can generalize within a task structure if it can discover representations that capture these reoccurring task-segments
RA, We derive an upper bound for this objective which only involves correlations between output vectors and nonlinear functions of input vectors
RA, Second, we propose a novel prototypical contrastive loss, ProtoCL, which can encourage prototypical alignment between two augmented views and prototypical uniformity, hence maximizing the inter-cluster distance
RA, We further experimented with random splits of the attribute space and found that the predictability of attributes provides an informative estimate of a model's ability to generalize
RA, We exemplify this approach by creating surrogate NAS benchmarks on the existing tabular NAS-Bench-101 and on two widely used NAS search spaces with up to 10^{21 architectures (10^{13 times larger than any previous tabular NAS benchmark)
RA, In this paper, we introduce GPT-Critic, an offline RL method for task-oriented dialogue
RA, We compare the BPNN to conventional deep neural networks (NNs) on several phase retrieval problems, comprising both synthetic and contemporary real-world problems (e,g,, metamaterials for which data collection requires substantial expertise and is time consuming)
RA, We create a new dataset based on the AI2Thor and PartNet datasets and perform extensive experiments that prove the effectiveness of our proposed method
RA, We employ the distributionally robust learning framework to solve the learning-to-abstain problems in each layer
RA, Our approach transforms the predicted probability values using a power-based probability transformation and then recomputes the gradients in the backward pass
RA, We evaluate the cell2state method using barcoded stem cell dataset (Biddy et al, (2018)) and simulation studies, compared with baseline approaches using features that are not dynamic-aware
RA, We first train a receiver model through interactions between the sender and the receiver
RA, We evaluate ABCD on image translation and retrieval tasks, and obtain state-of-the-art results
RA, Our ORViT block consists of two object-level streams: appearance and dynamics
RA, Thanks to its efficiency, we apply our method to more realistic and larger datasets with sophisticated neural architectures and achieve a significant performance boost while using larger synthetic training set
RA, Our proposed framework, dubbed Dually Sparsity-Embedded Efficient Tuning (DSEE), aims to achieve two key objectives:  (i) parameter efficient fine-tuning -  by enforcing  sparsity-aware weight updates on top of the pre-trained weights; and (ii) resource-efficient inference - by encouraging a sparse weight structure towards the final fine-tuned model
RA, Specifically, it is formed by three branches of transformers used to exploit the visual-linguistic interactions of different granularities in spatio-temporal domain between videos and text, detected objects and text, and actions and text
RA, Our method is a generative model based on normalizing flows
RA, We propose a method whereby two complementary uncertainty estimation methods account for both the Q-value and the environment stochasticity to better mitigate the negative impacts of noisy supervision
RA, Towards that end, we design a novel siamese graph neural network called NeuroSED, which learns an embedding space with a rich structure reminiscent of SED
RA, These improvements come at no additional computational cost during fine-tuning or inference
RA, We leverage this assumption to formulate an efficient numerical model that explicitly includes printing imperfections
RA, We present an algorithm whose performance improves along with the accuracy of the predictor, even though na\"{ively following the accurate predictor can still lead to a high clustering cost
RA, This limits the applicable methods to simpler ES, which trade off faster updates for lowered sample efficiency
RA, Our particular scenario features a heterogeneous power grid including dozens of renewable energy plants as well as traditional ones; the corresponding state space is in the order of thousands of variables of different types and ranges
RA, Based on this, we further design a subgraph level interpretation algorithm to reveal complex interactions between graph nodes that are overlooked by previous methods
RA, To capture such heterogeneity, we propose a hierarchical multimodal VAE (HMVAE) that represents modality-specific variations using latent variables dependent on a shared top-level variable
RA, We use gradients of trained neural networks as a measurement of this reflection
RA, We then develop a unified interpretation of overparameterized linear value estimation as minimizing the Euclidean norm of the weights subject to alternative constraints
RA,  We formulate the value function estimation procedure in value iteration as an approximate maximum inner product search problem and propose a locality sensitive hashing (LSH) type data structure to solve this problem with sublinear time complexity
RA, We present a simple new improvement operator that converges in fewer iterations than diffusion methods, while qualitatively producing better samples on natural language datasets
RA, We formulate the problem of good/bad data selection as utility optimization
RA, We design a training setup with several shortcut cues, named WCST-ML, where each cue is equally conducive to the visual recognition problem at hand
RA, We build on the work of Teshima et al, 2020 on bijective flows and study injective architectures proposed in Brehmer et al, 2020 and Kothari et al, 2021
RA, The result is a white-box model that learns transformation-based topological similarity akin to how a human would naturally and unconsciously ``distort'' an object when first seeing it
RA, To solve expensive multi-objective problems in a data-efficient manner, we implement popular multi-objective Bayesian optimization (MOBO) algorithms with state-of-the-art performance in a modular framework
RA, We design models to extract features of a program, represent the program as an embedding, and use this embedding to solve various analysis tasks
RA, In this work, we propose a gradient matching score (GM) that leverages gradient information at the shared weight for making informed splitting decisions
RA, Specifically, we enforce consistency between different scales across the student and teacher networks
RA, We base CCE on two prior ideas: counterfactual explanations and concept activation vectors, and validate our approach on well-known pretrained models, showing that it explains the models' mistakes meaningfully
RA, We provide analytical guarantees for the stability of COMLN, we show empirically its efficiency in terms of runtime and memory usage, and we illustrate its effectiveness on a range of few-shot image classification problems
RA, We also release a simple and efficient implementation of recurrent model-free RL for future work to use as a baseline for POMDPs
RA, By applying PA-AD to adversarial training, we achieve state-of-the-art empirical robustness in multiple tasks under strong adversaries
RA, We present several variants of SNVI and demonstrate that they are vastly more computationally efficient than previous algorithms, without loss of accuracy on benchmark tasks
RA, Finally, we show how to learn counterfactually-invariant representations with asymmetry learning in two physics tasks
RA, Specifically, we test a series of pair-wise cell imaging datasets using a new metric to compare existing models
RA,  We evaluate data generation quality by similarity and predictability against four multivariate datasets
RA, We present a unified model architecture for fast and simultaneous agent future heatmap estimation leveraging hierarchical and sparse image generation
RA, Based on the frequency principle on GNNs, we present a novel powerful GNNs framework, Multi-Scale Frequency Enhanced Graph Neural Networks (MSF-GNNs) which considers multi-scale representations from wavelet decomposition
RA, In this work, we propose multi-label iterated learning (MILe) to incorporate the inductive biases of multi-label learning from single labels using the framework of iterated learning
RA, Particularly, we find that prediction alignment and proper data augmentation (in terms of spatial transformations) are two criteria to achieve a generalizable RED approach
RA, Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e,g,, attention, MLP)
RA, We perform end-to-end learning on labeled observation trajectories to learn both the internal transition and observation models
RA, In this paper, we propose a new method Mutual Centralized Learning (MCL) to fully affiliate the two disjoint sets of dense features in a bidirectional paradigm
RA, We also find that the use of knowledge distillation, in conjunction with group-specific models, can help scale existing fair learning methods to hundreds of protected intersectional groups and reduce bias
RA,  The proposed algorithm, which we call the Learning Online with Guidance Offline (LOGO) algorithm, merges a policy improvement step with an additional policy guidance step by using the offline demonstration data
RA, In order to leverage unlabeled samples for reward learning, we infer pseudo-labels of the unlabeled samples based on the confidence of the preference predictor
RA, In this paper, we propose a calibration-aware sample-dependent Focal loss called AdaFocal that adaptively modifies \gamma from one training step to the next based on the information about the network's current calibration behaviour
RA, We then propose two straightforward yet effective techniques to mitigate the undesirable low-pass limitation
RA, We explore a particular form of composition based on neural modules and present a set of RL problems that intuitively admit compositional solutions
RA, Our method is applicable to discrete groups, continuous groups and combinations thereof
RA, Our novel Transformer Control Flow (TCF) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on the simple arithmetic task and a new variant of ListOps testing for computational depth generalization
RA, Specifically, VOS samples virtual outliers from the low-likelihood region of the class-conditional distribution estimated in the feature space
RA, We provide a narrative consistent with our measurement approach that has advantages over problematic prevailing approaches to evaluating and applying language models for common sense
RA, Following this result, an important extending question is whether this compositionality can still be achieved even without conditioning on text
RA, This approach allows us to conduct synthesis planning in a bottom-up manner and design synthesizable molecules by decoding from optimized conditional codes, demonstrating the potential to solve both problems of design and synthesis simultaneously
RA, Furthermore, using these algorithms to estimate the marginals of the weights allows us to make approximate Bayesian predictions that have higher accuracy than point-wise solutions
RA, To encourage the learning policy to consistently converge towards a previously undiscovered local optimum, RSPO switches between extrinsic and intrinsic rewards via a trajectory-based novelty measurement during the optimization process
RA, We model the problem as a Fill In the Blank game by masking some elements in the TS and imputing them with the rest
RA, We also clarify several previous observations in the literature by distinguishing the behavior of the sample-wise and parameter-wise learning curves, finding that sample-wise multiple descent can occur at scales dictated by the eigenstructure of the data covariance, but that parameter-wise multiple descent is limited to double descent, although strong anisotropy can induce additional signatures such as wide plateaus and steep cliffs
RA, To push the discriminator to learn better with challenging replaced tokens, we learn mixture weights over the auxiliary MLMs' outputs to maximize the discriminator loss by backpropagating the gradient from the discriminator via Gumbel-Softmax
RA, In this paper, we propose using a Limited-Memory Symmetric Rank-1 quasi-Newton approach which allows for indefinite Hessian approximations, enabling directions of negative curvature to be exploited
RA, We posit that these past data, which are typically discarded, provide rich contextual information for disambiguating the above-mentioned challenging cases
RA, Our approach rearranges input video frames into super images, which allow for training an image classifier directly to fulfill the task of action recognition, in exactly the same way as image classification
RA, We derive efficient encoding and decoding schemes that both have time complexity \mathcal{O (\log(D) \cdot |p|), where a naive scheme would have linear costs in D and |p|, making the approach highly scalable
RA, We then propose the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems
RA, Turning to the impact of architecture, we show that modularity improves extraction of systematic structure, but only achieves perfect systematicity in the trivial setting where systematic mappings are fully segregated from non-systematic information
RA, To mitigate the issue, we introduce a novel constraint – homophily unnoticeability that enforces GIA to preserve the homophily, and propose Harmonious Adversarial Objective (HAO) to instantiate it
RA, We propose Value Function Spaces: a simple approach that produces such a representation by using the value functions corresponding to each lower-level skill
RA, Building on this result, we propose policy smoothing where the agent adds a Gaussian noise to its observation at each time-step before passing it through the policy function
RA,  To address NPAS, we introduce Shapeshifter Networks (SSNs), which automatically learn where and how to share parameters in a network to support any parameter budget without requiring any changes to the architecture or loss function
RA, We compared an AlexNet model with no normalization or with canonical normalizations (Batch, Group, Layer) to the same models with divisive normalization added (before the canonical normalization, when those were used)
RA, To explore this relationship, we propose a mathematical formulation for assessing the safety of supervised learning models based on their maximum deviation over a certification set
RA, Fourth, we establish that KT applied to a sum of k and k_{\alpha (a procedure we call KT+) simultaneously inherits the improved MMD guarantees of power KT and the tighter individual function guarantees of KT on the target kernel
RA, Our approach showcases non-trivial transfer benefits for two different tasks – language modeling and image captioning
RA, We show that for contrastive methods, this multiview data can naturally be used to identify positive pairs (nearby in latent space) and negative pairs (far apart in latent space)
RA, We perform experiments on density modeling over synthetic discrete data and graph generation to evaluate our proposed method
RA, Specifically, we let the encoder and generator reconstruct images from two differently augmented variants of the original ones, one defining the pose and the other for identity
RA, We undertake analysis to characterize the geometry of the boundary, which is more curved within the adversarial subspace than within a random subspace of equal dimensionality
RA, 3) Overall, we integrate CAP and CAI into a Class-Aware Doubly Robust (CADR) estimator for training an unbiased SSL model
RA, We also integrate our model with GAIL, to achieve robustness to the problem of compounding error caused by unseen states
RA, We call our model Sliced Recursive Transformer (SReT), which is compatible with a broad range of other designs for efficient vision transformers
RA, We propose a novel training method based on a generator regularization using external or internal attributes every  n -th iteration using the pre-trained contrastive encoder and pre-trained attributes’ classifier
RA, It also reveals that contrary to the other token embeddings, the embeddings of these latent labels are sensitive to tasks; sometimes pretraining them can lead to significant performance loss rather than promotion
RA, Our ablation study also shows that both the mask-based update and the imputation accuracy play important roles in achieving the high performance in IA-MARL
RA, We compare the proposed model performance with a text similarity and Deep-and-Cross Network-based approach as the baseline
RA, To ensure a highly-varied conditional speech distribution in view generation, we design a novel diffusion-based speech synthesizer
RA, Additionally, we find that light-weight modality-specific parallel adapter modules further improve performance
RA, We also propose a novel technique that uses the limited training data of the restricted classes to remove the restricted class information from these parameters and uses the limited training data of the remaining classes to reuse these parameters for the remaining classes
RA, To alleviate this issue, NetAug augments the network (reverse dropout) instead of inserting noise into the dataset or the network
RA, Second, we propose an algorithm that optimizes for the worst-off group assignments from the constraint set
RA, We also derive new attack algorithms from our framework that can achieve a high AUC score while also highlighting the different factors that affect their performance
RA, We uncover the impact of using L2 vs  L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention through empirical architecture studies
RA, Based on this, we show that it is possible to identify the data representation up to simple transformations
RA, To improve its scalability and practicability, we propose a top-down beam-search approach to quickly identify the subspace that may contain adversarial examples
RA, Then we propose the Online Mirror Multiple Descent algorithm with two variants, which computes the composite gradient using either the vanilla min-norm solver or a newly designed  L1 -regularized min-norm solver
RA, The core idea of this framework is to first extract various components of visual dynamics using a set of spatiotemporal slots with independent parameters
RA, To improve the efficiency, we build the GNN on top of the intermediate layer output of the FiD encoder and only pass a few top reranked passages into the higher layers of encoder and decoder for answer generation
RA, The proposed algorithm can translate any output of the trained GAN from domain A to domain B
RA, We employ the proposed framework to address the following key questions in curriculum learning research: (a) What is the best curriculum to train a given model on a given dataset? (b) What are the characteristics of optimal curricula for different datasets and different difficulty scoring functions? We show that our framework outperforms competing state-of-the-art curriculum learning approaches in natural language inference and other text classification tasks
RA, We instantiate these wrappers with the binary histogram binning (HB) algorithm, and show that the overall procedure has distribution-free calibration guarantees
RA, Our optimization procedure backpropagates through the sampling process using the reparameterization trick
RA, To this end, we propose a novel contrastive learning framework, dubbed Contrastive Quant, to encourage the feature consistency under both (1) differently augmented inputs via various data transformations and (2) differently augmented weights/activations via various quantization levels, where the feature consistency under injected noises via quantization can be viewed as augmentations on both model weights and intermediate activations as a complement to the input augmentations
RA, Then, we use existing classifiers to annotate generated unlabeled examples with pseudo labels, which are used as additional training data or as additional prompts
RA, We then introduce the nuisance-randomized distribution, a distribution where the nuisance and label are independent
RA, This ensures that the learnability can be easily restored with a simple inverse transformation while remaining difficult to be detected or reverse-engineered
RA, Building on our analysis, we further investigate how the loss function affects double descent --- and thus uncover interesting properties of neural networks and their Hessian spectra near the interpolation threshold
RA, We conduct extensive experiments on the representative meta-learning scenarios to verify our proposed method, including few-shot learning and robust reweighting
RA, We provide a new synthetic dataset with more complex textures on objects and background and found several previous models not based on predictive learning overly rely on clustering colors and lose specificity in object segmentation
RA, We evaluate our model on the standard setting of DeepMind Control Suite, and also on a natural background setting, where the background is replaced by natural videos irrelevant to the task
RA, To test ImageNet multiclass anomaly detectors, we introduce a new dataset of anomalous species
RA, In this work, we devise a method that enables meta-learning on long-horizon, sparse-reward tasks, allowing us to solve unseen target tasks with orders of magnitude fewer environment interactions
RA, We show that with M agents and N total samples when certain generalized inner-product kernels (resp
RA, In that case, (a) disentanglement loss implicitly enlarges the potential ’understandable’ distribution to encourage convexity; (b) convexity can in turn improve robust and precise disentanglement
RA, Our D^2-GCN is achieved by identifying the importance of node features via a low-cost indicator, and thus is simple and generally applicable to various graph-based learning tasks
RA, We discover that the current Deep Ritz Methods is sub-optimal and propose a modified version of it
RA, Based on the second-order Taylor expansion of this metric, we propose Adaptive Temperature Knowledge Distillation (ATKD), which automatically changes the temperature of the teacher and the student, to reduce the sharpness gap
RA, We explored models with different degrees of biophysical detail, and found that models with conductance-based synapses provide markedly better predictions than current-based synapses for this system
RA, Then, we reorganize image tokens by preserving attentive image tokens and fusing inattentive ones to expedite subsequent MHSA and FFN computations
RA, Our work reveals that existing knowledge-aware GNN modules may only carry out some simple reasoning such as counting
RA, To further tighten the overapproximation, our method keeps the Taylor model remainders symbolic under the linear mappings when propagating Taylor models across the neural-network controller
RA, Then we propose the AutoSSL framework which can automatically search over combinations of various self-supervised tasks
RA, Most importantly, we propose a novel solution allowing us to run our algorithm over mini-batches with stochastic gradient fashion and to decouple the number of auxiliary variables with the size of the dataset
RA, We find that to get the maximum speed up the neural network needs to be a special graph convolutional network (GCN) with its aggregation function constructed from the gradients of the loss function
RA, We train RL agents from pixels with this auxiliary objective to learn robust representations that can compress away task-irrelevant information and are predictive of task-relevant dynamics
RA, We open-source (https://github.com/iclr2022anon/fast_finite_width_ntk) our two algorithms as general-purpose JAX function transformations that apply to any differentiable computation (convolutions, attention, recurrence, etc) and introduce no new hyper-parameters
RA, We experimentally investigated several normalization methods for minimizing the variance of the norm and found that the same performance can be obtained by using the L2 normalization and embedding space transformation without fine-tuning or meta-learning
RA, Then, we introduce the Predictive Path Elimination (PPE) algorithm, that learns a generalization of inverse dynamics and is provably sample and computationally efficient in EX-BMDPs when the endogenous state dynamics are near deterministic
RA, We experiment with a variety of simulated continuous control problems and show that our approach learns an optimal policy with up to 5 -- 1,000\times less data than model-based RL baselines and 10^3 -- 10^5\times less data than model-free RL baselines
RA, Our model also consists of an auto-regressive Transformer model that encodes the language input and decodes output, for both question-answering and command grounding with respect to the UI
RA, We introduce one specific use case for facilitating anonymized and safe image sharing
RA,   We perform extensive studies on benchmark datasets for one-class OOD detection and show state-of-the-art performance in the presence of pollution in the ID data, and comparable performance otherwise
RA, Our attack methods are capable of handling scenarios when the clients' data is independent and identically distributed and when the data is independent but not necessarily identically distributed
RA, In addition, we also developed several strategies to select the most vulnerable agents that help to further decrease the team reward of MARL
RA, By leveraging this non-contrastive loss, we show that the supervised ImageNet-1k pretraining with our method results in better transfer performance as compared to the baselines
RA, We then leverage a stochastic Frank-Wolfe (SFW) algorithm to solve this new constrained optimization, which naturally leads to sparse weight updates each time
RA, We enable multi-label CaPC and show that our mechanisms can be used to collaboratively improve models in a multi-site (distributed) setting
RA, The resulting pair of classifiers guide us to transfer text in the specified direction, creating sentences of the type not seen during training
RA, Based on this insight, we propose to exploit the abundant storage to preserve past experiences and alleviate the forgetting by allowing CL to efficiently migrate samples between memory and storage without being interfered by the slow access speed of the storage
RA, We perform analysis with the transformer-based language models, showing our weighted SVD largely alleviates the mismatched optimization objectives and can maintain model performance with a higher compression rate
RA, We apply this property to understanding the effects of climate change on different social and economic activities, evaluating sample quality, and selecting features targeting different decision tasks
RA, Our framework uses (1) a multi-task visual relationship encoder to extract constituent concepts from raw visual input in the source domain, and (2) a neural module net analogy inference engine to reason compositionally about the inferred relation in the target domain
RA, We then propose an optimization algorithm to solve FAFL which can be efficiently implemented in a federated setting and provide convergence guarantees
RA, Further, we leverage a learning to learn approach, guided by a held-out meta learning set and use an additive modeling to predict the MvCoM
RA, We analyze the performance of these linear probes by source and target task and by layer depth
RA, In particular, we change the (1) Architecture and task setup, to a Transformer-LSTM Hybrid as well as a Decoder-only transformer with language modeling loss (2) Noise level in the training distribution, starting with noisy data with filtering applied as well as clean data corrupted with synthetic iid noise
RA, We choose the subgraph encoder to be a GNN (mainly MPNNs, considering scalability) to design a general framework that serves as a wrapper to uplift any GNN
RA, For special cases of the loss and design space, we develop gradient-based methods to efficiently optimize our proposed family of acquisition functions, and demonstrate that the resulting BO procedure shows strong empirical performance on a diverse set of optimization tasks
RA, Our Transformers replace the LSTM-style mechanism to capture temporal dependencies
RA, This includes the analysis of convergence, representational capacity, and interpretability
RA, We introduce baseline results and metrics on this task, finding that modeling editing processes improves performance on a variety of axes on both our proposed task and related downstream tasks compared to previous single-step models of edits
RA, Our second contribution is improving programmatic policies to support compositionality by integrating primitive functions learned to grasp task-agnostic skills as a composite program to solve novel RL problems
RA,  We show that standard adversarial training on base categories along with centroid-based classifier in the novel categories, outperforms or is on-par with state-of-the-art advanced methods on standard benchmarks such as Mini-ImageNet, CIFAR-FS and CUB datasets
RA, We also release the set of considered benchmark tasks and implementations of our methods in order to further drive research in this challenging area
RA, We then provide generalization bounds which illustrate how pooling and patches yield improved sample complexity guarantees when the target function presents such regularities
RA, To overcome this limitation, we propose a tractable particle method that approximately solves the dual problem, and an importance re-weighted technique to reduce the computational cost
RA, To own knowledge, our method is the first to solve the challenging Google Research Football full game \mathtt{5\_v\_5
RA, We apply SeqPATE to a simple text generation task (sentence completion) and achieves 39% and 28% gains in Bleu4 on two datasets
RA, We examine HALP on both classification and detection tasks, over varying networks, on ImageNet1K and VOC datasets
RA, We show that by explicitly leveraging this compact representation to encode changes, we can efficiently adapt the policy to the target domain, in which only a few samples are needed and further policy optimization is avoided
RA, Second, we design mechanisms that transform the sensitive features such that the data obtained from projecting features back to the image protects from the leakage of sensitive information
RA, In addition, we find that model selection plays a key role and different selection strategies can significantly affect performance
RA, Thereafter, we extend our study to a bigger picture that the customers may develop algorithms to eliminate the effect of confusion neurons and recover the original network, and we show that our simple approach is somewhat capable of defending itself against the fine-tuning attack
RA, In particular, our regret bound reduces a factor of \sqrt{\text{log(T) compared to the popular OFUL algorithm \citep{Abbasi11 which uses the UCB approach, and reduces a factor of \sqrt{d\text{log(T) compared to another popular algorithm \citep{agrawal13 which uses the TS approach
RA, We further propose a novel fairness learning algorithm, termed Curvature Matching (CUMA), to simultaneously achieve both traditional in-distribution fairness and our new robust fairness
RA, Moreover, to better leverage MI-based collaboration signals, we propose a novel MARL framework, called Progressive Mutual Information Collaboration (PMIC) which contains two main components
RA, In addition, we devise a CPFE (Color Point Feature Extractor) to extract both 3D geometric and 2D semantic features in pseudo point clouds
RA, Altogether, this provides a single, unified strategy that offers both significantly reduced training requirements and improved performance across both low and richly labeled regimes and over varying degrees of imbalance
RA, Equipped with our enhanced training algorithm, we are the first to successfully scale up the batch size when training Vision Transformers (ViTs)
RA, Therefore, we introduce a second technique for light-weight fine-tuning where we only update a small number of the model parameters
RA, Our model unravels a sequence autoregressively while iteratively refining its predicted global structure
RA, Our method formulates the rule discovery process from noisy event data as a maximum likelihood problem, and designs an efficient and tractable branch-and-price algorithm to progressively search for new rules and expand existing rules
RA, To leverage the sparsity in hypergraph neural networks, SpaLoc represents the grounding of relationships such as parent and grandparent as sparse tensors and uses neural networks and finite-domain quantification operations to infer new facts based on the input
RA, Finally, we employ randomized smoothing to provably map similar individuals close together, in turn ensuring that local robustness verification of the downstream application results in end-to-end fairness certification
RA, In particular, we develop a novel spatiotemporal neural process model to mimic the simulator dynamics
RA, Specifically, we adopt the noisy labels and model predicted probability to estimate PTM and then correct the NTM in forward propagation
RA, Our algorithm does not rely on any feedback from annotators in the target domain and hence, which is able to not only work standalone but also benefit existing multi-round active learning algorithms by providing a warm-start
RA, This allows us to provide sparse transport plans and avoid numerical issues of methods that use entropic regularization
RA, We also evaluate LAE on the image generation task using three datasets (SVHN, CIFAR-10, and CelebA-HQ)
RA, To empirically evaluate our method, we present the Multi-Agent COordination (MACO) benchmark by collecting classic coordination problems in the literature, increasing their difficulty, and classifying them into different types
RA, For example, we improve robust accuracy by up to 7.5% and 6.7% in \ell_{\infty and \ell_2 threat model over baselines that are not using proxy distributions on the CIFAR-10 dataset
RA, Then, motivated by the recent development of powerful cross-modal representation learning approaches, we present Domino, an SDM that leverages cross-modal embeddings and a novel error-aware mixture model to discover and describe coherent slices
RA, Our method significantly outperforms prior approaches by 24% and 10% average normalized score over 12 games from the Jericho benchmark (Hausknecht et al,2020) in both deterministic and stochastic settings, respectively
RA, We propose new methods to induce denial-of-service attacks and incorporate domain-specific insights and constraints to accomplish two key goals: (i) create smooth adversarial attacks that are physiologically plausible; (ii) consider the realistic case where the attack happens at the origin of the signal acquisition and it propagates on the human head
RA, This approach, which we call reverse linear probing, provides a single number sensitive to the semanticity of the representation
RA, This enables training on extremely small local datasets, such as patient data across hospitals, while retaining the training efficiency and privacy benefits of federated learning
RA, In addition, we couple the training of NSM with an implicit shape reconstruction task, making NSM more robust to imperfect point cloud observations
RA, We study various aspects of our proposed model including, dependency on the number of eigencomponents utilized, latent polynomial filters learned, and performance of the individual polynomials on the node classification task
RA,  To further the understanding of such efficiency-oriented threats and raise the community’s concern on the efficiency robustness of NMT systems, we propose a new attack approach, TranSlowDown, to test the efficiency robustness of NMT systems
RA, Our method does not assume a fixed measurement process during training, and can thus be flexibly adapted to different measurement processes at test time
RA, To further reduce the network usage of our approach, we develop several compression-aware architecture modifications and evaluate their tradeoffs
RA, Then, instead of directly manipulating graphs, we interpolate graphons of different classes in the Euclidean space to get mixed graphons, where the synthetic graphs are generated through sampling based on the new graphons
RA, Specifically: 1) For ownership verification, watermarking techniques are commonly used but are often vulnerable to sophisticated watermark removal methods
RA, This enables rendering scenes even when objects or lights move, without retraining
RA, We show the advantages of our method in a new set of sequential deformable object manipulation tasks over previous reinforcement learning algorithms and the trajectory optimizer
RA, To facilitate further research, we provide an open-source package that administers the GDS benchmark with modular combinations of popular domain generalization algorithms and GNN backbone models
RA, We perform experiments on Gym's MuJoCo continuous control benchmarks and empirically validate performances
RA, We use the RTD score to gain insights on neural networks representations in computer vision and NLP domains for various problems: training dynamics analysis, data distribution shift, transfer learning, ensemble learning, disentanglement assessment
RA, The proposed method is compared with recent methods using (semi-)synthetic datasets
RA, We show that regularizing graph neural networks with WT-AWP consistently improves both natural and robust generalization across many different graph learning tasks and models
RA, Our bounds hold for general activation functions, including ReLU
RA, We instantiate this idea as a publicly available software framework called Sequoia, which features a wide variety of settings from both the Continual Supervised Learning (CSL) and Continual Reinforcement Learning (CRL) domains
RA, Finally, we discuss connections of our framework with AI architectures with team competition structure like multi-agent generative adversarial networks
RA, We study such a distinction and further propose a cooperative defense strategy termed by Co-SSAT
RA, We also adopt a consistency regularization on generated auxiliary samples between multi-binary classifiers and the model trained by SDG methods, to improve the model’s capability on unknown class identification
RA, We show that the “hard” (e,g, high loss) points usually selected in the optimization literature are typically noisy, leading to deterioration on real-world datasets
RA, We also describe the relationship with its deep RKHS variant as well as standard Deep Gaussian Processes
RA, Based on this insight, we propose jointly optimizing for current task loss and loss basin sharpness in order to explicitly encourage wider basins during sequential fine-tuning
RA, Furthermore, we investigate the use of optimism vs  pessimism for value functions in the offline-online setting due to their use in batch and online RL
RA, Our main observation is that the use of closed-form models opens doors to end-to-end training thanks to differentiability of the solvers
RA, We introduce a variational inference framework to jointly learn how to partition the edges into different communities and combine relation-specific GCNs for the end classification tasks
RA, Specifically, instead of using a permanent sparse gate, DTS-Gate begins as a dense gate that routes tokens to all experts, then gradually and adaptively becomes sparser while routes to fewer experts
RA, In this regard, we also present a regularization-based technique to do so, which is rather simple-to-implement yet effective in dynamical isometry recovery on modern residual convolutional neural networks
RA, We also propose a novel method to efficiently map CSQ to binarized neural network hardware using bitwise operations
RA, We evaluate UC-DiffOSI on articulated rigid body control tasks, including a wiping task that requires contact-rich environment interaction
RA, We present an efficient Epoch Gradient Descent algorithm for SDOT (SDOT-EGD) that computes the learning rate, number of iterations, and number of epochs in order to guarantee an arbitrarily small MRE in expectation
RA, Then, we use a recurrent model to sequentially predict the causal structure model based on previous observations to capture the non-stationary dynamic of the casual structure
RA, By combining these two aspects, we show that DisTop outperforms a state-of-the-art hierarchical reinforcement learning algorithm when rewards are sparse
RA, Based on the unique architecture of SEAL, we further propose a variant of SEAL that utilizes noise contrastive ranking (NCE) loss that by itself does not perform well as a structured energy network, but embodied in SEAL, it shows the greatest performance among the variants we study
RA, We find that double descent can be attributed to distinct features being learned at different scales: as fast-learning features overfit, slower-learning features start to fit, resulting in a second descent in test error
RA, In this process, the forgetting step selectively removes undesirable information from the model, and the relearning step reinforces features that are consistently useful under different conditions
RA, We propose POLARAE which exploits all four components and outperforms standard convolutional autoencoders and variational autoencoders trained iteratively with high diversity wrt OOD generalization to larger shapes in larger grids and new locations
RA, Our proposed model encourages each expert, i,e, decoder, to learn and generate stylistically-distinct summaries along dimensions such as abstractiveness, length, specificity, and others
RA, Our proposed model pools the image features using squared pooling regions, an approximation to the biologically-inspired foveated architecture, and uses the pooled features as an input to a Transformer Network
RA, We derive a novel score matching objective for CLD and show that the model only needs to learn the score function of the conditional distribution of the velocity given data, an easier task than learning scores of the data directly
RA, We design experiments to thoroughly test metrics on their ability to measure the diversity and fidelity of generated graphs, as well as their sample and computational efficiency
RA, We first use the zero-shot translation ability of large pretrained language models to generate translations for a small set of unlabeled sentences
RA, This method is well-suited for the setting where the distribution of target data changes rapidly (e,g,, a wireless channel), making it challenging to collect a large number of samples and retrain
RA, We leverage consistency regularization across different data augmentations of the two feature types to optimize the prediction consistency for the model
RA, Specifically, we consult empirical findings from a series of literature regarding Lottery Ticket Hypothesis to determine the optimal clustering scheme per layer, and develop a simple yet cost-efficient greedy approximation algorithm to determine which group kernels to keep within each filter group
RA, Then, we propose a framework to iteratively extract small subsets of training data that when augmented, closely capture the alignment of the fully augmented Jacobian with label/residual vector
RA, We demonstrate how to build a one-layer-size neural network to achieve similar performance compared to other traditional CNN models on various applications and datasets
RA, We also propose a new magnitude-based pruning algorithm to preserve the above property
RA, Towards this end, we use domain transportation to characterize the learning-intervention mechanism of recommendation
RA, To overcome it, we discover a novel connection to Temporal Point Process (TPP), where we use its intensity parameter to predict the user's likelihood of future engagements, based on but not biased by the past observations
RA, We optimize EMC with a novel discrete optimization algorithm, Cost-Optimized Local Search (COLS), which is guaranteed to improve the recourse set quality over iterations
RA, We thus constructed an efficient approximation named AND_AIL (the AND operator Approximate for Independent Logits) utilizing only comparison and addition operations, which can be deployed as an activation function in neural networks
RA, In this paper, we propose perturbation transformation to show how the input perturbation designed for certified robustness can be transformed into gradient perturbation during training
RA, Therefore, jointly training the three policies leads to efficient RL guided by a curriculum progressively improving the sparse reward and generalization
RA, We propose a novel one-step training procedure that makes such training feasible, and prove a lemma that provides theoretical justification for this training procedure
RA, We thus provide new baselines for unsupervised OOD detection methods
RA, Thus, CNC aims to improve representation alignment via contrastive learning
RA, This indicates that our algorithm enjoys linear speedup
RA, We find that when low-quality data is removed, robust overfitting and robustness overestimation can be largely alleviated; and robustness-accuracy trade-off becomes less significant
RA, To encourage further research on reward misspecification, address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors
RA, This inverse mapping gives informative feedback for our proposed retrieval system to maintain global contextual information of a given story
RA, We then propose an efficient algorithm that computes a low-rank approximation of NFK, which scales to large datasets and networks
RA, Furthermore, we use a machine translation model that takes input text and translates it into a discrete sequence of associated gesture chunks in the learned gesture space
RA, With this two-loss, two-predictor framework which we name Federated Robust Decoupling (Fed-RoD), the learned model can simultaneously achieve state-of-the-art generic and personalized performance, essentially bridging the two tasks
RA, Moreover, we present an ablation study to understand the contribution of each of its components
RA, We propose a semantic synthesis strategy that enables realistic simulation without naturally partitioned data
RA, We show that with a relatively small number of extra parameters, our method allows bias tuning to perform successful fine-tuning in both the single-feature and compositional setting
RA, We find that CIC improves on prior unsupervised skill discovery methods by 91% and the next-leading overall exploration algorithm by 26% in terms of downstream task performance
RA, This technique allows for outcomes to be weighed based on relative quality, does not require modification of the reward function to modulate agent behavior, and may be used for both continuous and discrete action spaces
RA, Additionally, we propose an inference procedure which utilizes scaled energy to achieve a final predictive distribution which can better separate OOD data, and is well calibrated under a distributional dataset shift
RA, We propose a technique to create ensembles of diverse models, and further propose Error Node Isolation (ENI), which prevents error nodes from sending messages to (and thereby influencing) other nodes
RA, We evaluated SpaceMAP on a range of artificial and real datasets with different manifold properties, and demonstrated its excellent performance in comparison with classical and state-of-the-art DR methods
RA, To these models, we oppose Critical Rationalist Networks (CRNs), which instead embrace a rationalist view on the acquisition of knowledge
RA, We adapt Clockwork VAE, a state-of-the-art temporal LVM for video generation, to the speech domain, similar to how WaveNet adapted PixelCNN from images to speech
RA, Our proposed architecture can be generally used for many implicit neural representation tasks with no additional memory overheads
RA, To evaluate the effectiveness of our method, we introduce PlasticineLab-M that extends the existing differentiable physics benchmark PlasticineLab to eight new challenging multi-stage soft-body manipulation tasks
RA, Specifically, we use CIVD (1) to integrate parametric and nonparametric few-shot classifiers; (2) to combine feature representation and surrogate representation; (3) and to leverage feature-level, transformation-level, and geometry-level heterogeneities for a better ensemble
RA, In doing so, we (1) sidestep standard VAE mechanisms such as sampling approximation, reparameterization trick and amortization, and (2) observe a much sparser encoding compared to autoencoders that use annealed discrete latents
RA, Specifically, we present settings of multi-agent performative prediction where under sufficient conditions, their dynamics lead to global stability and optimality
RA, Furthermore, we address OOD identification challenge by introducing an initialization scheme which provides reasonably larger PIs of the OOD samples than those of the in-distribution samples
RA, The proposed algorithm is found to improve the model’s generalization on three datasets: Colored MNIST (Kim et al, 2019), Princeton ModelNet40 (Wu et al, 2015), and NVIDIA Dynamic Hand Gesture Dataset (Molchanov et al, 2016)
RA, We propose the principle of attacking model space for solving bilevel attack objectives, and present Greedy Model Space Attack (GMSA), an attack framework that can serve as a new baseline for evaluating transductive-learning based defenses
RA, Our framework covers all the classical Wasserstein gradient flows including the heat equation and the porous medium equation
RA, We do this by maximizing the gradients of the new neurons and find an approximation to the optimal initialization by means of the singular value decomposition (SVD)
RA, Our findings are consistent for different NN architectures, including LeNet, VGG, AlexNet, and ResNet
RA, Specifically, (a) our spatial uncertainty score estimates how different OOD latent-space representations are from those of an in-distribution set using metrics such as Mahalanobis distance and cosine similarity and (b) our temporal uncertainty score determines deviations in correlations over time using representations of past inputs in a non-parametric, sliding-window based algorithm
RA,  In contrast, in this paper the controllability aspects of the coarse system are derived from coarse summaries {\em without knowledge of the fine-scale structure
RA, Specifically, in the deterministic environment, we develop a practical implementation of SPLID, which imposes \delta-distilled policy by discriminating First Hit Time (FHT)
RA, In a variety of text-based games, we show that this simple method results in competitive performance for agents
RA, We evaluate the proposed Dict-BERT model on the language understanding benchmark GLUE and eight specialized domain benchmark datasets
RA, To do so, we learn a set of orthogonal motion directions simultaneously, and use their linear combination to represent any displacement in the latent space
RA, Equipped with the learned fragment vocabulary, we propose Fragment-based Sequential Translation (FaST), which learns a reinforcement learning (RL) policy to iteratively translate model-discovered molecules into increasingly novel molecules while satisfying desired properties
RA, Our methodology expands upon prior work on “classical” streaming algorithms, as previous multi-pass and random order streaming algorithms can be seen as special cases of our algorithms, where the first pass or random order was used to implement the heavy edge oracle
RA, In addition, we discuss how the WILDS benchmark of Domain Generalizations and Subpopulation Shifts will aid in future work
RA, These metrics facilitate both more robust evaluation of NAS algorithms and provide practical method for designing complete NAS algorithms from separate supernet training and architecture selection techniques
RA, We examine the proposed method across multiple frameworks in a wide range of settings and demonstrate superior performance on multiple benchmarks from simulated to real-world data
RA, To aim for effective, rather than direct, sparsity, we develop a low-cost extension to most pruning algorithms
RA, Next, our model interprets supplemental sentences to relate the novel concept with other known concepts, such as ``X has property Y'' or ``X is a kind of Y''
RA, Instead of conventional Softmax classifiers, we use distance-based classifiers, which utilize the principle of linear discriminant analysis
RA, This decoupling allows for micro-updates, produced by gradient descent, to stack up, leading to the possible re-activation of weights that were set to zero in earlier training steps
RA, Our loss integrates CE with explicit {\cal L_1 regularization, which encourages label marginals to match target class proportions, thereby mitigating class imbalance but without losing generality
RA, We apply this ``identity-disentangled adversarial augmentation (IDAA)'' to different self-supervised learning methods
RA, Besides the model improvement, we also introduce a new evaluation metric for measuring models' ability to preserve the identity in the restored faces
RA, Our first contribution is a fine-grained analysis of the expressiveness of these neural networks, that is, the set of functions that they can realize and the set of problems that they can solve
RA, One of our main technical contributions is to characterize the stability of certain fixed point strategies through a refined perturbation analysis of a structured Markov chain, which may be of independent interest
RA, This includes the improvements on Natural Questions R@5 to 77.9%(+2.1%), TriviaQA R@5 to 78.2%(+1.4), and MS-MARCO MRR@10 to 39.5%(+1.3%)
RA, We then propose MixRL, a data augmentation meta learning framework for regression that learns for each example how many nearest neighbors it should be mixed with for the best model performance using a small validation set
RA, To adapt to directed graphs, our model generates multiple highly interpretable latent variables as node representations, and the interpretability of representing node influences is theoretically proved
RA, We propose a novel metric called phoneme attention relationship (PAR) to investigate that phonetic localization in the lower layers extracts phonologically meaningful features from speech and standardizes the phonetic variance in the utterance for proper linguistic localization in the upper layers
RA, We make this observation practical by designing self-supervised learning methods that use unlabelled data and augmentations to train robust representations
RA, We apply {LatTe-Flows to a challenging sensor-signal forecasting task, using multivariate time-series measurements collected by wearable devices, an increasingly relevant health application
RA, To combat memorization and promote generalization, we present a simple yet effective noising framework that can be combined with existing models
RA,  We then solve the query design problem by optimizing these risk bounds with respect to the choice of query set and obtain a finite sample statistical rate, which depends primarily on the eigenvalue spectrum of a certain linear operator on the RKHSs
RA, We significantly outperform past methods in three tasks: 1) reconstructing the input structure, 2) generating valid, diverse, and realistic materials, and 3) generating materials that optimize a specific property
RA, Our approach is to assume that the assignment of objects to slots during generation is a deterministic function of the scene latent variable
RA, We also propose to regularize batch-normalization parameters for better preserving dynamical isometry for the whole network
RA, To achieve this goal, we reformulate the data input by eliminating the sensitive information and strengthen model fairness by minimizing the marginal contribution of the sensitive feature
RA, For instance, our method on a hardware-deployed ResNet-20 model trained on CIFAR-10 can achieve over 91% test accuracy and 94% attack success rate by flipping only 10 bits out of 2.2 million bits
RA, To address the first issue, we perform volume rendering only to produce a low-resolution feature map, and progressively apply upsampling in 2D
RA, Utilizing finDML, we find bias in DML representations to propagate to common downstream classification tasks
RA, We show that the traditional labeling of adversarial examples inherited from their clean counterparts will lead to implicit label noise
RA, We propose this may be due to a double-counting effect of the image statistics, once in the perceptual distance and once in the training procedure
RA, To facilitate the development of methods that can work reliably on real-world distribution shifts, we provide an open-source package containing all of the relevant data loaders, model architectures, and methods
RA, We introduce a new evaluation metric for VL grammar induction, CCRA, and show a 3.3% improvement over a strong baseline on Flickr30k Entities
RA, Our approach significantly outperforms the existing alternatives on several datasets, especially when segmenting objects from rare classes
RA, We show that if the scaling measures are constructed such that they overlap with p and q, then a single multi-class logistic regression can be trained to accurately recover p/m and q/m on samples from p, q and m
RA, We learn such representations on a collection of video walkthroughs and demonstrate successful transfer to multiple downstream navigation tasks
RA,  We consider the methodology of boosting, borrowed from supervised learning, for converting weak learners into an effective policy
RA, Additionally, we show that policies trained on suboptimal data that is sufficiently noisy can attain better performance than even BC algorithms with expert data, especially on long-horizon problems
RA, Lastly, we draw links of our proposed method with self-supervised contrastive learning without negative data pairs
RA, In the multi-agent setting, we evaluate on TrajNet to showcase the model's socially-consistent predictions
RA, To learn the global model, the objective is to minimize the optimal transport cost of the global model's predictions from the confident sum of soft-targets assigned by local models
RA, We propose MetaQNL, a "Quasi-Natural" language that can express both formal logic and natural language sentences, and MetaInduce, a learning algorithm that induces MetaQNL rules from training data consisting of questions and answers, with or without intermediate reasoning steps
RA, Thus, a center-suppressed sampling is further designed to enlarge the variance of crops
RA, We then refurbish or apply unsupervised learning by splitting noisy examples into multiple groups using the Gaussian mixture model for addressing label noise
RA, This setting mimics a dynamical eye viewing objects in perceptually-challenging conditions
RA, We also consider a variant of this algorithm in which the particles are sometimes restarted at random samples drawn from the data set, and  show that performing these restarts at every iteration step corresponds to score matching training
RA, We construct a detailed dataset composed of relevant signals over the past year of the pandemic
RA, This connection suggests that, more generally, techniques from the noisy label literature can be useful to improve robust generalization
RA, To mitigate this issue, we propose simple bias decay methods including a novel adaptive one and found that this simple remedy can fill a large portion of the performance gaps that occur in large batch optimization
RA, To evaluate the performance of the proposed method, it is run on the ModelNet40 and ScanObjectNN datasets by employing the state-of-the-art point cloud classification models; including PointNet, PointNet++, and DGCNN
RA, We present empirical results across a range of models and across both expository and real-world image and language domains
RA, This output token selection between the two generators allows the adapter to take into account only on the task-relevant parts in sequence generation, and therefore makes it more robust to overfitting as well as more stable in RL training
RA, We propose a multi-tasking Neural Net debiasing method with stochastic gradient descent minimization of a combined Reisz representer and regression loss, while sharing representation layers for the two functions
RA, We then use insights from the motor thalamocortical circuit, featuring a specific module that shapes motif transitions
RA, We show a formal connection between our formulation and optimal transport by relaxing AT into DRO problem with an \infty-Wasserstein constraint
RA, Furthermore, we introduce conditional generative modelling with a Kantorovich dual objective by constructing an affine latent model with respect to the covariates
RA, We show that using an asymmetric update rule pushes new classes to adapt to the older ones (rather than the reverse), which is more effective especially at task boundaries, where much of the forgetting typically occurs
RA, For example, our method reduces the FLOPs of DeiT-S by over 42.6% while only sacrificing 0.46% top-1 accuracy
RA, We formalize our approach as offline targeted environment design(OTED), which automatically learns a distribution over simulator parameters to match a provided offline dataset, and then uses the learned simulator to train an RL agent in standard online fashion
RA, It enables us to draw from the latest RL advances, such as path consistency learning, to combine the best of on-/off-policy updates, and learn effectively from sparse reward
RA, We find that this relationship holds across loss objectives and architectures, and further demonstrate the trend both on the standard OSR benchmarks as well as on a large-scale ImageNet evaluation
RA, Following this result, our method aligns supports by minimizing a symmetrized relaxed optimal transport cost in the discriminator 1D space via an adversarial process
RA, We perform an extensive study of the key features of the proposed framework and analyze the characteristics of the learned representations
RA, Then, we improve mixup generation with these properties from two aspects: we enhance modeling non-linear mixup relationships between two samples and discuss learning objectives for mixup generation
RA, First, we show how to re-interpret AdamW as an approximation of a proximal gradient method, which takes advantage of the closed-form proximal mapping of the regularizer instead of only utilizing its gradient information as in Adam-\ell_2
RA, Overall, our learning-based simplification approach offers a valuable new tool to explore the basis of network decisions
RA, For example, we build 3D CNNs equivariant to the symmetries of platonic solids or choose G=SO(2) when working with 3D data having only azimuthal symmetries
RA, Our method is agnostic to the type of latent symmetry; we demonstrate its usefulness over C_4 \times S_5 using G-convolutions and GNNs, over D_4 \ltimes (\mathbb{R^2,+) using E(2)-steerable CNNs, and over \mathrm{SO(3) using tensor field networks
RA, We show that recommender systems that optimize for staying in the trust region can avoid manipulative behaviors (e,g,, changing preferences in ways that make users more predictable), while still generating engagement
RA, This is in contrast with vanilla Q-learning which does not take into account unknowns during training
RA, Our approach integrates insights from neuroscience and introduces a framework with high potential for applications in model-based reinforcement learning, where flexible and informative state-space rollouts are of particular interest
RA, Using adversarial distributions, we investigate OOD detectors with reported near-perfect performance on standard benchmarks like CIFAR-10 vs SVHN
RA, We extend this method for sustained symmetric locomotion tasks for deep reinforcement learning using a Normalized Cumulative Fatigue (NCF) model
RA, We provide the condition under which the ASWD is a valid metric and show that this can be obtained by an injective neural network architecture
RA, Then we propose the MetaTag framework: 1) to learn the metric over a limited number of streaming-snapshot modeled temporal graphs, 2) and adapt the learned metric to unseen temporal graphs via a few examples
RA, This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction
RA, Our key idea is to rank weights based on a measure of contribution, and selectively use the most salient weights to derive the output for OOD detection
RA, Finally, we extend our approach to semi-supervised regime utilizing unlabeled samples in the support set and further improving few-shot performance in the presence of unlabeled data
RA, We then compute the cosine distance between the concatenated weights of the linear models
RA, Our method then computes the subset of K attribute phrases that form the best clustering of the images
RA, We develop variational objectives under the object-supervised and self-supervised settings to model the stochasticity of predicted dynamics
RA, This highlights a curious fact: minimum a posteriori functions can generalise best, and gradient descent can select for those functions
RA, Next, we develop a global smoothing algorithm for certifying the robustness of a ﬁnite-horizon cumulative reward under adversarial state perturbations
RA, To solve the performance degradation, we propose to apply \text{Global Server Corrections on the server to refine the locally learned models
RA, To this end, we formalize the problem of task-induced representation learning (TARP), which aims to leverage such task information in offline experience from prior tasks for learning compact representations that focus on modelling only task-relevant aspects
RA, This allows us to impose a very tight bound over the error of the aggregation algorithm in presence of adversarial updates from malicious parties
RA, Our proposal replaces the fixed weights of a fully-connected layer with a function of an additional input (reliability score) at each input, mimicking the ability of cortex to up- and down-weight inputs based on the presence  of other data
RA, We provide model scaling studies to verify the computational efficiency of the proposed solutions and conduct several analyses to reveal the source of coding gain of transformers over ConvNets, including better spatial decorrelation, flexible effective receptive field, and more localized response of latent pixels during progressive decoding
RA, We find that these permutations lead to a huge variance of accuracy, making MAML unstable in few-shot classification
RA, This dictionary enables two new concept-guided auxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a local task for facilitating semantic object-centric correspondence learning
RA, We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP tasks verbalized via natural language instruction templates
RA, As an example, we show that including semantically related non-neighboring sentences in the same training example yields improved sentence representations and question answering abilities
RA, We applied GrID-Net on multimodal single-cell assays that profile chromatin accessibility (ATAC-seq) and gene expression (RNA-seq) in the same cell and show that it dramatically outperforms existing methods for inferring regulatory locus-gene links, achieving up to 71% greater agreement with independent population genetics-based estimates
RA, This indicates that ViTs heavily use features that survived such transformations but are generally not indicative of the semantic class to humans
RA, Overall, our work suggests a way of advancing artificial intelligence and robotics research inspired by systems neuroscience
RA, Subject to this constraint, our algorithm then optimizes for a generalized notion of set coverage (i,e, the true discovery rate) that allows for any number of true answers for a given query (including zero)
RA, Finally, we discuss differences between Bellman error and the non-stationary objective used by iterative methods and deep reinforcement learning, and highlight how the effectiveness of this objective relies on generalization during training
RA, We exhibit these findings on large-scale variants of the benchmark game Hanabi
RA, Our discovered hybrid ViT model family, dubbed NASViT, achieves top-1 accuracy from 78.2% to 81.8% on ImageNet from 200M to 800M FLOPs, and outperforms all the prior art CNNs and ViTs, including AlphaNet and LeViT, etc
RA, We also conduct systematic studies on the transfer learning capability of PoNet and observe that PoNet achieves 96.0 percent of the accuracy of BERT on the GLUE benchmark, outperforming FNet by 4.5 percent relative
RA, We further propose a novel personalized federated self-supervised learning algorithm, Per-SSFL, which balances personalization and consensus by carefully regulating the distance between the local and global representations of data
RA, We conducted experiments in high-dimensional continuous control environments for locomotion and dexterous manipulation
RA, Thus, we encourage researchers to treat pessimism with DISDAIN
RA, Specifically, we introduce a regularization term to borrow learning capacity and realize information extrusion from the weights which will be masked
RA, Key in our algorithm design is a connection between autonomous exploration and multi-goal stochastic shortest path, a new problem that naturally generalizes the classical stochastic shortest path problem
RA, Our work suggests an alternative understanding of contrastive learning: the role of aligning positive samples is more like a surrogate task than an ultimate goal, and it is the overlapping augmented views (i,e, the chaos) that create a ladder for contrastive learning to gradually learn class-separated representations
RA, More importantly, we propose a mechanism to automatically build the virtual policy from a memory buffer of past policies, providing a new capability for dynamically selecting appropriate trust regions during the optimization process
RA, This guided walk framework allows for explainability via the walk trajectory information
RA, Our approach relies only on human videos and does not require robot demonstration, which facilitates data collection and is more in line with human imitation behavior
RA, Thus, the block structure reflects meaningful dataset statistics, but is simultaneously unique to each model
RA, Furthermore, our hierarchy of models gradually introduces more physical structure, which we show improves interpretability, generalizability (over larger ranges of time scales and Reynolds numbers), preservation of physical symmetries, and requires less training data
RA, To address the non-stationarity in the continual learning environment, we employ a Q function with task-specific and task-shared components to support fast adaptation
RA, We compared the proposed method with other meta-learning approaches in the few-shot learning tasks such as 1D stochastic regression, image inpainting, and classification
RA, This split-mix strategy achieves customization with high efficiency in communication, storage, and inference
RA, Finally, this abstain mechanism allows us to combine models in a compositional architecture that significantly boosts overall robustness without sacrificing accuracy
RA, Our techniques reveal a deep connection between GANs and PRGs, which we believe will lead to further insights into the computational landscape of GANs
RA, In order to illustrate our idea in a concrete use case, we focus here on gender bias in facial recognition and conduct extensive numerical experiments on standard datasets
RA, We utilize our parsing agent for exemplar generation and type conditioned concept generation in Omniglot challenge without any further training
RA, We hypothesize that these samples are in the low-density region of the ground truth distribution where models are not well trained
RA, This makes it possible for instance to evaluate the model on a single long trial after training on smaller chunks
RA, To obtain dimension independence, we require G_Q^2 \leq p public data samples, where G_Q is the Gaussian width of the smallest convex set Q such that the public loss functions are 1-strongly convex with respect to \|\cdot\|_Q
RA, We explore the noise tolerance of models trained on such automatically-created datasets and show that these models perform comparably to models trained on ground truth for reasonable levels of noise
RA, We first show how to learn a decomposable representation required by SF
RA, (3) Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet with various network architectures show that BWCP outperforms its counterparts by achieving better accuracy given limited computational budgets
RA, In detail, we find that similar sized pathways advance the solution quality at a similar pace, with high redundancy
RA, We then analyze when and why previously proposed methods are likely to fail or reduce to the same performance as the baseline in this harder setting and why we should think carefully about extending such methods beyond the well-curated environments
RA, Through our profound investigation, we discover that in the presence of a LS-trained teacher, KD at higher temperatures systematically diffuses penultimate layer representations learnt by the student towards semantically similar classes
RA, To capture the motion information, we estimate the optical flow and design a context-motion updating operator to integrate features between frames recurrently
RA, Besides that, we propose a method to answer such queries and demonstrate in our experiments that qualifiers improve query answering on a diverse set of query patterns
RA, Moreover, we formally relate multi-distribution density ratio estimation and class probability estimation, theoretically justifying the use of any strictly proper scoring rule composite with a link function for multi-distribution DRE
RA, We cast the design optimization problem as a bi-level optimization problem
RA, We analyze benefits and challenges of learning with a hierarchical policy structure and suggest directions for future work
RA, To handle challenges in visual representation learning such as discontinuities in images and high resolution inputs, we propose principled architectural modifications to FNO which results in memory and computational efficiency
RA, This work proposes a representation learning framework that combines a new gradient-based SOMRL model and autoencoders
RA, We prove that SS-MAIL is part of the family of AIL methods by providing a theoretical connection to cost-regularized apprenticeship learning
RA, To optimize the non-standard compositional objective, we propose an efficient and provable stochastic optimization algorithm
RA, Intuitively, this objective encourages the model to make the same prediction across different augmentations, thus enforcing the invariances encoded in these augmentations, while also maintaining confidence in its predictions
RA, Our surface parameterization network can be conveniently plugged into a differentiable rendering pipeline and trained using multi-view images and rendering loss
RA, We consider a practical MARL setting, where the rewards and actions of each agent are only known to itself, and the knowledge of joint actions of the agents is not assumed
RA, To mitigate the limitations, we propose our model-based stochastic gradient Langevin dynamics sampling method
RA, We then point out that this problem stems from training the head
RA, This framework enables Safety Agent to learn to take actions that minimise future safety violations (during and after training) by performing safe actions at certain states while Task Agent performs actions that maximise the task performance everywhere else
RA, To evaluate the individual stages, we introduce the Fishbowl dataset positioned between complex real-world scenes and common object-centric benchmarks of simplistic objects
RA, This imposes an inductive bias towards natural images which enables the image-to-image MLP-mixer to learn to denoise images based on relatively few examples
RA, Specifically, we use the teacher model to encode category texts and image regions of object proposals
RA,  We first infer a sequence of subgoals to be executed based on language instructions by a high-level policy composition controller (PCC)
RA, Additionally we propose a novel adaptive ``truncation" mechanism for counteracting the negative impacts brought by the ``irreversible" transitions such that learning via retracing can be maximally effective
RA, We also design an efficient algorithm without back-propagation to further reduce the computation complexity of the objective solver
RA, We develop a gradient-based algorithm for it by approximating the original constrained ERM using differentiable surrogate losses and Lagrangians
RA, In this way, ORCA gradually increases the discriminability of the model during the training and reduces the gap between intra-class variance of seen with respect to novel classes
RA, Then,  we propose variational information bottleneck as our objective, which is composed of two parts
RA, Furthermore, we exploit the notion of on-average stability in order to obtain a data-dependent quantity in the bound
RA, Our method can also be used to train deep EBMs for high-dimensional discrete data
RA, In particular, to analyze the sharpness, we instead explore the eigenvalue problem for the low-dimensional matrix which is a rank-one modification of a diagonal matrix
RA, Hence, we transform the supervised inversion task into an unsupervised seismic data reconstruction task
RA, Furthermore, we introduce an linear independence loss to further increase the performance by removing linear dependency along the feature dimension of the batch representation matrix (rank maximization), which has both anticollapsing and redundancy removal effects
RA, In particular, we represent the network weights for each layer as a linear combination of several low-rank (or rank-1) matrices
RA, We change several classical numerical methods to corresponding pseudo numerical methods and find that pseudo linear multi-step method is the best method in most situations
RA,  We adopt the variational auto-encoder (VAE) framework and use a latent variable to generate diverse conformations
RA, Moreover, we propose to prepend an input transformation module to the network that can partially undo test-time distribution shifts
RA, To handle a variable number of joints across designs, we use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions
RA, We propose the Implicit Bacdoor Adversarial Unlearning (I-BAU) algorithm to solve the minimax
RA, We expand the MLP-like models’ applicability, making them a versatile backbone for dense prediction tasks
RA, In this paper, we tackle this problem from a novel angle---instead of using the original surrogate models, can we obtain a Meta-Surrogate Model (MSM) such that attacks to this model can be easier transferred to other models? We show that this goal can be mathematically formulated as a well-posed (bi-level-like) optimization problem and design a differentiable attacker to make training feasible
RA, We summarize inherent time-conditioned topological properties of the data as time-aware multipersistence Euler-Poincar\'e surface and prove its stability
RA, We utilize our bounds to identify the layers of robustly trained models that contribute the most to a lack of robustness, as well as compare the same layer across different training methods to provide a quantitative comparison of their relative robustness
RA, To illustrate the flexibility of DIVA, we report experiments on sample auto-curation tasks such as outlier rejection, dataset extension, and automatic aggregation of multi-modal data
RA, Our rotation-equivariant representation enables us to estimate local orientations to image keypoints accurately
RA, We call our approach RL based on expected quadratic utility maximization (EQUMRL)
RA, In this setting, we demonstrate that variance mostly arises early in training as a result of poor "outlier" runs, but that weight initialization and initial exploration are not to blame
RA, Taking inspiration from cognitive science, we term representations for reoccurring segments of an agent's experience, "perceptual schemas"
RA, Our upper bound employs a rank-one Nystrom approximation to the kernel function, with the novelty of leading to an online algorithm that optimizes landmark placement
RA,  Moreover, we formulate NCC in an Expectation-Maximization (EM) framework, in which E-step utilizes spherical k-means to estimate the pseudo-labels of instances and distribution of prototypes from the target network and M-step leverages the proposed losses to optimize the online network
RA, We open-source all our code and believe that surrogate NAS benchmarks are an indispensable tool to extend scientifically sound work on NAS to large and exciting  search spaces
RA, Even without any hyper-parameter search, we find that BPNNs consistently outperform the population of optimized NNs in scarce data scenarios, and do so despite being much smaller models
RA, Furthermore, we also show the flexibility of LAM by proposing a per-class loss-adjustment heuristic to achieve a performance profile
RA, We do a grid search for the transform parameters on residual networks
RA, Further, our method reveals potentiallatent meta-states of the underlying evolution process
RA, Then we update the sender model to obtain an approximately optimal scheme using the receiver model
RA, In this way, visual object regions interact with uniform patch tokens and enrich them with contextualized object information
RA, We leverage sparsity in these two directions by exploiting both unstructured and structural sparse patterns in pre-trained language models via magnitude-based pruning and \ell_1 sparse regularization
RA, Meanwhile, we design a cross-modality attention module to align the interactions modeled by the three branches of transformers
RA, We develop a training method for LLF that trains the conditional flow inversely and avoids estimating the labels
RA, We further show that in combination with reinforcement learning, our model can be used to discover control policies that outperform state-of-the-art controllers
RA, This allows to distribute the workload across an arbitrary number of nodes in a cluster, while maintaining the feasibility of second order (covariance) learning on high-dimensional problems
RA, Our proposed model consists of a series of neural networks whose parameters are themselves parametric functions of a time variable
RA, We perceptually visualize the explanations from both stages to provide a visual grounding to introspection
RA, Given this understanding, we then develop new algorithmic tools for improving recursive value estimation with deep models
RA, Moreover, we build the connections between the theory of approximate maximum inner product search and the regret analysis of reinforcement learning
RA, We then develop an algorithmic framework, DataSifter, to detect a variety of and even unknown data issues---a step towards general robustness to bad training data
RA, We explain the abundance of certain cues via their Kolmogorov (descriptional) complexity: solutions corresponding to Kolmogorov-simple cues are abundant in the parameter space and are thus preferred by DNNs
RA, We relate the embedding gap to a relaxation of universally we call the manifold embedding property, capturing the geometric part of universality
RA, To further accelerate the optimization in a time-efficient manner, we propose a novel strategy called Believer-Penalizer (BP), which allows batch experiments to be accelerated asynchronously without affecting performance
RA, Therefore, we propose a new program embedding approach that constructs a program representation based on the assembly code and simultaneously exploits the rich graph structure information present in the program
RA, This allows us to split more edges (layers) for a given budget, resulting in substantially improved performance as NAS search spaces usually include dozens of edges (layers)
RA, This combination effectively strengthens the weak supervision signal from potentially noisy pseudo-labels
RA, We apply SNVI to a neuroscience model of the pyloric network in the crab and demonstrate that it can infer the posterior distribution with one order of magnitude fewer simulations than previously reported
RA, In addition, we introduce FastEnsemble, a fast ensemble method which only requires less than 8% of the full-ensemble training time to generate a new ensemble member
RA,  We experiment with varying sizes of training data to measure the impact of data availability on generation quality for our VAE method as well as several state-of-the-art data generation methods
RA, However, we also highlight that generating scene-consistent predictions goes beyond the mere generation of collision-free trajectories
RA, Specifically, we design an information propagation rule which considers the properties of different frequency signals and exploits the advantages of different frequency signals for better node representation
RA, By integrating these RED principles with image denoising, we propose a new Class-Discriminative Denoising based RED framework, termed CDD-RED
RA, We associate each local feature with a particle that can bidirectionally random walk in a discrete feature space by the affiliations
RA, We show on ImageNet's People Subtree that combining these insights can further reduce the bias amplification of fair learning algorithms by 15% ---a surprising reduction given that the dataset has 196 protected groups but fewer than 10% of the training dataset has protected attribute labels
RA,  We also extend our algorithm to the even more challenging incomplete observation setting, where the demonstration data contains only a censored version of the true state observation
RA, To further improve the label-efficiency of reward learning, we introduce a new data augmentation that temporally crops consecutive subsequences from the original behaviors
RA, In addition to the baseline model, our techniques are also successfully applied to ViT variants
RA, We further propose a compositional lifelong RL method that leverages accumulated neural components to accelerate the learning of future tasks while retaining performance on previous tasks via off-line RL over replayed experiences
RA, Alongside,  we introduce a novel unknown-aware training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data
RA, In this paper, we propose an architecture called Slot2Seq that achieves this text-free DALL\cdotE by learning compositional slot-based representations purely from images, an ability lacking in DALL\cdotE
RA, We train these networks on hundreds of thousands of artificial pathways generated from a pool of purchasable compounds and a list of expert-curated templates
RA, For better pretraining efficiency, we propose a way to assemble multiple MLMs into one unified auxiliary model
RA, Furthermore, we use a modified Adaptive Regularized Cubics approach, which generates a sequence of cubic subproblems that have closed-form solutions
RA, To this end, we propose a novel end-to-end trainable Hindsight framework to extract this contextual information from past traversals and store it in an easy-to-query data structure, which can then be leveraged to aid future 3D object detection of the same scene
RA, We also experiment with the prevalent ResNet image classifiers in computer vision to further validate our idea
RA, Finally, we analyze iterated learning, a procedure in which generations of networks learn from languages generated by earlier learners
RA, We believe our methods can serve for a more reliable evaluation of the robustness of GNNs
RA, We show that our certificates are tight by constructing a worst-case scenario that achieves the bounds derived in our analysis
RA, To gain in- sight into mechanisms underlying the improved performance, we examined several aspects of network representations
RA, We then show that for interpretable models including decision trees, rule lists, generalized linear and additive models, the maximum deviation can be computed exactly and efficiently
RA, We also introduce a novel metric to predict the transferability of an emergent language by translating emergent messages to natural language captions grounded on the same images
RA, This paper suggests several techniques for dealing with visual representation learning in such a future
RA, Moreover, we present model applications beyond posture-identity disentangling, thanks to the latent-space reducing feature of the leveraged VQSN module
RA, To date, the most widely used defense against test-time adversarial attacks is adversarial training, where one incorporates adversarial attacks into the training procedure
RA, Our best model establishes significant improvement on ImageNet over state-of-the-art methods while containing fewer parameters
RA, In addition, we investigate the impact of regularization techniques and each part of the system by performing an ablation study
RA, We also propose several baseline approaches and compare our approach with them in order to demonstrate its efficacy
RA, Thus, our algorithms can be used as a tool to perform an accurate and informed estimation of privacy risk in machine learning models
RA, Importantly, we advocate a unified evaluation protocol based on ImageNet, and report several sample quality scores including FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against reference images for various baselines
RA, Our adversarial attack framework, BaB-Attack, opens up a new opportunity for designing novel adversarial attacks not limited to searching the input space, and enables us to borrow techniques from integer programming theory and neural network verification to build stronger attacks
RA, We further derive regret bounds of both variants and show that the  L1 -regularized variant enjoys a lower bound
RA, We also apply the proposed GNN based reranking method to enhance the passage retrieval results in the retrieving module
RA, Technically, we realize the new method by building on a pre-trained StyleGAN generator as GAN and a pre-trained CLIP model for representing the domain gap
RA, Searching our novel GGDP family with DDSS, we achieve strong results on unconditional image generation on both CIFAR and ImageNet 64x64 (e,g,, 7.59 on CIFAR10 with only 10 inference steps, and 4.67 with 25 steps, compared to 13.62 and 6.56 with the strongest respective DDIM(\eta=0) baselines)
RA, Under this distribution, we define the set of representations such that conditioning on any member, the nuisance and the label remain independent
RA, Third, we introduce a new benchmark for anomaly segmentation by introducing a segmentation benchmark with road anomalies
RA, Our core idea is to leverage prior experience extracted from offline datasets during meta-learning
RA, This further improves downstream tasks: controllable image synthesis, cross-modality image translation and zero-shot synthesis
RA, We conducted extensive experiments on CIFAR100 and ImageNet and achieved significant improvements
RA, First, under the same amount of input image tokens, our method reduces MHSA and FFN computation for efficient inference
RA, We show that POLAR can be seamlessly integrated with existing Taylor model flowpipe construction techniques, and POLAR significantly outperforms the current state-of-the-art techniques on a suite of benchmarks
RA, This approach enables us to train high-performance policies that are robust to visual distractions and can generalize well to unseen environments
RA, We also provide several ablated comparisons which point to substantial improvements arising from the principled method of obtaining data
RA, We also propose and extensively evaluate a novel feature scoring technique based on the angular Mahalanobis distance, and propose a simple and novel technique for feature ensembling during evaluation that enables a big boost in performance at nearly zero run-time cost compared to the standard use of model ensembling or test time augmentations
RA, We further derive an upper bound on the attacker's performance loss due to inaccurate distribution estimation
RA, As CarM is complementary to existing CL methods, we conduct extensive evaluations of our method with seven popular CL methods and show that CarM significantly improves the accuracy of the methods across different settings by large margins in final average accuracy (up to 28.4%) while retaining the same training efficiency
RA, Our method can directly compress a task-specific model while achieving better performance than other compact model strategies requiring expensive model pre-training
RA, Our NSM approach (a) isolates the relational structure from the source domain with high accuracy, and (b) successfully utilizes this structure for analogical reasoning in the target domain
RA, We find that some target tasks are easily predicted irrespective of the source task, and that some other target tasks are more accurately predicted from correlated source tasks than from embeddings trained on the same task
RA, Lastly, we find that changing the training distribution to use back-translated data instead of parallel data, can impact the scaling exponent
RA, We call our proposed method GNN-AK (GNN As Kernel), as the framework resembles a convolutional neural network by replacing the kernel with GNNs
RA, Our method is simple and easy to scale, and with little effort can lead to robust few-shot classifiers
RA, Our work unveils practical insights for applying domain adaptation methods on time series data and builds a solid foundation for future works in the field
RA, We then make use of the neural MI estimates to improve agents' policies: to maximize the MI lower bound associated with superior collaboration to facilitate better collaboration and to minimize the MI upper bound associated with inferior collaboration to avoid falling into local optimal
RA, Moreover, we introduce a multi-modal data augmentation method named SynAugment to utilize all data augmentation approaches tailored to LiDAR-only methods
RA, To this end, we introduce a small number of task-specific adapter parameters that are tuned during self-training while keeping the PLM encoder frozen
RA, The proposed algorithm alternates between the rule generation stage and the rule evaluation stage, and uncovers the most important collection of logic rules within a fixed time limit for both synthetic and real event data
RA, We further introduce a sparsification loss to regularize the number of hyperedges in intermediate layers of a SpaLoc model
RA, Our model automatically infers the latent process which describes the intrinsic uncertainty of the simulator
RA, The proposed method inherits the strong convergence guarantees of the Douglas-Rachford splitting family
RA, We carry out a case study and experiments on the MACO and StarCraft II micromanagement benchmark to demonstrate the dynamics of sparse graph learning, the influence of graph sparseness, and the learning performance of our method
RA, We propose a robust discrimination approach to characterize the impact and further provide a deeper understanding of why diffusion-based generative models are a better choice for proxy distribution than generative adversarial networks
RA, This measure is also able to detect when the representation correlates with combinations of labelled concepts (e,g, "red apple") instead of just individual attributes ("red" and "apple" separately)
RA, To train NSM, we present a self-supervised data collection pipeline that generates pairwise shape assembly data with ground truth by randomly cutting an object mesh into two parts, resulting in a dataset that consists of 19,226 shape assembly pairs with numerous object meshes and diverse cut types
RA, Finally, we combine our insights to train a large Transformer language model with 1.1B shared parameters (approximately 13B before sharing) on a swarm of preemptible T4 GPUs with less than 400Mb/s network throughput
RA, By comparison, our NTL-based ownership verification provides robust resistance to state-of-the-art watermark removal methods, as shown in extensive experiments with 6 removal approaches over the digits, CIFAR10 & STL10, and VisDA datasets
RA, Combined with a volumetric path tracing procedure, our framework is capable of rendering light transport effects including occlusions, specularities, shadows, and indirect illumination, both within individual objects and between different objects
RA, Additionally, we present and test another simple modification to DT called Unsupervised DT (UDT), show its connection to distribution matching, inverse RL and representation learning, and empirically demonstrate their effectiveness for offline imitation learning
RA, In this manner, the generator receives feedback directly based on the value of its samples for model training purposes
RA, In addition, we perform two studies showing that UC-DiffOSI operates well in environments with changing or unknown dynamics
RA, We believe DisTop opens new perspectives by showing that bottom-up skill discovery combined with representation learning can tackle different complex state spaces and reward settings when it is endowed with the ability to explicitly select the skills to improve
RA, We leverage this understanding to propose more targeted forgetting steps, and show that they significantly improve upon existing algorithms
RA, We further show that a guided version of the training process can explicitly govern which summary style is partitioned between decoders, e,g, high abstractiveness vs  low abstractiveness or high specificity vs  low specificity, and also increase the stylistic-difference between individual decoders
RA, We construct a Foveated model using our proposed approach and compare it against a Full-resolution model, which does not contain any pooling
RA, We also derive a new sampling scheme for efficient synthesis from CLD-based diffusion models
RA,  We then amplify these zero-shot translations by using them as few-shot demonstrations for sampling a larger synthetic dataset
RA, We then apply the proposed MDN adaptation method to the problem of end-of-end learning of a communication autoencoder, which jointly learns the encoder, decoder, and a channel networks to minimize the decoding error rate
RA, Furthermore, we discuss how this pruning framework will potentially work with more advanced unsupervised clustering schemes and inductive biases on weights shifting discovered in the future
RA, We use the RPG to build a ResNet18 network with the number of weights equivalent to one convolutional layer of a conventional ResNet and show this model can achieve 67.2% ImageNet top-1 accuracy
RA, We design a principled transportation-constraint risk minimization objective and convert it to a two-player minimax game
RA, We develop a Recurrent Intensity Modeling (RIM) trick to convert sequential ItemRec models to UserRec based on Marked-TPP decomposition
RA, Additionally, we constructed efficient approximations of the logit-space equivalents to the OR and XNOR operators
RA, We propose Multivariate Gaussian mechanism to analyze the privacy guarantee of this transformed gradient perturbation and precisely quantify the level of DP achieved by input perturbation
RA, We compare our method with popular RL/planning approaches targeting similar problems and the ones with environment generators or adversarial agents
RA, CNC also learns better-aligned representations between different groups in each class, reducing the alignment loss substantially compared to prior methods
RA, We also suggest a new contextual sentence encoding architecture to embed a sentence in consideration of the surrounding context
RA, Ultimately, we use translated quantized gestures from the input text as an input to the autoencoder’s decoder to produce gesture sequences
RA, Informed by our ﬁndings, we call out community suggestions for future federated learning works
RA, We show how to achieve an unbiased estimate of the policy gradient for a broad class of CDF-based objectives via sampling, subsequently incorporating variance reduction measures to facilitate effective on-policy learning
RA, Consequently, we propose Edge Pruning and Network Pruning techniques that target both the input graph and the neural networks used to process the graph
RA, Despite being autoregressive only in latent space, we find that the Clockwork VAE outperforms previous LVMs and reduces the gap to deterministic models by using a hierarchy of latent variables
RA, Our CIVD-based workflow enables us to achieve new state-of-the-art results on mini-ImageNet, CUB, and tiered-ImagenNet datasets, with {\sim2%{-5% improvements upon the next best
RA, We first show how the discrete variational method (A) ties into gradient ascent for network weights, and how it (B) uses the decoder network to select binary latent states for training
RA, We call this technique Gradient Maximizing Growth (GradMax) and demonstrate its effectiveness in variety of vision tasks and architectures
RA, We evaluate TIME-LAPSE on both audio and vision tasks using public datasets and further benchmark our approach on a challenging, real-world, electroencephalograms (EEG) dataset for seizure detection
RA, To accomplish this, we require knowledge of some inherent parametric structure of the fine-scale system that makes this type of inverse problem feasible
RA, We also show that our method can learn policies that generalize across different text-based games
RA, In response to this observation, using a simple analogy of pressure distribution in coupled cylinders from thermodynamics, we design novel layerwise sparsity quotas that outperform all existing baselines in the context of random pruning
RA, Our result is a hierarchy of problems they can solve, defined in terms of various hyperparameters such as depth and width
RA, From this understanding, we discover that attention maps can be reused as long as their localization capability is preserved
RA, We specifically show that the well-studied problem of Gaussian process (GP) bandit optimization is a special case of our framework, and that our bounds either improve or are competitive with known regret guarantees for the Mat\'ern kernel
RA, We also provide several standard datasets and evaluation metrics for the broader machine learning community
RA, This removes the need for sequential scene decomposition and enables us to propose an inference algorithm that uses orderless scene decomposition to indirectly estimate an ordered slot posterior
RA, We propose to learn the sensitive-irrelevant input via sampling among features and design an adversarial network to minimize the dependence between the reformulated input and the sensitive information
RA, To mitigate the inconsistencies caused by 2D upsampling, we propose multiple designs including a better upsampler choice and a new regularization loss to enforce 3D consistency
RA, To address this problem, we present Partial Attribute De-correlation (\pad) to disentangle feature representations from sensitive attributes and reduce performance gaps between subgroups in both embedding space and downstream metrics
RA, Towards better labeling, we show that predicted distribution from a classifier, after scaling and interpolation, can provably reduce the implicit label noise under mild assumptions
RA, We also evaluate our model via two derived tasks, i,e, language grammar induction and phrase grounding, and improve over the state-of-the-art for both
RA, Under this assumption of weak learnability, we give an efficient algorithm that is capable of improving the accuracy of such weak learning methods iteratively
RA, We observe that modern offline RL methods trained on suboptimal, noisy data in sparse reward domains outperform cloning the expert data in several practical problems
RA, A distinguishing feature of our method is that all models are trainable on a single desktop GPU (1080 Ti) in under 48h
RA,  Applying our system to CIFAR-10 and CIFAR-100 datasets down-sampled via 8x8 sensor, we found that (i) classification accuracy, which is drastically reduced by this down-sampling, is mostly restored to its 32x32 baseline level when using a moving sensor and recurrent connectivity, (ii) in this setting, neurons in the early layers exhibit a wide repertoire of selectivity patterns, spanning the spatiotemporal selectivity space, with neurons preferring different combinations of spatial and temporal patterning, and (iii) curved sensor's trajectories improve  visual acuity compared to straight trajectories, echoing recent experimental findings involving eye-tracking in challenging conditions
RA, We then systematically characterize several patterns in backfill dynamics and leverage our observations for formulating a novel problem and neural framework, Back2Future, that aims to refines a given model's predictions in real-time
RA, The proposed method also performs effectively against popular defense methods
RA, We find that most standard neural network models have a propensity towards exemplar-based extrapolation and discuss the implications of these findings for research on data augmentation, fairness, and systematic generalization
RA, We also propose a random forest method which learns a locally linear representation of the Reisz function
RA, We develop a method to constrain the RNNs to function similarly to the thalamocortical circuit during motif transitions, while preserving the large expressivity afforded by gradient-based training of non-analytically tractable RNNs
RA, This connection motivates using an entropic regularizer-- a standard tool in optimal transport--- for our problem
RA, We derive an objective for learning the simulator parameters which corresponds to minimizing a divergence between the target offline dataset and the state-action distribution induced by the simulator
RA, We apply the approach to a wide range of text generation tasks, including learning from noisy/negative examples, adversarial attacks, and prompt generation
RA, Second, we use this correlation to boost the performance of the cross-entropy OSR 'baseline' by improving its closed-set accuracy, and with this strong baseline achieve a new state-of-the-art on the most challenging OSR benchmark
RA, Eventually, we propose a general mixup training method called AMix to improve discriminative representations on various scenarios
RA, We compare these models on 3D shapes and molecular datasets, observing improved performance by matching the model's symmetries to the ones of the data
RA, Our methods discover a wide range of samples that are obviously outlier but recognized as in-distribution by the detectors, indicating that current state-of-the-art detectors are not as perfect as they seem on existing benchmarks
RA, In summary, in this paper we present the first RL model to use biomechanical cumulative effort for full-body movement generation without the use of any finite state machines, morphological specification or motion capture data
RA, To achieve this, our calibration applies tools from differential privacy to measure the information revealed by a query
RA, This yields an approach that is both architecture-independent and possesses the benefits of comparing models in weight space
RA, Differently from standard clustering methods, our method can cluster according to image attributes other than the object category
RA, Finally, we propose a local smoothing approach which makes use of adaptive search in order to obtain tight certiﬁcation bounds for reward
RA, We implement this through a robust knowledge transfer protocol between the local models
RA, Third, we investigate several approaches to make MAML permutation-invariant, among which meta-training a single vector to initialize all the N weight vectors in the classification head performs the best
RA, We show the resulting model, Concept-guided Vision Transformer (or RelViT for short) significantly outperforms prior approaches on HICO and GQA by 16% and 13% in the original split, and by 43% and 18% in the systematic split
RA, We evaluate this instruction-tuned model, which we call FLAN, on unseen task types
RA, From this understanding, we ask: can training the model to rely less on these features improve ViT robustness and out-of-distribution performance?  We use the images transformed with our patch-based operations as negatively augmented views and offer losses to regularize the training away from using non-robust features
RA, To provide a comprehensive comparative analysis of all proposed algorithms, we also develop a distributed training system and related evaluation protocol for SSFL
RA, This new problem and its connection to autonomous exploration can be of independent interest
RA, Our proposed method, dubbed as Memory-Constrained Policy Optimization (MCPO), is examined on a diverse suite of environments including robotic locomotion control, navigation with sparse rewards and Atari games, consistently demonstrating competitive performance against recent on-policy constrained policy gradient methods
RA, We also explore how the phenomenon evolves through training, finding that the block structure takes shape early in training, but the underlying representations and the corresponding dominant datapoints continue to change substantially
RA, The proposed method is applied to improve state-of-the-art replay-based methods and achieves superior performance on popular benchmarks
RA, To improve the attack success rate of adversarial examples, we match the adversarial attacks with the directions which effectively decrease the ground truth density
RA, We further apply it to neural and behavioural recordings in non-human primates performing two different reaching tasks, and show that iLQR-VAE yields high-quality kinematic reconstructions from the neural data
RA, Our method is also applicable to non-convex losses, as it does not rely on convexity assumptions to ensure DP guarantees
RA, Our proposed methods, is able to learn decoupled state and reward  features representations
RA, Additionally, we explore the situation in which networks start to ``skip'' layers and how the skipping of layers is expressed
RA, Based on these observations, we propose different metrics to consider when evaluating an algorithm on benchmark tasks
RA, This systematic diffusion essentially curtails the benefits of distilling from a LS-trained teacher, thereby rendering KD at increased temperatures ineffective
RA, We show that our framework leads to methods that strictly generalize their counterparts in binary DRE, as well as new methods that show comparable or superior performance on various downstream tasks
RA, In there, the input convexity of ICGNN allows us to compute the gradient of the lower level problem (i,e, control problem with a given heater allocation) without bias
RA, This includes imposing a block-diagonal structure on the channel mixing weights, adaptively sharing weights across tokens, and sparsifying the frequency modes via soft-thresholding and shrinkage
RA, To achieve this, we designed a new loss function term that weighs these learned relevances and provides an estimated unsupervised error to be used in combination with a reconstruction loss
RA, Moreover, we leverage the self-supervised formulation to introduce a novel teacher forcing-based curriculum (Trajectory Forcing) that improves sample efficiency by progressively increasing the length of the generated trajectory
RA, The proposed algorithm enhances the capabilities  of  both robust feature learning and robust classifier learning  by alternatively taking a gradient descent step for the CE loss and for the AUC loss in a systematic way
RA, Toward this end, we propose a mini-batch Markovian sampled fully decentralized actor-critic algorithm and analyze its finite-time convergence and sample complexity
RA, Based on this observation, we propose a novel federated learning algorithm, coined FedBABU, which only updates the body of the model during federated training (i,e, the head is randomly initialized and never updated), and the head is fine-tuned for personalization during the evaluation process
RA, We show that our approach allows learning generative models that generalize beyond the occlusions present in the input videos, and represent scenes in a modular fashion that allows sampling plausible scenes outside the training distribution by permitting, for instance, object numbers or densities not observed in the training set
RA, Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher
RA, We then discriminatively control the agent’s navigation by a master policy by alternating between navigation policy and various independent interaction policies
RA, We exploit this connection to derive sharpness-impurity-Jacobian relation and to explain how the sharpness influences the learning dynamics and the generalization performance
RA, We also introduce a new large-scale dataset OpenFWI, to establish a more challenging benchmark for the community
RA, To update the network for a new task, we learn a low-rank (or rank-1) matrix and add that to the weights of every layer
RA, According to our experiments, by directly using pre-trained models on Cifar10, CelebA and LSUN, PNDMs can generate higher quality synthetic images with only 50 steps compared with 1000-step DDIMs (20x speedup), significantly outperform DDIMs with 250 steps (by around 0.4 in FID) and have good generalization on different variance schedules
RA, To handle the roto-translation equivariance, we adopt a loss that is invariant to rotation and translation of molecule coordinates, by computing the minimal achievable distance after any rotation and translation
RA, Using policy gradient methods, our approach enables first-order optimization of agent design and control as well as experience sharing across different designs, which improves sample efficiency tremendously
RA, We then construct a supragraph convolution module which simultaneously accounts for the extracted intra- and inter- spatio-temporal dependencies in the data
RA, These findings point towards future design considerations for robust models that arise from our methodology
RA, We propose a dense histogram alignment loss to assign an orientation to keypoints more consistently
RA, We show that one cause for early variance is numerical instability which leads to saturating nonlinearities
RA, We propose Composable Perceptual Schemas (CPS), which learns a composable state representation where perceptual schemas are distributed across multiple, relatively small recurrent "subschema" modules
RA, We get better test metrics and lower generalization gaps when combined with regularization techniques such as label smoothing
RA, We further model object dynamics via a separate ``Object-Dynamics Module'', which captures trajectory interactions, and show how to integrate the two streams
RA, For instance, our DSEE-BERT obtains about 35% inference FLOPs savings with <0.1% trainable parameters and comparable performance to conventional fine-tuning
RA, In this way, the three branches of transformers can support each other to exploit the most discriminative semantic information in different modalities for accurate predictions of captions, especially for the subject, predicate, and object parts in a sentence
RA, We showcase this by implementing a first-of-its-kind self-correcting printer
RA, The new method is shown to outperform the standard backward dynamic programming program currently in use, both in terms of the objective function (total cost of operation over a period) and computational cost
RA, Finally, we ground the proposed machine introspection to human introspection in the application of image quality assessment
RA, In particular, we extract two regularizers that penalize out-of-span top-layer weights and co-linearity in top-layer features respectively
RA, To the best of our knowledge, this is the first work that combines LSH with reinforcement learning that resulting in provable improvements
RA, In our WCST-ML, we observe that the inborn bias of models leans toward simple cues, such as color and ethnicity
RA, We also provide a graphical user interface (GUI) for users with little or no experience with coding, machine learning, or optimization to visualize and guide the experiment design intuitively
RA, Specifically, CrossMatch achieves 17.33 and 21.53 mAP with only 0.5% and 1% labeled data respectively on MS COCO,  outperforming other state-of-the-art methods by \sim3 mAP
RA, We therefore propose a learnable trajectory recombination model that takes as input a set of predicted trajectories for each agent and outputs its consistent reordered recombination
RA, To enhance the consistent output of multi-scale representation, we utilize consistency regularized loss
RA, To estimate the class probability, we propose the features' accessibility that measures the expected number of visits to the support features of that class in a Markov process
RA, We investigate the performance of our proposed method on autoencoders and feed-forward neural network models and compare our approach to state-of-the-art first-order adaptive stochastic methods as well as L-BFGS
RA, Here we find that networks with output modularity successfully converge over generations to a fully systematic `language’ starting from any dataset in our space
RA, We also use the Gini index, a measure of the inequality of a distribution, as a metric for sparsity of the distribution of activities within a given layer
RA, We find that our translation-based metric highly correlates with the downstream performance on modeling natural languages (for instance \rho = 0.83 on Hebrew), while topographic similarity, a popular metric in previous works, shows surprisingly low correlation (\rho = 0.003), hinting that simple properties like attribute disentanglement from synthetic domains might not capture the full complexities of natural language
RA, Using our analysis, we provide new insight into the consequences of adversarial training by quantifying the increase in boundary distance within adversarial subspaces, the redistribution of proximal class labels, and the decrease in boundary curvature
RA, We expect this standardized evaluation protocol to play a critical role in advancing image-to-image translation research
RA, We propose several new regularizers for controlling the domain gap to optimize the weights of the pre-trained StyleGAN generator to output images in domain B instead of domain A
RA, Our method is compatible with any pre-trained DDPM without re-training, only needs to be applied once, and does not finetune the parameters of the pre-trained DDPM
RA, We evaluate NURD on several tasks including chest X-ray classification where, using non-lung patches as the nuisance, NURD produces models that predict pneumonia under strong spurious correlations
RA, Specifically, we propose to (1) extract reusable skills and a skill prior from offline datasets, (2) meta-train a high-level policy that learns to efficiently compose learned skills into long-horizon behaviors, and (3) rapidly adapt the meta-trained policy to solve an unseen target task
RA, Specifically, ATKD trained the best ResNet18 model on ImageNet as we knew (73.0% accuracy)
RA, An example is that we improve the recognition accuracy of DeiT-S by 1% for ImageNet classification at the same computational cost of a vanilla DeiT-S
RA, We also design subgraph sampling strategies which greatly reduce memory footprint and improve speed while maintaining performance
RA, Our method holds the highest entry on the KITTI 3D object detection leaderboard∗, demonstrating the effectiveness of SFD
RA, We present a comprehensive study on six NLU tasks to validate the effectiveness of LiST 
RA, To remedy the information loss in sampled sub-graphs, we propose a novel sampling and label calibration paradigm based on an information-theoretic measure information sufficiency
RA, We design Bayesian active learning algorithms to iteratively query the simulator, gather more data, and continuously improve the model
RA, In addition, we establish a linear convergence rate for our formulation of the OT problem
RA, We use these insights to evaluate a large number of self-supervised representations, ranking them by interpretability, and highlight the differences that emerge compared to the standard evaluation with linear probes
RA, We train NSM on the collected dataset and compare it with several point cloud registration methods and one part assembly baseline approach
RA, Our NTL-based authorization approach instead provides a data-centric protection, which we call applicability authorization, by significantly degrading the performance of the model on unauthorized data
RA, We reduce overfitting in image classification by repeatedly forgetting later layers of the network
RA, Our framework provides new insights into score-based denoising diffusion models and can be readily used for high-resolution image synthesis
RA, This dataset is then distilled by discarding the few-shot demonstrations and then fine-tuning
RA, We propose a method for adapting the autoencoder without modifying the encoder and decoder neural networks, and adapting only the MDN model of the channel
RA, Additionally, such a method allows us to build an arbitrarily complex neural network with any amount of parameters
RA, RIM allows us to rethink recommendation in a Matching (Mtch) scenario, where the benefits of the users (e,g,, ItemRec relevance) and item providers (e,g,, item-exposure guarantees) are considered at the same time
RA, We deployed these new activation functions, both in isolation and in conjunction, and demonstrated their effectiveness on a variety of tasks including image classification, transfer learning, abstract reasoning, and compositional zero-shot learning
RA, To satisfy the overall DP requirement, we add additional gradient perturbation during training and propose Mixed Multivariate Gaussian Analysis to analyze the privacy guarantee provided by the transformed gradient perturbation and additional gradient perturbation
RA, We use the resulting approach to train agents with different “risk profiles” in penalty-based formulations of six OpenAI Safety Gym environments, finding that moderate emphasis on improvement in training scenarios where the agent performs poorly generally improves agent behavior
RA, To allow for a broad comparison to other approaches, the emerging encoding was then evaluated on denoising and inpainting tasks, which are canonically benchmarks for image patch models
RA, We show that TIME-LAPSE is more driven by semantic content compared to other methods, i,e, it is more robust to dataset statistics
RA, To this end, we assume that the underlying fine-scale network is generated by the stochastic block model (SBM) often studied in community detection
RA, Next, we analyze the learning properties of these neural networks, especially focusing on how they can be trained on a small graphs and generalize to larger graphs
RA, To evaluate this idea, we implement the layer-wise attention map reuse on real GPU platforms and achieve up to 1.96 times speedup in inference and 33% savings in training time with noticeably improved ASR performance for the challenging benchmark on LibriSpeech dev/test-other dataset
RA, In light of our analyses, we tailored the training objective accordingly to effectively mitigate the double descent and verified its effectiveness on three benchmark datasets
RA, We show how to use a non-convex variant of the Frank-Wolfe method, coupled with recent advances in gradient boosting that allow incorporating a weak learner with multiplicative approximation guarantee, to overcome the non-convexity and attain global optimality guarantees
RA, Specifically, we show that Back2Future refined top COVID-19 models by 6.65% to 11.24% and yield an 18% improvement over non-trivial baselines
RA, Similarly, we boost the performance of the existing state-of-the-art method by improving its closed-set accuracy, but this does not surpass the strong baseline on the most challenging dataset
RA, Our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen
RA, We evaluate our method on a attribute clustering tasks and demonstrate that our method significantly outperforms methods that do not use language-guidance
RA, We use the proposed RL robustness certiﬁcation framework to evaluate six methods that have previously been shown to yield empirically robust RL, including adversarial training and several forms of regularization, on two representative Atari games
RA, We evaluate prior federated learning algorithms against poisoning attacks, and we show that Cronus is the only secure method that withstands the parameter poisoning attacks
RA, Using this training system, we conduct experiments on a synthetic non-i,i,d, dataset based on CIFAR-10, and an intrinsically non-i,i,d, dataset GLD-23K
RA, We propose Intrinsic Adversarial Attack (IAA), which smooths the activation function and decreases the impact of the later layers of a given normal model, to increase the alignment of adversarial attack and the gradient of joint data distribution
RA, We study this approach on a non-trivial continuous control problems with compositional structure built into the reward functions of various tasks
RA, Finally, we shed light on the question - to smooth or not to smooth a teacher network? - in order to help practitioners make informed decisions
RA, The resulting model is highly parallel with a quasi-linear complexity and has linear memory in the sequence size
RA,  We conduct extensive empirical studies on imbalanced benchmark and medical image datasets, which unanimously verify the effectiveness of the proposed method
RA, We show that the sample complexity of this algorithm is \mathcal{O(N^{2/\epsilon^{2\log(N^{5/\epsilon))
RA, We benchmark on LVIS by holding out all rare categories as novel categories not seen during training
RA, Finally, we infer manipulation actions with the corresponding object masks using the appropriate interaction policy
RA, We also introduce an additional selector vector that assigns different weights to the low-rank matrices learned for the previous tasks
RA, Our method outperforms strong baselines on four public datasets, which shows the effectiveness of our method and the great potential of the direct approach
RA, Our extensive experiments on highway traffic flow, Ethereum token prices, and COVID-19 hospitalizations demonstrate that TAMP-S2GCNets outperforms the state-of-the-art tools in multivariate time series forecasting tasks
RA, We investigate several fixes to this issue and find that one particular method is surprisingly effective and simple -- normalizing penultimate features
RA, Our main technical novelty is an expressive attention function that enables subschemas to dynamically attend to features shared across all positions in the agent's observation
RA, Our approach is very generic and may be used across a wide range of different network architectures and datasets
RA,  We hope that our novel way of combining data structures and iterative algorithm will open the door for further study into the cost reduction in optimization
RA, Our findings emphasize the importance of active human intervention to remove the inborn model biases that may cause negative societal impacts
RA, We relate our method to learning a centrality on an affiliation network and demonstrate its capability to be plugged in existing methods by highlighting centralized local features
RA, We also investigate how the receptive fields (RFs) in the first convolutional layer (where RFs are most interpretable) change with normalization
RA, Our method sets new state-of-the-art performance by large margins for several well-known graph ML tasks; specifically, 0.08 MAE on ZINC, 74.79% and 86.887% accuracy on CIFAR10 and PATTERN respectively
RA, We discuss efficient implementation of the proposed method on GPUs, including how to incorporate stopping criteria without any extra cost
RA, We promote compositionality in the communication between agents in a referential game through partial weight perturbations
RA, By using our method to leverage GPT-3's zero-shot translation capability, we achieve a new state-of-the-art in unsupervised translation on the WMT14 English-French benchmark, attaining a BLEU score of 42.1
RA, For example, we build a ResNet34 with model parameters reduced by more than 400 times, which still achieves 41.6% ImageNet top-1 accuracy
RA, Notably, as RIM is based on sequence models, we extend to Online Matching (OnlnMtch) to serve ItemRec in real-time while satisfying the item guarantees by approximate ratios in the long-term, borrowing techniques from online dual decomposition in convex optimization
RA, We interpret and explore this observation, which leads to improved performance over the widely-used Proximal Policy Optimization algorithm in all environments tested
RA, For image patches, however, we observed advantages of sparse codes that give rise to state-of-the-art performance in ‘zero-shot’ denoising and inpainting benchmarks
RA, We also propose a sequential OOD detection evaluation framework to emulate real-life drift settings and show that TIME-LAPSE outperforms spatial methods significantly
RA, We quantify controllability using the ``average controllability'' metric and bound the difference between the controllability of the fine-scale system and that of the coarse-scale system
RA, Our third contribution is to reappraise the datasets used for OSR evaluation, and construct new benchmarks which better respect the task of detecting semantic novelty, as opposed to low-level distributional shifts as tackled by neighbouring machine learning fields
RA, The performance comparison indicates that representation regularization-based personalization method is able to outperform other variants
RA, We conduct comprehensive transferable attacks against multiple DNNs and show that our IAA can boost the transferability of the crafted attacks in all cases and go beyond state-of-the-art methods
RA, Our hierarchical agent, named HACR (Hierarchical Approach for Compositional Reasoning), generates a human interpretable and short sequence of sub-objectives, leading to efficient interaction with an environment, and achieves the state-of-the-art performance on the challenging ALFRED benchmark
RA, Our method also offers better memory efficiency compared to episodic memory-based approaches
RA, Sparse codes can consequently make VAEs competitive on tasks where they have previously been outperformed by non-generative approaches
RT, We provide theoretical analysis comparing LaGraph to related methods in different domains
RT, The central idea of the proposed method is to encapsulate the relational learning problem with a probabilistic graphical model in which we perform inference to learn about data relationship and other relational processing tasks.
RT, We find empirically that common fine-tuned schedules decay the learning rate after the weight norm bounces
RT, Theoretically, by referring to the literature on infinite-width neural networks, we demonstrate the crucial dependence of the quality of uncertainty on the manner in which ensembling is performed, a phenomenon that arises due to the dynamic programming nature of RL and overlooked by existing offline RL methods Our theoretical predictions are corroborated by pedagogical examples on toy MDPs, as well as empirical comparisons in benchmark continuous control domains
RT, Furthermore, we prove that SelfCon loss guarantees the lower bound of label-conditional mutual information between the intermediate and the last feature
RT, We generally show that incorporating the bias-variance trade-off in the acquisition functions mitigates unnecessary and expensive data labeling
RT, More broadly, our findings support the idea that localized texture summary statistic representations may drive human invariance to adversarial perturbations and that the incorporation of such representations in DNNs could give rise to useful properties like adversarial robustness
RT, Thus, we demonstrate a link between the training objective functions and the above information-theoretic formulation
RT, To deeply understand the inherent reason, a theoretical framework is established, which inspires us that the flatness of the optimized low-bit model on calibration and test data is crucial
RT, In this paper, to further understand our observations, we theoretically show that the poor data separability is one key reason causing this strong tension between under-represented and well-represented classes
RT, Furthermore, we discover that sharing parameters leads to semantic concepts from different modalities being encoded more closely in the embedding space, facilitating the learning of common semantic structures (e,g,, attention patterns) across modalities
RT, Nevertheless, we find that due to the significant quantum errors (noises) on real machines, gradients obtained from naive parameter shift have low fidelity and thus degrade the training accuracy
RT, By theoretical analysis, we argue that global uniformity and local separation are both necessary to the learning quality
RT, In this paper, we propose a theoretical convergence analysis of memory-based continual learning with stochastic gradient descent
RT, Finally, we show that a single generalist Palette model trained on 3 tasks (colorization, inpainting, JPEG decompression) performs as well or better than task-specific specialist counterparts
RT, We also prove that all direct causes of the target can be fully discovered, which further enables us to obtain generalization guarantees in the nonlinear setting
RT, Moreover, we exploit the fact that good candidates of adversarial examples can be easily found via gradient based attacks, and build an adversarial candidates pool to further guide the search in activation space via diving techniques
RT, We show that it is equivalent to the regret commonly used in the zero-order multi-objective bandit setting and overcomes the problem that the latter is hard to optimize via first-order gradient-based methods
RT, In this paper, we present a theoretical analysis on the convergence of IBP training
RT, Second, our solution allows for more degrees of freedom to control the domain gap, i,e, what aspects of image I_B are used to define the domain B
RT, First, we theoretically analyze the example-level SP via the influence function and deduce the influence of each example on the final SP Moreover, to avoid the calculation burden of Hessian for each example, we propose a simple yet effective MetaSP algorithm to simulate the acquisition of example-level SP
RT, This work not only provides a theoretical convergence guarantee but also finds the convergence rate of PipeGCN to be close to that of the vanilla distributed GCN training without staleness
RT, We prove that the representations in this set always perform better than chance, while representations outside of this set may not
RT, Theoretically, the initial centers from HST initialization can achieve lower error than those from another popular initialization method, k-median++, in the non-DP setting Moreover, with privacy constraint, we show that the error of applying DP local search followed by our private HST initialization improves previous results, and approaches the known lower bound within a small factor
RT, We show that ensemble methods can improve adversarial robustness to multiple attacks if the ensemble is adversarially diverse, which is defined by two properties: 1) the sub-models are adversarially robust themselves and yet 2) adversarial attacks do not transfer easily between sub-models
RT, We propose LEAP, a theoretically-grounded architecture that extends Variational Autoencoders (VAEs) by enforcing our conditions through proper constraints in causal process prior
RT, We hence introduce Density Conservative Q-Learning (D-CQL), a batch-RL algorithm with strong theoretical guarantees that carefully penalizes the value function based on the amount of information collected in the state-action space
RT, We analyze the optimization and the generalization performance of the proposed framework for the \ell_2 loss
RT, We present several theoretical results to justify our approach and empirically validate that our random projections can effectively retrieve the underlying signature of a path We show the surprising performance of the proposed random features on several tasks, including (1) mapping the controls of Stochastic Differential Equations to the corresponding solutions and (2) using the random signatures as time series representation for classification tasks
RT, We also prove that PINN and the modified version of DRM can achieve minimax optimal bounds over Sobolev spaces
RT, We further demonstrate a simple yet effective strategy that combines LayerNorm-tuning with general fine-tuning methods to improve their performance and benchmark them on few-shot adaption and distribution shift tasks Finally, we provide an empirical analysis and recommend general recipes for efficient transfer learning of vision and language models
RT,We show that an arbitrary lower bound of the optimal value function can be used to improve the Bellman value target during value learning
RT, We show that despite nonconvexity of the empirical loss, a variant of SGD converges in polynomially many iterations to a good solution that generalizes
RT, Our convergence analysis of COMP-AMS shows that such gradient averaging strategy yields same convergence rate as standard AMSGrad, and also exhibits linear speedip effect w,r,t, the number of local workers
RT, We provide a rigorous theoretical justification for LOCo by inspecting the regret of this dynamic-embedding-based Bayesian optimization algorithm, where the neural network is iteratively retrained with the regularizer
RT, Through the lens of causality, we conclude the universal label space as a confounder to be the causing factor of memorization and frame the two lines of prevailing methods as different deconfounder approaches
RT, Second, by maintaining the same computational cost, our method empowers ViTs to take more image tokens as input for recognition accuracy improvement, where the image tokens are from higher resolution images
RT, Furthermore, to evaluate its objective function, exponential generation paths concerning the number of modalities are required
RT, We qualitatively analyze the goodness of our transfer scheme by showing individual examples of the important features our target network focuses on in different layers compared with the (closest) competitors
RT, We provide theoretical understanding and empirical evidence to justify the flexibility of homophily in this  search task By evaluating the framework on 7 real-world datasets, our experimental results show that AutoSSL can significantly boost the performance on downstream tasks including node clustering and node classification compared with training under individual tasks
RT, We show the utility of our method on two different optimization problems on graphs and point-clouds
RT, We theoretically and empirically show that for graphs exhibiting homophily (low heterophily), impactful structural attacks always lead to increased levels of heterophily, while for graph with heterophily the change in the homophily level depends on the node degrees Our extensive empirical analysis shows that GNNs adopting this design alone can achieve significantly improved empirical and certifiable robustness compared to the best-performing unvaccinated model
RT, We then prove that such a probabilistic graphical model can be mapped to a 1-layer NN for efficient training
RT, We show that Pseudo-KD can be equivalent to an efficient variant of self-distillation techniques, without the need to store the parameters or the output of a trained model
RT, From a theoretical perspective, we rigorously analyze the time and space complexity of graph-based NNS, assuming that an n-element dataset is uniformly distributed within a d-dimensional ball of radius R in the hyperbolic space of curvature -1 From a practical perspective, we illustrate this result on word embedding data: we compare graph-based NNS for GloVe and Poincare GloVe word embeddings Overall, our theoretical and empirical analysis suggests that graph-based NNS can be considered a default approach for similarity search in hyperbolic spaces
RT, We have also confirmed that it can boost 3D point cloud matching performance significantly
RT, Importantly, we provide two main theoretical results: we give bounds on the performance of the transferred policy on a new task, and we give bounds on the necessary and sufficient number of tasks that need to be learned throughout an agent's lifetime to generalise over a distribution
RT, Finally, we show that it is possible to use the noisy constraint across modalities to train self-supervised video models
RT, We provide rigorous theoretical analysis of SPEDE, and demonstrate the practical superior performance over the existing state-of-the-art empirical algorithms on several benchmarks
RT, We further note an overlooked fact that existing DNN initializations were derived to enhance SGD training (e,g,, avoid gradient explosion or collapse), but was unaligned with the challenges of training with SFW We hence also present the first learning-based initialization scheme specifically for boosting SFW-based DNN training
RT, We theoretically analyze tradeoffs showing that Powerset voting requires strong correlations between labels to outperform Binary voting We empirically compare our techniques with DPSGD on large real-world healthcare data and standard multi-label benchmarks
RT, Theoretically, Firth bias reduction removes the first order term O(N^{−1) from the small-sample bias of the Maximum Likelihood Estimator Here we show that the general Firth bias reduction technique simplifies to encouraging uniform class assignment probabilities for multinomial logistic classification, and almost has the same effect in cosine classifiers
RT, We show that if the generative map is ``strongly invertible" (in a sense we suitably formalize), the inferential model need not be much more complex Conversely, we prove that there exist non-invertible generative maps, for which the encoding direction needs to be exponentially larger (under standard assumptions in computational complexity) Importantly, we do not require the generative model to be layerwise invertible, which a lot of the related literature assumes and isn't satisfied by many architectures used in practice (e,g, convolution and pooling based networks) Thus, we provide theoretical support for the empirical wisdom that learning deep generative models is harder when data lies on a low-dimensional manifold
RT, We further prove generalization bounds for learning with this objective
RT,  We provide theoretical analysis on the continuity and the bounds of the expected Q-values using the learned action representations
RT, We first establish that the test loss of encoder-decoder transformer models scales as a power law in the number of training samples, with a dependence on the model size
RT, Theoretically, we show that our framework is strictly more powerful than 1&2-WL, and is not less powerful than 3-WL
RT, Moreover, this kind of attack is semantically robust: our filter composition cannot be distinguished from any other filter composition used extensively every day to enhance images; this raises new security issues and challenges for real-world systems
RT, We first show that special cases of the H-entropy lead to popular acquisition functions used in BO procedures such as knowledge gradient, expected improvement, and entropy search
RT, We prove theoretically and empirically that our technique produces a uniform distribution on the manifold regardless of the training set distribution
RT, We also provide empirical head-to-head comparisons across a variety of synthetic and public real-world benchmarks
RT, Furthermore, we show that the unmasking process can be efficiently implemented (a) without referring to any latent weights or scores; and (b) by only leveraging approximated gradients, so that the whole training algorithm is computationally light
RT, We also provide the results of extensive ablation studies to justify the design choices of our model
RT, This yields some new results: (1) a *global* analysis of the implicit bias valid for \eta^{-2 steps, in contrast to the local analysis of Blanc et al, (2020) that is only valid for \eta^{-16 steps and (2) allowing *arbitrary* noise covariance
RT, Both theoretically and through empirical experiments, we show that a particular form of the DAIR regularizer consistently performs well in a variety of settings
RT, Our empirical study reveals several advantages of the self-supervision pre-trained model when pruned for multiple tasks
RT, We further provide theoretical insights to justify the effectiveness of GraphMVP
RT, Notably, we show that self-attention is closely related to the notion of self-expressiveness in subspace clustering, wherein data points to be clustered are expressed as linear combinations of other points with global coefficients that are adapted to the data and capture long-range interactions among data points We also show that heuristics in sparse self-attention can be studied in a more principled manner using prior literature on sparse coding and sparse subspace clustering We thus conclude that the key innovations of attention mechanisms relative to prior art are the use of many learnable parameters, and multiple heads and layers
RT, Theoretical analysis reveals that CACR generalizes CL's behavior by positive attraction and negative repulsion
RT, We analyze the estimation error and provide a bound on the structural function, providing theoretical support for our proposed method
RT, We show that, under the short-run non-mixing MCMC scenario, the estimation of the energy-based model actually follows the perturbation of maximum likelihood, and both the short-run Langevin flow and normalizing flow form a two-flow generator that we call CoopFlow We provide a theoretical understating of the CoopFlow algorithm and show that it is a valid generator because it converges to a moment matching estimator
RT, We first theoretically characterize the applicability of Q-learning in this setting This theoretical result motivates the design of Latent Action Q-learning or LAQ, an offline RL method that can learn effective value functions from state-only experience
RT, The consequence is superior overall generalization and performance of our system on a wide range of challenging goal-conditioned tasks in comparison to the current state-of-the-art
RT, We theoretically prove that our algorithm can converge to the targeted regions
RT, Theoretically, we provide rigorous theoretical proof and understanding of Tmean(·)
RT, Importantly, our proposed framework is end-to-end permutation equivariant with respect to node ordering
RT, Additionally, we prove it is a non-expansion for any fixed exploration hyperparameter, unlike the softmax policy which requires a state-action specific temperature to obtain a non-expansion (called mellowmax)
RT, We perform theoretical analysis and demonstrate that our approach reduces sample complexity compared with random sampling in high dimension
RT, Therefore, the genuine problem caused by IDN is empirical, instead of underlying, data distribution mismatch during training
RT, Using toy datasets, we empirically validate that ALD can properly obtain samples from target distributions in both conditional and unconditional cases, and ALD converges significantly faster than traditional LD
RT, For example, we show that L_p regression on Vandermonde matrices can be approximately solved using time \mathcal{O(T(\mathbf{A)\log n+(dp)^\omega\cdot\text{polylog\,n), where T(\mathbf{A) is the time to multiply \mathbf{A\mathbf{x for an arbitrary vector \mathbf{x\in\mathbb{R^d, and \omega is the exponent of matrix multiplication
RT, We theoretically consolidate our method by proving that the smaller the variance of payoff functions is, the less likely action selection will change after removing the corresponding edge
RT, We prove that the difference between the robustness of a classifier on the two distributions is upper bounded by the conditional Wasserstein distance between them
RT, We show that such a centralized approach makes CEM vulnerable to local optima and impair its sample efficiency, even in a one-dimensional multi-modal optimization task
RT, We find that Domino accurately identifies 36% of the 1,235 slices in our evaluation framework -- a 12 percentage-point improvement over prior methods
RT, For example, our methods reduce robust generalization gap and overfitting by 34.44% and 4.02%, with comparable robust/standard accuracy boosts and 87.83%/87.82% training/inference FLOPs savings on CIFAR-100 with ResNet-18 Besides, our approaches can be organically combined with existing regularizers, establishing new state-of-the-art results in AT
RT, We show formally that the resulting algorithm maximizes coverage of the underlying state in block MDPs with stochastic observations, providing theoretical backing to our hypothesis that this procedure avoids uncontrollable and stochastic distractions
RT, In particular, for the first time, we show the existence of {left heavy tails and propose a theoretical model that can explain the appearance of these tails
RT, This work provides a theoretical analysis of the inductive biases of self-attention modules, where our focus is to rigorously establish which functions and long-range dependencies self-attention blocks prefer to represent
RT, Finally, we also suggest that supervised classifiers can be used to automatically label large datasets with a rich space of attributes
RT, We conclude by describing and demonstrating an application that requires an image completion model with the capabilities ours exhibits: the use of Bayesian optimal experimental design to guide a sensor
RT, We theoretically and empirically show that our proposed model learns a better filter, thereby improving classification accuracy We further show that our model is scalable by evaluating over large graphs
RT, More importantly, we show, both theoretically and empirically, the advantages of our boostrapping procedures, by which unsupervised approaches rival supervised counterparts
RT,  We further prove that some of the proposed certification methods are theoretically tight and some are NP-Complete problems
RT, From the perspective of information theory, we demonstrate that the proposed method encourages context embedding to be differentiated from a target dataset Plus, we observe that the proposed method maintains performance and captures context embedding under restricted task distributions, where typical NPs suffer from lack of effective tasks to learn context embeddings
RT, However, even if we carefully pick such combinations of models and algorithms, the out-of-distribution performance is still much lower than the in-distribution performance
RT, We show that with high probability over the initialization and training data, a GCN will efficiently learn to detect communities on graphs drawn from a stochastic block model Our proof relies on a fine-grained analysis of the training dynamics in order to overcome the complexity of a non-convex optimization landscape with many poorly-performing local minima
RT, In addition, we observe that KPConv ignores the kernel relationship and treats each kernel point equally when formulating neighbor-kernel correlation via Euclidean distance Furthermore, we capitalize Inverted Residual Bottleneck (IRB) to craft a design space and employ a predictor-based Neural Architecture Search (NAS) approach to automate the design of efficient 3D networks based on MAKPConv
RT, We prove that the latent variable recovers a prognostic score, and the model identifies individualized treatment effects
RT, More specifically, we first analyze the WL algorithm from a geometric point of view and argue that discriminating nodes based on only the consistency of categorical labels do not fully capture important structural information
RT, Our theoretical analysis shows that the learned latent action space can boost the sample-efficiency of downstream imitation learning, effectively reducing the need for large near-optimal expert datasets through the use of auxiliary non-expert data
RT, Theoretical analysis shows that at equilibrium, our method recovers classic domain adaptation when the graph is a clique, and achieves non-trivial alignment for other types of graphs
RT, We further prove that these bounds nearly match the minimal number of parameters any continuous function approximator needs to approximate Korobov functions, showing that neural networks are near-optimal function approximators
RT, We empirically show that SGDA with the same vector norm as Adam reaches similar or even better performance than the latter We then propose a synthetic theoretical framework to understand why nSGDA yields better performance than SGDA for GANs The critical insight in our analysis is that normalizing the gradients forces the discriminator and generator to update at the same pace
RT, Furthermore, we evaluate the different approaches in the GMM setting by modifying VIPS, which has previously only been tested in combination with MORE, and show that the results from the GVA setting are transferable to GMMs, setting a new standard for GMM-based variational inference
RT, First, we find via extensive experiments that off-the-shelf self-supervised representations are already more robust to class imbalance than supervised representations Third, inspired by the theoretical insights, we devise a re-weighted regularization technique that  consistently improves the SSL representation quality on imbalanced datasets with several evaluation criteria, closing the small gap between balanced and imbalanced datasets with the same number of examples
RT, We first show both theoretically and empirically that strong smoothing in AT increases local smoothness of the loss surface which is beneficial for robustness but sacrifices the training loss which influences the accuracy of samples near the decision boundary
RT, We show that this optimization approach leads to performance comparable to the state-of-the-art in task-sequential continual learning across multiple settings, without retaining a memory that scales in size with the number of tasks
RT, More importantly, we introduce a theoretical framing to analyze this loss through a view of how it embeds strata of different sizes We show that our loss maintains distinctions between strata in embedding space, even though it does not explicitly use strata labels
RT, We also explore the theoretical foundations of the problem, proving that, in general, identifying the accuracy is just as hard as identifying the optimal predictor and thus, the efficacy of any method rests upon (perhaps unstated) assumptions on the nature of the shift Finally, analyzing our method on some toy distributions, we provide insights concerning when it works
RT, We show the relation to previous work on the joint optimization model of energy network and feedforward model (INFNET) as we show that it is equivalent to SEAL using margin-based loss if INFNET relaxes their loss function
RT, However, understanding the effects of experimental assumptions is the most crucial part of any evaluation, as the experimental protocol may supply implicit information
RT, Insights from our analysis provide a coherent view on the dynamics of iterative training in neural networks and offer a clear path towards performance improvements
RT, Unsurprisingly, we found that convolution and high training diversity were important contributing factors to OOD generalization of translation to untrained shapes, sizes, time-points and locations, however these weren’t sufficient for rotation and scaling
RT, Furthermore, during the propagation process, we further augment features by randomly dropping node features Our theoretical study and experimental results further support the effectiveness of FAHGNN for mitigating issues of over-smoothing and enhancing the robustness of the model
RT,We empirically show that the test error of deep networks can be estimated by training the same architecture on the same training set but with two different runs of Stochastic Gradient Descent (SGD), and then measuring the disagreement rate between the two networks on unlabeled test data We further theoretically show that this peculiar phenomenon arises from the well-calibrated nature of ensembles of SGD-trained models This finding not only provides a simple empirical measure to directly predict the test error using unlabeled test data, but also establishes a new conceptual connection between generalization and calibration
RT, We show that we can identify informative and diverse subsets of data that lead to deep learning models with similar performance as the ones trained with the original dataset
RT, We prove that stochastic gradient descent applied to augmented subsets found by our approach have similar training dynamics to that of fully augmented data
RT, Based on our knowledge, the existing literature only confirms that pruning is needed and it can be achieved up to certain sparsity We empirically show that in the first regime the pruned lottery ticket sub-network remains a Ramanujan graph
RT, We prove the consistency, generalization, and excessive risk bounds for the proposed objective, and elaborate how they compare to the current results Finally, we carry out extensive real-data and semi-synthetic experiments to demonstrate the advantage of our approach, and launch online testing with a real-world IR system
RT, Finally, to complement our discussions on RIM and Mtch, we provide empirical simulations on real-world datasets with open-source codes
RT, We show that the BPD enables prediction of human behavior and human-AI collaboration equally as well as imitation learning-based human models while using far less data
RT, We extend the theoretical analysis of Allen-Zhu & Li (2020) to show that a trained NN's feature kernel is highly dependent on its parameter initialisation, which biases different initialisations of the same architecture to learn different data attributes in a multi-view data setting We further use our theory to motivate practical considerations for improving student generalisation when using distillation with feature kernels, which allows us to propose a novel approach: Feature Kernel Distillation (FKD)
RT, Moreover, we prove that Mixed Multivariate Gaussian Analysis can work with moments accountant to provide a tight DP estimation
RT, We prove that the gap between worst-group and average loss for each class is upper bounded by the alignment loss for that class
RT, We mathematically guarantee that the predicted complex is always identical regardless of the initial placements of the two structures, avoiding expensive data augmentation
RT, First, we show that an ensemble of the lightweight and full finetuning models achieves the best of both worlds: performance matching the better of full and lightweight finetuning, both ID and OOD Finally, we provide some explanatory theory in a multiclass logistic regression setting with a large number of classes, describing how distillation on ID data can transfer the OOD behavior of one model to another
RT, Finally, we hypothesize that there exist lower-bounds of total number of bits for representing parameters and connections with regard to performance metrics for a given optimization problem
RT, Our algorithm is proved to have O\left(\frac{1{N\epsilon^4\right) iteration complexity for finding an \epsilon-stationary point, where N is the number of machines
RT, Here, we empirically show that these problems share one common cause --- low-quality samples in the dataset
RT, Moreover, we find instances of phase transitions: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward
RT, Moreover, to prevent the compact model from forgetting the knowledge of the source data during knowledge distillation, a collaborative knowledge distillation (Co-KD) method is developed to unify the source data on the server and the target data on the edge device to train the compact model
RT,Conceptually, our approach is more flexible and does not need unrealistic assumptions, e,g,, market clearing, that are commonly used for analytical tractability
RT, Furthermore, we show that practical NFKs exhibit low-rank structures We show that the low-rank approximation of NFKs derived from unsupervised generative models and supervised learning models gives rise to high-quality compact representations of data, achieving competitive results on a variety of machine learning tasks
RT, We show theoretically that, under certain assumptions, having access to a specific set of diverse policies, which we call a set of independent policies, can allow for instantaneously achieving high-level performance on all possible downstream tasks, although these tasks are typically more complex than the ones on which the agent was trained Based on this theoretical analysis, we propose a simple algorithm that iteratively constructs this set of policies In addition to empirically validating our theoretical results, we compare our approach with recently proposed diverse policy set construction methods and show that, while others fail, our approach is able to build a behavior basis that enables instantaneous transfer to all possible downstream tasks We also show empirically that having access to a set of independent policies can better bootstrap the learning process on downstream tasks where the new reward function cannot be described as a linear combination of the features
RT, In order to validate the whole approach, we empirically evaluate our proposition on random and realistic TSP problems against relevant state-of-the-art deep RL methods
RT, Finally, by expanding the scope of AMT, we expose the need for more consistent evaluation metrics and better dataset alignment, and provide a strong baseline for this new direction of multi-task AMT
RT, We evaluate the effectiveness and generalization of the proposed query-based framework on the Karel task and the list processing task
RT, In this work, we first propose a new theoretical framework, with which we analyze the value of federated learning in improving fairness Our analysis reveals that federated learning can strictly boost model fairness compared with all non-federated algorithms We then theoretically and empirically show that the performance tradeoff of FedAvg-based fair learning algorithms is strictly worse than that of a fair classifier trained on centralized data
RT, Our theoretical result reveals the benefits of modeling the relation between two consecutive tasks by learning a globally consistent directional mapping function
RT, We justify the use of this framework with theoretical results and validate it with experiments on benchmark datasets
RT, Under similar assumptions, we show that the generalization error of kernel ridge regression (KRR) has the same asymptotics
RT, To the best of our knowledge, we are the first to study the miscalibration in probabilistic embeddings
RT, We provide theoretical grounding for our method and show experimentally the model's ability to learn the true latent taxonomic structure from data
RT, We empirically show that reconstruction approaches perform better when the style vs  content ratio is low and GCL with popular augmentations benefits from moderate style
RT, However, theoretical justifications on the benefits of large learning rate are highly limited, due to challenges in analysis We prove a convergence theory for constant large learning rate well beyond 2/L, where L is the largest eigenvalue of Hessian at GD initialization Moreover, we rigorously establish an implicit bias of GD induced by such a large learning rate, termed ‘balancing', meaning that magnitudes of X and Y at the limit of GD iterations will be close even if their initialization is significantly unbalanced
RT, We theoretically show that Markov chains evolving with equivariant Markov kernels can induce an invariant distribution by design, and further propose building blocks for the Markov kernels to preserve the desirable equivariance property
RT, To summarize, CIVD provides a mathematically elegant and geometrically interpretable framework that compensates for extreme data insufficiency, prevents overfitting, and enables fast geometric ensemble for thousands of individual Dirichlet Tessellations
RT, We complement our theoretical predictions with simulations showcasing the predictive power of our results
RT, We theoretically prove that PI3NN can calculate PIs for a series of confidence levels without retraining NNs and it completely avoids the crossing issue
RT, Therefore, non-convex gradient flows in fact converge to optimal solutions of a convex optimization problem
RT, We observe empirically that such behavior hurts its overall generalization We validate our hypothesis by estimating the gain on the accuracy when the model has access to an additional modality
RT, On the positive side, we report a somewhat surprising empirical result of "transductive adversarial training": Adversarially retraining the model using fresh randomness at the test time gives a significant increase in robustness against attacks we consider
RT, Our analysis shows that a simple summary of graph statistics, such as weighted degree and eigenvector centrality, over just a few epochs, can be used to accurately predict the performance of NNs
RT, Our analysis supports our previous observation that tuning hyperparameters does indeed leak private information, but we prove that, under certain assumptions, this leakage is modest, as long as each candidate training run needed to select hyperparameters is itself differentially private
RT, Also, we perform a case study showing the impact of the generated adversarial examples, which shows that NODEAttack generated adversarial examples can decrease 50% efficiency of an object-recognition-based mobile application
RT, Theoretically, we present explicit sample complexity bounds to characterize the prediction error on unseen domains in terms of the number of domains with training data and the number of data per domain To our knowledge, this is the first finite-sample guarantee for zero-shot domain adaptation
RT,  Our analysis indicates the necessity of underlying structure to make possible the learning of community-based representations, and to be able to quantify  accurately the controllability of coarsely characterized  networked dynamical systems
RT, Under certain assumptions, we show that SPLID has good theoretical properties of performance improvement and local convergence guarantee
RT, We discovered that we can indeed use the same architecture and pretrained weights of a neural net model to understand both images and point-clouds
RT, We analytically characterize the convergence behavior of LBGM, revealing the nature of the trade-off between communication savings and model performance Additionally, we show that LBGM is a general plug-and-play algorithm that can be used standalone or stacked on top of existing sparsification techniques for distributed model training
RT, These two new objectives come from our theoretical and empirical analysis of the tuple-based metric losses on the hyperspherical embedding space
RT, For example, we show that effective compression of a randomly pruned LeNet-300-100 can be orders of magnitude larger than its direct counterpart, while no discrepancy is ever observed when using SynFlow for pruning (Tanaka et al, 2020) Further, equipped with effective sparsity as a reference frame, we partially reconfirm that random pruning with appropriate sparsity allocation across layers performs as well or better than more sophisticated algorithms for pruning at initialization (Su et al, 2020)
RT, To the best of our knowledge, this is the first algorithm with a sublinear regret guarantee for learning linear mixture SSP In complement to the regret upper bounds, we also prove a lower bound of \Omega(dB_{\star \sqrt{K), which nearly matches our upper bound
RT, In this work, we develop a theoretical analysis that links these two types of losses, exposing their advantages and weaknesses Our theoretical results explain the wide experimental evidence in the medical-imaging literature, whereby Dice losses bring improvements for imbalanced segmentation Based on our theoretical analysis, we propose a principled and simple solution, which enables to control explicitly the label-marginal bias
RT, We show that this highly structured latent space can be directly used for molecular graph generation by the use of a GAN
RT, We further show that IDAA learned on a dataset can be transferred to other datasets
RT, Surprisingly, we find that the contrastive method learns a disentangled representation with only minor modifications
RT, We theoretically analyze the learnability and the generalization error of OSL, and empirically validate its efficacy in both open-ended regression and classification tasks
RT, Our theoretical results are further supported by the empirical results illustrating the optimization and generalization of these models based on gradient-descent training
RT, Based on the above connection, we provide some theoretical analysis and find that layer normalization plays a key role in the over-smoothing issue of Transformer-based models
RT, To the best of our knowledge, Self-GenomeNet is the first self-supervised framework that learns a representation of nucleotide-level genome data, using domain-specific characteristics
RT, As the bound is input-conditioned, it is to our knowledge the first generalization error bound applicable to the problems of detecting out-of-distribution and misclassified in-distribution samples for neural networks; we find that it performs competitively in both cases when tested on image classification tasks
RT, Our theoretical analysis reveals that a common class of learning algorithms, including unconstrained multilayer perceptrons (MLPs), provably fails to learn a quasimetric consistent with training data
RT, We also show that this performance can be further improved by selectively deciding which transitions to share, again without introducing any additional models or classifiers
RT, With this structure, we theoretically show the improved recovery accuracy achievable by PLISA Furthermore,  we analyze the empirical Rademacher complexity of PLISA to characterize its generalization ability to solve new problems outside the training set This paper contains novel theoretical contributions to the area of learning-based algorithms in the sense that  (i) PLISA is generically applicable to a broad class of sparse estimation problems, (ii) generalization analysis has received less attention so far, and (iii) our analysis makes novel connections between the generalization ability and algorithmic properties such as stability and convergence, which leads to a tighter bound that can explain the empirical observations
RT, To explain this behavior, we propose an information-theoretic argument based on a noisy manifold distance oracle, which leaks manifold information through the adversary's gradient estimate
RT, We show that the adversarial bias introduced in training data, via the sampling or labeling processes, can significantly reduce the test accuracy on fair models, compared with regular models
RT, We empirically demonstrate that the unsupervised embeddings by GPCA paired with a 1- or 2-layer MLP achieves similar or even better performance than many sophisticated baselines on semi-supervised node classification tasks across five datasets including Open Graph Benchmark Finally, we capitalize on the discovered relationship to design an effective initialization strategy based on stacking GPCA, enabling GCN to converge faster and achieve robust performance at large number of layers
RT, We show that both N:M sparsity and integer quantization and their combinations can be framed as non-convex constrained optimization problems and solved in a unified manner
RT, To properly evaluate the generalization performance, we propose test set augmentation and train set sifting to emphasize unseen data
RT, We propose a theoretical framework for studying reward learning and the associated optimal experiment design  problem
RT, Our theoretical results suggest that statistical procedures with asymptotic guarantees and sheer (but finite) amounts of compute are not the only things that make sequence modeling work; computability concerns must not be neglected as we consider more expressive model parametrizations
RT, Qualitative and quantitative analysis establishes that our approach successfully learns a smoothly traversable scene-level latent space
RT, We further provide a preliminary theoretical result to support our conjecture
RT, We show that the theory of fair credit assignment provides a unique axiomatic solution that generalizes several existing recommendation and metric explainability techniques in the literature More specifically, we show existing approaches implicitly approximate second-order Shapley-Taylor indices and use this perspective to extend CAM, GradCAM, LIME, SHAP, SBSM, and other methods to search engines Finally, we evaluate these methods and show that these game-theoretic measures yield more consistent explanations for image similarity architectures
RT, To our best knowledge, this is the first method that effectively maintains dynamical isometry during pruning for large-scale deep neural networks
RT, We prove that a smaller sketching dimension of the column space of a tall matrix is possible, assuming the knowledge of the indices of the rows of large leverage scores We show empirically that learned sketches, compared with their "non-learned" counterparts, do improve the approximation accuracy for important problems, including LASSO and matrix estimation with nuclear norm constraints
RT, However, the theoretical construct of a feedforward Spiking Neural Network (SNN) for approximating a temporal sequence remains unclear, making it challenging to optimize SNN architectures for learning complex spatiotemporal patterns In this work, we establish a theoretical framework to understand and improve sequence approximation using a feedforward SNN Moreover, we prove that heterogeneous neurons with varying dynamics and skip-layer connections improve sequence approximation using feedforward SNN
RT, We show that the best-performing type of the output layer  depends on the data distribution drifts and/or the amount of data available  Our analysis and results  shed light on the dynamics of the output layer in continual learning scenarios,  and suggest a way of  selecting the best type of  output layer for a given scenario
RT, In particular, we propose a general theoretical framework indicating factors that can be reformed as a function class regularization process, which could lead to the improvement of domain generalization Our analysis, for the first time, shows that ``robustness" is actually not the causation for domain generalization; rather, robustness induced by adversarial training is a by-product of such function class regularization
RT, Inspired by our observations, we further advance the analyses of double descent to understand robust overfitting better
RT, We show that these activations effectively capture the semantic information from an input image and appear to be excellent per-pixel representations for the segmentation problem
RT, In this work, we first show that the state-of-the-art solutions for such well-separated cases have limited applicability, may suffer from theoretical inconsistencies or lack formal guarantees and therefore perform poorly in the general case We formally justify our method with the scaled-Bregman theorem and show that it does not suffer from the issues that plague the existing solutions
RT, First, we show that the truncation threshold and width of the attraction field dictate the order of the first exit time from the associated local minimum
RT, We prove sample complexity and running time bounds on our method, that are polynomial in the natural parameters of the problem: approximation guarantee, discount factor, distribution mismatch and number of actions
RT, Then, we cast the domain generalization as a rate-distortion problem, and use the information bottleneck penalty to measure how well the domain-free latent image is reconstructed from a compressed representation of a domain-specific image compared to its direct prediction from the domain-specific image itself We prove that the information bottleneck penalty guarantees that domain-invariant features can be learned
RT, We show that the known properties of outlier detection translate to edge detection in images and we give supporting theoretical justifications In addition, we provide a proof of concept of its utility by using a novel magnitude layer to defend against adversarial attacks
RT, We show that the delay impacts in both cases can still be upper bounded by an additive penalty on both the regret and total incentive costs
RT, Moreover, we empirically find that views with similar appearances are trivial for the Siamese model training Remarkably, our method takes a careful consideration of positive pairs for contrastive learning with negligible extra training overhead
RT, Our analysis shows that adversarial examples are neither in high-frequency nor in low-frequency components, but are simply dataset dependent
RT, Second, our protocol is secure against a semi-honest server, as it only reveals sums of the updates
RT, We show that the long-tailed representations are volatile and brittle with respect to the true data distribution
RT, Armed with this measure, we experimentally found the standard no bias decay heuristic~\citep{goyal2017accurate, he2019bag, which recommends the bias parameters and \gamma and \beta in BN layers are left unregularized in training, is a crucial reason for performance degradation in large batch optimization
RT, In our theoretical analysis, we show that the common causes of the heterophily and oversmoothing problems---namely, the relative degree of a node (compared to its neighbors) and its heterophily level---trigger the node representations in consecutive layers to "move" closer to the original decision boundary, which increases the misclassification rate of node labels under certain constraints We theoretically show that: (1) Nodes with high heterophily have a higher misclassification rate Based on our theoretical insights, we propose simple modifications to the GCN architecture (i,e, learned degree corrections and signed messages), and we show that they alleviate the heteorophily and oversmoothing problems with extensive experiments on nine real networks
RT, We show that a single hyperparameter in our method matches the accuracies from Euclidean, Mahalanobis and other forms of distances used for estimating the weights of random variables
RT, Even though our methodology applies to arbitrary functionals, we experimentally find that it beats state of the art performance of the prior neural net based estimator of Shi et al, (2019) for the case of the average treatment effect functional
RT, We then show that this thalamocortical inductive bias not only acts in synergy with gradient-descent RNN training to improve accuracy during in-training-distribution motif production, but also leads to zero-shot transfer to new motif chains with no performance cost
RT, We then prove the existence and uniqueness of an optimal regularized distribution of adversarial examples against a class of classifier (e,g,, a given architecture) that we eventually use to robustly train a classifier Using these theoretical insights, we propose to use Langevin Monte Carlo to sample from this optimal distribution of adversarial examples and train robust classifiers outperforming the standard baseline and providing a speed-up of respectively \times 200 for MNIST and \times8 for CIFAR-10
RT, Moreover, our framework can guarantee the identified model to meet resource specifications of mobile devices and FPGA, and even achieve the real-time execution of DeiT-T on mobile platforms
RT, In this paper, we theoretically derive a bias-free and state/environment-dependent optimal baseline for DR, and analytically show its ability to achieve further variance reduction over the standard constant and state-dependent baselines for DR
RT, We show that our learned common space represents an extensible manifold (where new points not seen during training can be mapped), improves the classification accuracy of stimulus features of unseen timepoints, as well as improves cross-subject translation of fMRI signals
RT, We show both theoretically and empirically that the leave-one-out error is capable of capturing various phenomena in generalization theory, such as double descent, random labels or transfer learning Our work therefore demonstrates that the leave-one-out error provides a tractable way to estimate the generalization ability of wide, deep neural networks, opening the door to potential, new research directions in the field of generalization
RT, Furthermore, we show that our approach can be viewed as a limit of existing notions of alignment by increasing transportation assignment tolerance
RT, Concretely, we jointly optimize for input simplification by reducing inputs' bits per dimension as given by a pretrained generative model, as well as for the classification performance Finally, for dataset condensation, we find that inputs can be simplified with only minimal accuracy degradation
RT, In particular, we provide a theoretical understanding of the use of strong data augmentation for Semi-Supervised Learning (SSL), which can be interesting in its own right
RT, We show theoretically that the algorithm can achieve global convergence for bilinear problems under mildconditions We also empirically show that GDA-AM solves a variety of minimax problems and improves GAN training on several datasets
RT, In this work, we generalize the Wigner-Eckart theorem proposed in (Lang et al,), which characterizes general G-steerable kernel spaces for compact groups G over their homogeneous spaces, to arbitrary G-spaces
RT, We theoretically show that our classification layer optimizes over all possible kernel functions on the space of embeddings to learn an optimal nonlinear classifier
RT, Finally, we show that for quadratic, strongly-convex functions, there are easy-to-construct permutations that lead to accelerated convergence compared to random
RT, To our best knowledge, this is the first time that LTH is demonstrated to be relevant in the context of inverse problems or image priors, and such compact DNN-based priors may potentially contribute to practical efficiency
RT, Furthermore, we provide theoretical guarantees that our method can improve OOD uncertainty estimation while ensuring the convergence performance of the in-distribution environment
RT, We show that, contrary to conventional wisdom, the improved generalization of HNNs is the result of modeling acceleration directly and avoiding artificial complexity from the coordinate system, rather than symplectic structure or energy conservation
RT, We measure their performances theoretically by the expected Hamming error, assuming that the regression coefficients are  iid drawn from a two-point mixture and that the Gram matrix is block-wise diagonal
RT, We provide both empirical and theoretical insights, characterizing and explaining the mechanism by which DICE improves OOD detection
RT, We empirically show that our method, which we call Zest, can be applied to two problems that require measurements of model similarity: detecting model stealing and machine unlearning
RT, This error is found to be already better than chance, corroborating the findings of Valle-Pérez et al, (2019) and underscoring the importance of architecture
RT, We rigorously analyze the convergence of distributed methods  with periodic model averaging for training GNNs and show that naively applying periodic model averaging but ignoring the dependency between nodes will suffer from an irreducible residual error
RT, We provide an insightful proof of the identifiability of the proposed framework
RT, Namely, we prove these challenges arise due to an ill-behaved (more precisely, flat) loss landscape
RT, Furthermore, our formulation generalizes existing continuous optimization based causal discovery methods, providing a unified view of such models
RT, Moreover, we show that their performance is entirely determined by a trade-off between the stable rank of the embedded matrix and the distance preservation between embedded coordinates We further establish that the now ubiquitous Fourier feature mapping of position is a special case that fulfills these conditions  Consequently, we present a more general theory to analyze positional encoding in terms of shifted basis functions To this end, we develop the necessary theoretical formulae and empirically verify that our theoretical claims hold in practice
RT, Second, we find that MAML is sensitive to the class label assignments during meta-testing
RT, To examine the systematic generalization of visual reasoning models, we introduce systematic splits for the standard HICO and GQA benchmarks Our ablation analyses also reveal our model's compatibility with multiple ViT variants and robustness to hyper-parameters
RT, More importantly, our learning bounds are expressed in terms of system quantities, which provide natural instance-dependent characterizations that previous results are short of
RT, This theoretically motivated degree of freedom for “pretraining example design" indicates new training schemes for self-improving representations
RT, To our knowledge, GrID-Net is the first single-cell analysis tool that accounts for the temporal lag between a genomic locus becoming accessible and its downstream effect on a target gene's expression
RT, We show that patch-based negative augmentation consistently improves  robustness of ViTs across a wide set of ImageNet based robustness benchmarks Furthermore, we find our patch-based negative augmentation are complementary to traditional (positive) data augmentation, and together boost the performance further
RT, We further show through an ablation analysis that principled excitation/inhibition and initialization play significant roles in our NCAP architecture
RT, Consequently, we show that the Bellman error is a poor metric for comparing value functions, and therefore, an ineffective objective for off-policy evaluation
RT, We show that the robustness of WAFL is more general than related approaches, and the generalization bound is robust to all adversarial distributions inside the Wasserstein ball (ambiguity set)
RT, We show that this performance drop can be mitigated with (1) the use of large pretrained models; (2) hyperparameters that suit DP optimization; and (3) fine-tuning objectives aligned with the pretraining procedure
RT, Furthermore, we also prove the first lower bound for the autonomous exploration problem In particular, the lower bound implies that one of our proposed algorithms, Value-Aware Autonomous Exploration, is nearly minimax-optimal when the number of L-controllable states grows polynomially with respect to L
RT, We also show that our theory aligns well with existing contrastive methods on both synthetic and real-world datasets
RT, We show that using RIRL yields a rich spectrum of new equilibrium behaviors that differ from those found under rational assumptions
RT, By evaluating our model on predicting the links between millions of biomedical terms in both transductive and inductive settings,  we verified the effectiveness of our proposed model on obtaining higher prediction accuracy than baselines and understanding the reason for a link prediction
RT, Our work provides the first theoretical analysis of the effect message compression has on distributed training over vertically partitioned data We prove convergence of non-convex objectives to a fixed point at a rate of O(\frac{1{\sqrt{T) when the compression error is bounded over the course of training
RT, We show that our physics informed learning method is capable of: (a) solving inverse problems over the physically interpretable parameter space, as well as over the space of Neural Network parameters; (b) learning Lagrangian statistics of turbulence; (c) combining Lagrangian trajectory based, probabilistic, and Eulerian field based loss functions; and (d) extrapolating beyond training sets into more complex regimes of interest
RT, We address this gap by providing a generalization of Cover's Function Counting Theorem that quantifies the number of linearly separable and group-invariant binary dichotomies that can be assigned to equivariant representations of objects Finally, we test our theory on intermediate representations of randomly initialized and fully trained convolutional neural networks and find perfect agreement
RT, We find that this form of pre-training enables generalization in policy learning: for test tasks involving novel goals or environment states, initializing policies with language models improves task completion rates by nearly 20%
RT, Further, it successfully enhanced the \epsilon_\infty = 1/255 robustness of a state-of-the-art model from 26% to 86% while only marginally reducing its natural accuracy from 97.8% to 97.6%
RT, Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks
RT, Crucially, we do not need  any stroke-level or vector supervision; we only use raster images for training
RT, We generalise Shapley value in coalitional game theory to Markov convex game (MCG) and use it as a value factorisation method for MARL We show that the generalised Shapley value possesses several features such as (1) efficiency: the sum of optimal local values is equal to the optimal global value, (2) fairness in factorisation of the global value, and (3) sensitiveness to dummy agents Moreover, we show that MCG with the grand coalition and the generalised Shapley value is within \epsilon-core, which means no agents would deviate from the grand coalition We show the performance of SHAQ on Predator-Prey for modelling relative overgeneralisation and StarCraft Multi-Agent Challenge (SMAC)
RT, Moreover, we prove that the convergence of the proposed federated learning method is robust to a wide range of failures and attacks
RT, We further show that our algorithm has a natural "noise stability" property: If in a bounded region around the current iterate, the public loss satisfies \alpha_v-strong convexity in a direction v, then using noisy gradients instead of the exact gradients shifts our next iterate in the direction v by an amount proportional to 1/\alpha_v (in contrast with DP stochastic gradient descent (DP-SGD)), where the shift is isotropic)
RT, Finally, we exhibit the practical utility of the proposed method by creating parallel datasets for languages beyond the ones explored in prior work, thus expanding the set of programming languages for automated code translation
RT,  Importantly, we conceptualize and formulate information gain (IG) in the adversarial Bayesian learning context and prove, training a BNN with IG bounds the difference between the conventional empirical risk with the risk obtained from adversarial training---our intuition is that information gain from benign and adversarial examples should be the same for a robust BNN
RT, Overall, we provide a comprehensive understanding of sharper generalization bounds of minimax problems
RT, In this paper, we provide a theoretical explanation for this phenomenon: we show that in the nonconvex setting of learning over-parameterized two-layer convolutional neural networks starting from the same random initialization, for a class of data distributions (inspired from image data), Adam and gradient descent (GD) can converge to different global solutions of the training objective with provably different generalization errors, even with weight decay regularization
RT,  We find that this claim is not quite true, and in fact, GCNs can achieve strong performance on heterophilous graphs under certain conditions
RT, We show this systematic diffusion qualitatively by visualizing penultimate layer representations, and quantitatively using our proposed relative distance metric called diffusion index (\eta) Our discovery is comprehensively supported by large-scale experiments and analyses including image classification (standard, fine-grained), neural machine translation and compact student network distillation tasks spanning across multiple datasets and teacher-student architectures
RT, We show that under mild conditions on the kernel and L^2 target regression function the training dynamics have three stages that are based on the behaviors of the models in the two worlds Our theoretical results also mathematically formalize some interesting deep learning phenomena Finally, we give a concrete example comparing the dynamics of two different kernels which shows that faster training is not necessary for better generalization
RT, In this paper, we extend this theoretical result to the much wider class of nonlinear ReLU-activated feedforward networks containing fully-connected layers and skip connections To the best of our knowledge, this is the first time a low-rank phenomenon is proven rigorously for these architectures, and it reflects empirical results in the literature
RT, The theoretical analysis and experimental results conducted in this paper both show its convergence rate of  O (1 / \sqrt {K ) stays the same as Mini-batch SGD
RT, Finally, we investigate the robustness of the proposed solution with respect to the quality of input representations and the number of demonstrations
RT, However, we show that both sharp and flat minima can have a low perturbed loss, implying that SAM does not always prefer flat minima Theoretically, we show the convergence of GSAM and provably better generalization than SAM
RT, We then provide theoretical insight into why it improves convergence rate upon uniform sampling during early learning We show that our method does provide states distributed close to an ideal prioritized sampling distribution estimated by the brute-force method, which does not suffer from the two limitations
RT, We show that the learned state-space trajectories of RNNs trained by these two learning rules under all curricula tested are indistinguishable
RT, We show that this duplicitous behavior extends beyond predictions to feature attributions, which may likewise have negative implications for the intelligibility of a model, and one's ability to find recourse for subjects We prove that that prediction disagreement between selective ensembles is bounded, and empirically demonstrate that selective ensembles achieve consistent predictions and feature attributions while maintaining low abstention rates
RT, We show theoretically and empirically that FSL is robust by design and also significantly communication efficient; all this without compromising clients' privacy
RT, In further analysis we find that instruction modeling is most important for tasks that require complex reasoning, while understandably offering smaller gains in environments that require simple plans
RT, In addition, we show that these data transformations open the door to unforeseen vulnerabilities in the new applied different domain Also, we present simulations with the original and transformed data to show that the data conversion is not always needed and exposes the new domain to unsought menaces
RT, We show that Nash equilibria (NEs) and first-order stationary policies are equivalent in this setting, and give a local convergence rate around strict NEs
RT, We theoretically show the equivalence between policy-matching and state-action-visitation matching, and thus the compatibility of many prior work with our framework
RT, We develop theoretical results to reveal the relationship between optimal early stopping time and model dimension as well as sample size of the dataset for certain linear regression models
RT, We present theoretical results to demonstrate that our approach is competitive with any estimator for the target metric under some general conditions
RT, However, we find that existing works cannot generalize well on non-iid scenarios with different heterogeneity degrees of the underlying data distribution among devices We then prove a theorem referred to as the {\em adaptive trade-off theorem, showing adaptive local adaptation is equivalent to optimizing such information flow based on the information theory With these theoretical insights, we propose a new framework called {\em adaptive federated meta-learning (AFML), designed to achieve generalizable personalized federated learning that maintains solid performance under non-IID data scenarios with different degrees of diversity among devices
RT, Intriguingly, we discover that substituting the gradient in the second moment estimation term with the momentumized version in Adam can well solve the issues
RT, We use our theoretical result to devise an initialisation scheme for DEQs that allows them to solve kGLMs in their forward pass at initialisation We empirically show that this initialisation scheme improves training stability and performance over random initialisation
RT, i)Our framework can easily incorporate many DNN-architectures of RS (u2i), and increase the HitRate and Recall by a large margin iii) We empirically show that our framework can diversify the generated candidates, and ensure fast convergence to better results
RT, We use statistical learning theory and a thorough experimental analysis to show how multiple tasks can interact with each other in a highly non-trivial fashion when trained on a single model
RT, We show that PER-ETD converges to the same desirable fixed point as ETD, but improves the exponential sample complexity of ETD to be polynomials
RT, We empirically show that the morphological information is crucial for modular reinforcement learning, substantially outperforming prior state-of-the-art methods on multi-task learning as well as transfer learning settings with different state and action space dimensions
RT, We theoretically characterize when in-context learning occurs despite the distribution mismatch between prompts and pretraining data
RT, In particular, we show that GD has implicit regularization effects on the Jacobian norm weighted with the impurity of the probability output
RT, This problem is mathematically formulated by a second order partial differential equation (PDE), but is hard to solve
RT, We also find that the model is capable of making use of newly defined functions and theorems during test time
RT, We provide theoretical justification of the sampling quality of PIS in terms of Wasserstein distance when sub-optimal control is used
RT,  We theoretically analyze its convergence and the generalizability of the robustness gained by solving minimax on clean data to unseen test data
RT, Theoretically, we show new existence results for both kernel exponential and deformed exponential families, and that the deformed case has similar approximation capabilities to kernel exponential families
RT, Furthermore, we check the transferability of our method on public image matching benchmarks
RT, In particular, we prove that if we know the exact mechanisms under which the latent properties evolve, then identification can be achieved up to any equivariances that are shared by the underlying mechanisms
RT, We also show that heavy-tailed representation vectors emerge from the learning even though no sparseness prior is used, lending further biological plausibility to the model
RT, We show that surrogate NAS benchmarks can model the true performance of architectures better than tabular benchmarks (at a small fraction of the cost), that they lead to faithful estimates of how well different NAS methods work on the original non-surrogate benchmark, and that they can generate new scientific insight
RT, Inspired by the Helson and Sarason Theorem,  we recover coefficients of a rational function of Blaschke products using a Blaschke Product Neural Network (BPNN), based upon the magnitude observations as input
RT,  We prove that there is a bijective mapping between the original hierarchical cost-sensitive loss and the set of layer-wise abstaining losses under symmetry assumptions
RT, We also show various practical benefits of our method in continual learning and neural architecture search
RT, First, compared to conventional self-supervised representations, the auxiliary-information-infused representations bring the performance closer to the supervised representations, which use direct downstream labels as supervision signals
RT, The efficiency of our algorithm can be further improved by utilizing GNN characteristics
RT, We also illustrate the value of introspective networks in downstream tasks that require generalizability and calibration including active learning and out-of-distribution detection
RT, First, we show that classical updates such as temporal difference (TD) learning or fitted-value-iteration (FVI) converge to different fixed points than residual minimization (RM) in the overparameterized linear case Further, we provide an analysis of the generalization error of these methods, demonstrating per iterate bounds on the value prediction error of FVI, and fixed point bounds for TD and RM
RT, We prove that, with our choice of approximation factor, our Sublinear LSVI algorithms maintain the same regret as the original LSVI algorithms while reducing the runtime complexity to sublinear in the number of actions
RT, We propose a theoretical framework for evaluating the worst-case performance of data selection heuristics
RT, We find that such networks universally approximate a large class of manifolds simultaneously with densities supported on them Among others, our results apply to the well-known coupling and autoregressive flows Our results leverage a new theoretical device called the embedding gap, which measures how far one continuous manifold is from embedding another Our proof also establishes that optimality of a network can be established ``in reverse,''  resolving a conjecture made in Brehmer et al, 2020 and opening the door for simple layer-wise training schemes Finally, we show that the studied networks admit an exact layer-wise projection result, Bayesian uncertainty quantification, and black-box recovery of network weights
RT, To the best of our knowledge, this is the first work to use multi-scale consistency in semi-supervised object detection
RT, We show that both of the approaches enjoy strong theoretical motivations and efficient computation under the MPC setup
RT, Our proposed algorithm, PA-AD, is theoretically optimal and significantly more efficient than prior RL-based works in environments with large state spaces
RT, Specifically, we provide an empirical understanding on the impact of curriculum learning on model calibration under a variety of general contexts
RT, We prove that low-frequency signals can be learned faster in GNNs, i,e, easier to suffer from over-smoothing than high-frequency signals The theoretical study and experimental results further show the effectiveness of MSF-GNNs on relieving the issues of over-smoothing
RT, We also show that MILe is effective for real-world large-scale noisy data such as WebVision
RT, We conduct the first thorough empirical analysis of how existing bias mitigation methods scale to this setting, using large-scale datasets including the ImageNet People Subtree and CelebA
RT,  We provide a theoretical analysis of our algorithm, and provide a lower bound on the performance improvement in each learning episode
RT, We show that the self-attention mechanism inherently amounts to a low-pass filter, which indicates when ViT scales up its depth, excessive low-pass filtering will cause feature maps to only preserve their Direct-Current (DC) component
RT, We provide evidence for the following conclusion: a language model with relatively few parameters, trained for relatively few steps, can perform robustly across language tasks in a manner that demonstrates compositionality, at the cost of GPU-time for language evaluation
RT, Most importantly, we improve the accuracy of DP-SGD on CIFAR10 by 4% for a DP guarantee of \varepsilon=3
RT, Driven by theoretical observations, provision of an auxilary behavior cloning objective as variational regularization to estimates results in accurate value estimation, well-conditioned search spaces and expressive parameterizations
RT, In this paper, we present the first theoretically grounded distributed methods for solving variational inequalities and saddle point problems using compressed communication: MASHA1 and MASHA2 Our theory and methods allow for the use of both unbiased (such as Randk; MASHA1) and contractive (such as Topk; MASHA2) compressors
RT, First, we show that KT applied directly to the target kernel yields a tighter O(\sqrt{\log n/n) integration error bound for each function f in the reproducing kernel Hilbert space Third, we prove KT with a fractional power kernel k_{\alpha for \alpha > 1/2 yields better-than-Monte-Carlo MMD guarantees for non-smooth kernels, like Laplace and \Matern, that do not have square-roots Finally, we illustrate the practical benefits of target KT and KT+ for compression after high-dimensional independent sampling and challenging Markov chain Monte Carlo posterior inference
RT, We show that the flexibility gained through this is Beta loss function produces consistent improvements over cross entropy loss for open set recognition and produces state of the art results relative to recent methods
RT, We find that the resulting representations rival those learned directly from real data, but that good performance requires care in the sampling strategy applied and the training method
RR, Extensive experimental results on CIFAR-10/100 and ImageNet show that Zero-CL outperforms or is on par with state-of-the-art symmetric contrastive learning methods
RR, Our experimental results demonstrate the superiority of LaGraph in performance and the robustness to decreasing of training sample size on both graph-level and node-level tasks
RR, Experimental results demonstrate that the use of meaningful 3D information in SphereNet leads to significant performance improvements in prediction tasks Our results also demonstrate the advantages of SphereNet in terms of capability, efficiency, and scalability
RR, Through extensive experiments in vision, NLP, and RL, we show that if the weight norm does not bounce, we can simplify schedules even further with no loss in performance
RR, We demonstrate that while efficient variants outperform current state-of-the-art, they do not match MSG with deep ensembles
RR, The experimental results demonstrate that our method can significantly alleviate the limitations of ratio matching and perform more effectively in practice
RR, Comprehensive experiments conducted on various datasets show better synthesis image quality and disentangling scores of our model
RR, In our experiments including ImageNet-100, SelfCon surpasses cross-entropy and Supervised Contrastive (SupCon) learning without the need for a multi-viewed batch We demonstrate that the success of SelfCon learning is related to the regularization effect associated with the single-view and sub-network
RR, Across seven simulators, we empirically show that B-QBC outperforms the benchmark functions, whereas QB-MGP is the most robust acquisition function and achieves the best accuracy with the fewest iterations
RR, Although there have been much effort made to improve the efficiency of ZO algorithms, however, we explore  a  new direction, i,e, learn an optimal sampling policy based on reinforcement learning (RL) to generate perturbation  instead of using totally random strategy, which make it possible to calculate a  ZO gradient  with only 2 function evaluations Experimental results with different ZO estimators show that our ZO-RL algorithm can effectively reduce the query complexity of ZO algorithms especially in the later stage of the  optimization process, and converge faster than existing ZO algorithms
RR, We show that our models outperform the state-of-the-art in aforementioned experiments
RR,  Moreover, performance on robust and texture-model images showed similar trends within participants, while performance on non-robust representations changed minimally across the visual field
RR, Our Dynamic Parameterized Networks significantly outperforms state-of-the-art methods in the offline experiments on the public dataset and real-world production dataset, together with an online A/B test
RR, We demonstrate significant accuracy improvements over other competitive methods on two datasets
RR, The experimental results show that InfoSCC-GAN outperforms vanilla EigenGAN in image generation on several popular datasets, yet providing an interpretable latent space Finally, we demonstrate that thanks to the stochastic EigenGAN generator, the proposed framework enjoys a truly stochastic generation in contrast to vanilla deterministic GANs yet with the independent training of an encoder, a classifier, and a generator
RR,  We conduct fine-tuning experiments on two well-benchmarked corpora: LibriSpeech (matching the pre-training data) and AMI (not matching the pre-training data) The results substantiate the efficacy of ATM on significantly improving the recognition performance under mismatched conditions (up to 11.6% relative) while still yielding modest improvements under matched conditions
RR, Further experiments demonstrate that its effectiveness lies in label-correlation utilization rather than document representation This result suggests that they are more related to task information (i,e, the actual labels) than the other tokens
RR, Our experiments demonstrate that our algorithm’s high-confidence fairness guarantees are valid in practice and that our algorithm is an effective tool for training models that are fair when demographic shift occurs
RR, We show IA-MARL outperforms a decentralized approach and even can achieve the performance of MARL without missing training data when sufficient imputation accuracy is supported
RR, Extensive experiments on various tasks including computer vision (image classification, object detection) and natural language processing (text classification and question answering) prove its superiority
RR, We also observe that our model improved the average AUC by 5% in cold-start situations
RR, We demonstrate experimentally on 2D images of roads and 3D image stacks of neural processes that networks trained in this manner are better at recovering the topology of the curvilinear structures they extract
RR, We verify the tightness of our bounds on Gaussian and banana-shaped sources, and demonstrate the scalability of our upper bound algorithm on natural images Our results indicate room for improving the compression performance of state-of-the-art methods by one PSNR at various bitrates
RR,  We evaluate our bounds on estimating the MI of VAEs and GANs trained on the MNIST and CIFAR datasets, and showcase significant gains over existing bounds in these challenging settings with high ground truth MI
RR, Our experiments showed that SynCLR outperformed the SOTA contrastive learning methods with a 17.2% relative reduction of EER in speaker verification tested on an unseen speech corpus, and considerably reduced 50.8% relative FIDs in a challenging speech-to-image translation task given out-of-domain test speeches
RR, Empirically, we study the effectiveness of LISA on nine benchmarks ranging from subpopulation shifts to domain shifts The results indicate that LISA consistently outperforms other state-of-the-art methods with superior invariant representations
RR, Further empirical and human evaluations demonstrate that our model not only makes training more efficient, but also generates more readable and diverse expressions than previous models
RR, Experimental results show that the proposed MS-CLIP outperforms OpenAI CLIP by 13% relatively in zero-shot ImageNet classification (pre-trained on YFCC100M), while simultaneously supporting a reduction of parameters In addition, our approach outperforms OpenAI CLIP by 1.6 points on a collection of 19 downstream vision tasks
RR, We demonstrate that these priors can significantly improve the quality of the recovered triggers, resulting in substantially improved Trojan detection accuracy as validated on both synthetic and publicly available TrojAI benchmarks
RR, The results demonstrate that our on-chip training achieves over 90% and 60% accuracy for 2-class and 4-class image classification tasks
RR, The proposed approach achieves state-of-the-art performance at large epsilon bounds (such as an  ℓ∞  bound of  16/255  on CIFAR-10) while outperforming existing defenses (AWP, TRADES and PGD-AT) at standard perturbation bounds ( 8/255 ) as well
RR, Experimentally, we trained more than 5,000 DNN models with 12 DNN architectures including large models (e,g,, VGG16) and 4 datasets, and estimated corresponding TICs in order to comprehensively study the relationship between the generalization gap and the TIC estimates Experimental results indicate that estimated TIC well correlates generalization gaps under the conditions that are close to NTK regime We further demonstrate that TIC can yield better trial pruning ability for hyperparameter optimization over existing methods
RR, We demonstrate the effectiveness of NetAug on image classification and object detection
RR, Through experiments on image and tabular datasets, we show improvements in the minority group’s performance while preserving overall aggregate accuracy across groups
RR, Experiments show that the proposed algorithm improves the performance of continual learning over existing methods for several image classification tasks
RR, We provide a thorough empirical evaluation of our attack strategies on various machine learning tasks trained on benchmark datasets
RR, Extensive experiments on both synthetic and real-world datasets show that our approach outperforms a variety of baseline methods
RR, In experiments, we can successfully generate adversarial examples for hard input instances where existing strong adversarial attacks fail, and outperform off-the-shelf MIP solver based attacks in both success rates and efficiency Our results further close the gap between the upper bound of robust accuracy obtained by attacks and the lower bound obtained by verification
RR, Extensive experiments demonstrate the effectiveness of the proposed algorithm and verify the theoretical advantage of the  L1 -regularized variant
RR, Extensive experiments on common ODQA benchmark datasets (Natural Question and TriviaQA) demonstrate that KG-FiD can improve vanilla FiD by up to 1.5% on answer exact match score and achieve comparable performance with FiD with only 40% of computation cost
RR, Our results show significant visual improvements over the state of the art as well as multiple applications that highlight improved control
RR, In addition, exhaustive experiments illustrate the generalizability of the discovered curricula across the three datasets and two difficulty scoring functions
RR, In an empirical evaluation, we find that with the right M2B wrapper, HB performs significantly better than other calibration approaches
RR, Empirically, our approach, named ViTGAN, achieves comparable performance to the leading CNN- based GAN models on three datasets: CIFAR-10, CelebA, and LSUN bedroom
RR, Empirical results show that our algorithm significantly outperforms state-of-the-art methods on benchmark LL datasets
RR, The resulting mirror descent adversarial inverse reinforcement learning (MD-AIRL) algorithm gradually advances a parameterized reward function in an associated reward space, and the sequence of such functions provides optimization targets for the policy space We empirically validate our method in discrete and continuous benchmarks and show that MD-AIRL outperforms previous methods in various settings
RR, Importantly, our problem reduces to a min-min, i,e, cooperative, problem and can provide a natural evaluation metric for unsupervised dataset alignment
RR, Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline
RR, Extensive experiments, built on top of two state-of-the-art contrastive learning methods SimCLR and BYOL, show that Contrastive Quant consistently improves the learned visual representation, especially with limited labeled data under semi-supervised scenarios For example, our Contrastive Quant achieves a 8.69% and 10.27% higher accuracy on ResNet-18 and ResNet-34, respectively, on ImageNet when fine-tuning with 10% labeled data
RR, To our best knowledge, this is the first work to demonstrate the effectiveness of pre-trained models in terms of sample efficiency and generalisability enhancement in MARL
RR, Extensive experiments show that PipeGCN can largely boost training throughput (up to 2.2x) while achieving the same accuracy as its vanilla counterpart and outperforming existing full-graph training methods
RR, Experiments demonstrate the effectiveness of SKG in improving the performance of LMs on both concept-to-sentence and concept-to-story datasets while facilitating the model to learn with less effort
RR,   Our empirical results show that, compared to the vanilla training baseline, Fast AdvProp is able to further model performance on a spectrum of visual benchmarks, without incurring additional training cost Additionally, our ablations find FastAdvProp scales better if larger models are used, is compatible with existing data augmentation methods (i,e, Mixup and CutMix), and can be easily adapted to other recognition tasks like object detection
RR, Empirically, experiments are conducted to demonstrate the effectiveness of our methods
RR, We empirically demonstrate the success and practicability of our method on visual classification tasks
RR, We empirically demonstrate that {MAT produces an ensemble of models that is adversarially diverse and significantly improves performance over single models or vanilla ensembles while being comparable to previous state-of-the-art methods
RR, We show that TESSERACT provides robustness against even an adaptive white-box version of the attack
RR, The results show that our method consistently improves the performance of the meta-learning algorithms and demonstrates the effectiveness of Minimax-Meta Regularization
RR, Experiments demonstrate that temporally causal latent processes are reliably identified from observed variables under different dependency structure and our approach considerably outperforms those existing methods without leveraging history or nonstationarity information
RR, The results show that the proposed model is consistently better than the previous models
RR, We conduct extensive experiments in these more realistic settings for out-of-distribution detection and find that a surprisingly simple detector based on the maximum logit outperforms prior methods in all the large-scale multi-class, multi-label, and segmentation tasks, establishing a simple new baseline for future work
RR, The performance of our approach is outlined on many classical benchmark in batch-RL
RR, Experimental results on continuous control tasks in navigation and manipulation demonstrate that the proposed method can efficiently solve long-horizon novel target tasks by combining the strengths of meta-learning and the usage of offline datasets, while prior approaches in RL, meta-RL, and multi-task RL require substantially more environment interactions to solve the tasks
RR, We empirically evaluate our method's convergence and generalization behavior and performed extensive experiments on two benchmark datasets and two medical imaging datasets under two non-IID settings The results show that the proposed personalized FL framework, APPLE, achieves state-of-the-art performance compared to several other personalized FL approaches in the literature
RR, We validate the theoretical results on 90 UCI benchmarking datasets (with average data size N \approx 1000) and show that each agent needs to share a total of 200N/M bits (resp. 3N/M real values) to closely match the performance of the centralized algorithms, and these numbers are independent of parameter and feature dimensions
RR, Qualitative and quantitative experiments show improvement in C-Dis-RL and latent convexity by CIR More experiments demonstrate CIR can also improve other downstream tasks, such as new attribute value mining, data augmentation, and eliminating bias for fairness
RR, Extensive experiments and ablation studies on 6 GCN model and dataset pairs consistently validate that the proposed D^2-GCN can (1) largely squeeze out unnecessary costs from both the aggregation and combination phases (e,g,, reduce the inference FLOPs by \downarrow1.1\times \sim \downarrow37.0\times and shrink the energy cost of GCN inference by \downarrow1.6\times \sim \downarrow8.4\times), while offering a comparable or an even better accuracy (e,g,, \downarrow 0.5% \sim \uparrow 5.6%); and (2) help GCNs to go deeper by boosting their trainability (e,g,, providing a \uparrow 0.8% \sim \uparrow 5.1% higher accuracy when increasing the model depth from 4 layers to 64 layers) and thus achieving a comparable or even better accuracy of GCNs with more layers over SOTA techniques (e,g,, a \downarrow0.4% \sim \uparrow38.6% higher accuracy for models with 64 layers)
RR,  We show that in most cases, simply lower bounding with the discounted episodic return performs at least as well as common baselines such as TD3, SAC and Hindsight Experience Replay (HER)
RR, Our experimental results demonstrate that GEKO outperforms baselines on all 12 targets with and without prior drug-target measurement data
RR, To demonstrate the effectiveness of EISL, we conduct experiments on a wide range of tasks, including machine translation with noisy target sequences, unsupervised text style transfer with only weak training signals, and non-autoregressive generation with non-predefined generation order Experimental results show our method significantly outperforms the common cross-entropy loss and other strong baselines on all the tasks
RR, In particular, our generalization bounds are adaptive: they automatically optimize over a family of kernels that includes the Neural Tangent Kernel, to provide the tightest bound
RR, Numerical experiments are conducted to justify the theoretical findings, and demonstrate that the proposed method can achieve same test accuracy as full-gradient AMSGrad with substantial communication savings
RR, Our empirical results further demonstrate the effectiveness of LOCo on several synthetic and real-world benchmark Bayesian optimization tasks
RR, Extensive experiments have demonstrated the effectiveness of the proposed framework in condensing different graph datasets into informative smaller graphs In particular, we are able to approximate the original test accuracy by 95.3% on Reddit, 99.8% on Flickr and 99.0% on Citeseer,  while reducing their graph size by more than 99.9%, and the condensed graphs can be used to train various GNN architectures
RR, Our method enjoys high flexibility, and we test it in two scenarios: traditional image generation (on top of StyleGAN2) and continuous image generation (on top of INR-GAN) To the best of our knowledge, our work is the first one which explores text-controllable continuous image generation
RR, Theoretical guarantees and experimental results demonstrate the effectiveness of the privacy protection without appreciable hindering on the model performance
RR, Numerical results show that the proposed paradigm can achieve the same accuracy while reducing the number of communication rounds by an order of magnitude compared to federated averaging
RR, We demonstrate that our method performs better than existing scalable multimodal VAEs in inference and generation
RR, We demonstrate our method on various unsupervised AD tasks with image and tabular data
RR, We see upwards of 5% accuracy improvements compared with the state-of-the-art knowledge transfer methods on four benchmark (target) image datasets CUB200, Stanford Dogs, MIT67 and Stanford40 where the source dataset is ImageNet
RR, Extensive experimental studies on four levels of complex text-based games have demonstrated the superiority of the proposed method compared to the state-of-the-art
RR, We demonstrate the advantage of our approach on the challenging high-dimensional continuous control domains including Ant and Humanoid, where the RFN leads to faster convergence and higher performance
RR, Our experiments in simulated settings and on three diverse NLP datasets show that our method improves over strong seq2seq baselines by about 9% on absolute F1 score
RR, We show empirically that our method reliably achieves higher accuracy than other comparable local (biologically plausible) learning methods on MNIST, CIFAR10 and ImageNet
RR,Empirical studies on the robustness of graph neural networks (GNNs) have suggested a relation between the vulnerabilities of GNNs to adversarial attacks and the increased presence of heterophily in perturbed graphs (where edges tend to connect nodes with dissimilar features and labels)
RR, We demonstrate that our approach can achieve SOTA performance on diverse visual control tasks on the DeepMind Control Suite when the background is replaced with natural videos In addition, we show that our approach outperforms well-established baselines for generalization to unseen environments on the Procgen benchmark
RR, We experimentally found that directly using the feature vector extracted using standard pre-trained models to construct a prototype classifier in meta-testing does not perform as well as the prototypical network and linear classifiers with fine-tuning and feature vectors of pre-trained models
RR, Empirically, we present three use cases where controlling orthogonal variation is important: style transfer, domain adaptation, and fairness
RR, Finally, we conduct experiments on multiple image classification (CIFAR-10 and CIFAR-100) and natural language understanding datasets (the GLUE benchmark) across various neural network architectures and demonstrate that our method is competitive against strong baselines
RR, We provide experiments on challenging exploration problems which show that our approach works empirically
RR, Experimental results on tabular, image and text data show that Kernel SGD converges up to 30 times faster than the existing second-order optimization techniques, and also shows remarkable performance in generalization
RR, Our findings suggest that an explicit spatial memory and a semantic search policy can provide a stronger and more general representation for state-tracking and guidance, even in the absence of expert trajectories or low-level instructions
RR, We show in experiments that our method, which introduces negligible extra computational cost, achieves tests accuracies with vanilla deep networks that are competitive with ResNets (of the same width/depth), and significantly higher than those obtained with the Edge of Chaos (EOC) method
RR, Our experiments show that for most of the tasks, when trained jointly for multi-tasks, VUT has achieved accuracy either on par with or exceeding the accuracy when the model is trained for individual tasks separately
RR, We also demonstrate that graph-based methods outperform other existing baselines on Poincare GloVe word embeddings
RR, Experimental results show that WeaveNet approximates two strongly NP-hard variants of stable matching in a comparative performance with the gold standard hand-crafted algorithms under the limited size of problem instances
RR, Through extensive experiments, it is shown that HYPOCRITE is more effective than other baseline in terms of both attack success rate and perturbed ratio
RR, Through the experiments, we show that our memory architecture can achieve competitive results compared to state-of-the-art transformer-based methods while maintaining constant memory capacity independent of sequence length
RR, We show that the proposed modifications can be effectively applied to most existing self-supervised learning objectives with large gains in performance
RR, Experimental results on real-world datasets demonstrate that the proposed attack framework achieves strong performance even if the server deploys advanced defense mechanisms
RR, We verify our approach in a series of experiments, where we perform transfer learning both after learning a set of base tasks, and after learning an arbitrary set of tasks We also demonstrate that as a side effect of our transfer learning approach, an agent can produce an interpretable Boolean expression of its understanding of the current task Finally, we demonstrate our approach in the full lifelong setting where an agent receives tasks from an unknown distribution and, starting from zero skills, is able to quickly generalise over the task distribution after learning only a few tasks---which are sub-logarithmic in the size of the task space
RR, Extensive numerical experiments on multi-agent Mujoco tasks verify the effectiveness of our proposed approach
RR, Further, we demonstrate that our method is relatively robust to label noise
RR, Our experimental evaluation shows the effectiveness of the method: it successfully finds large provably robust regions including ones containing \approx 10^{573 adversarial examples for pixel intensity and \approx 10^{599 for geometric transformations We verify empirically that our robust adversarial examples are significantly more effective against state-of-the-art defenses based on randomized smoothing than the individual attacks used to construct the regions
RR, Experiments on CIFAR-10 and Tiny-ImageNet datasets demonstrate that our new framework named SFW-pruning consistently achieves the state-of-the-art performance on various benchmark DNNs over a wide range of pruning ratios
RR, We find that our techniques outperform all others in the centralized setting
RR, Experiments show that using positive and negative review datasets from different categories, we can successfully transfer the sentiment without changing the category
RR, Moreover, the evaluation of compressing an already compact model shows our method can further reduce 9% to 30% parameters with an insignificant impact on task accuracy
RR, Experiments in stochastic environments show that a dense estimation for quantile functions enhances distributional RL in terms of faster empirical convergence and higher rewards in most casesThis result is surprising because GANs have well-known limitations (e,g, mode collapse) and are known to not learn the data distribution accurately
RR, Several additional experiments are presented to explore reasons why GANs do well at this task
RR, We demonstrate that the method can work in a semi-supervised scenario, and that a portion of the unsupervised data can belong to a different domain entirely We outperform competing baselines on the challenging problem of synthetic-to-real object pose transfer
RR, Experimental results verify the effectiveness of our proposed method on four multivariate time series forecasting benchmark datasets
RR, In this work, we demonstrate the effectiveness of Firth bias reduction in few-shot classification Finally, we show the robustness of Firth bias reduction, in the case of imbalanced data distribution
RR, We show the prominence of iBOT by achieving a 79.5% linear probing accuracy and an 83.8% fine-tuning accuracy with ViT-B/16 on ImageNet-1K
RR, Experiments on real-world datasets demonstrate the effectiveness of our framework in achieving both accuracy and fairness
RR, Experiments show that this approach contributes to resolve the imbalance between these corpora, and can tailor previous pre-trained NLMs to generate coherent and semantically valid text reflecting a given small fine-tuning corpus
RR, Extensive experiments on challenging face recognition benchmarks demonstrate the advantages of our method in jointly handling imbalances due to multiple variations
RR, We further provide a number of experiments to demonstrate how the proposed evaluation can be employed in empirical studies of basic questions -- for example, whether the effectiveness of existing self-supervised learning methods is skewed towards image classification versus other tasks, such as dense pixel-wise predictions
RR, We demonstrate the efficacy of the proposed framework through an extensive empirical study on a collection of simulated and real datasets
RR, Experimental results show that our methods significantly improve the performance of two typical offline RL methods in environments with large and discrete action spaces
RR,  We also provide empirical results that demonstrate the effectiveness of our proposed algorithm with respect to the prior art in order to learn robust models from very large datasets
RR,  Experimental results demonstrate that the method is also effective against ensemble-adversarially trained models and it has a low cost in terms of queries to the victim model
RR, The experiments show that HACO achieves a substantially high sample efficiency in the safe driving benchmark
RR, We evaluated our novel architecture on several synthetic and real-world datasets, evaluated by likelihood and prediction accuracy
RR, In an extensive experimental study, we compare its behavior with respective state-of-the-art techniques from SSL, SSDA, and UDA and find that AdaMatch either matches or significantly exceeds the state-of-the-art in each case using the same hyper-parameters regardless of the dataset or task
RR, We argue that the unmasking process plays an important role in enlarging the capacity of the subnetworks and thus grants two major benefits: (i) the disguised subnetworks easily outperform the hidden counterparts; (ii) the unmasking process helps to relax the quality requirement on the sparse subnetwork mask so that the expensive edge-popup algorithm can be replaced with more efficient alternatives Extensive experiments with several large-scale models (ResNet-18, ResNet-50, and WideResNet-28), on CIFAR-10/100 datasets, demonstrate the competency of PaB over edge-popup and other counterparts
RR, Experiment results demonstrate that our algorithm excels in discovering optimal programmatic policies that are highly interpretable
RR,  We demonstrate the effectiveness of these distances in non-trivial continuous control domains
RR, Empirically, we demonstrate the benefits of our approach on both image classification and natural language processing benchmarks
RR, These achieve good empirical performance on standard vision datasets, while providing a precise description of their functional space that yields new insights on their inductive bias
RR, Our experimental results on large-scale benchmarks and real images demonstrate the effectiveness of the reblurring losses in improving the perceptual quality of the deblurred images in terms of NIQE and LPIPS scores as well as visual sharpness
RR, In experiments, our method achieves the new state of the art on the SPair-71k dataset, while performing on par with existing state-of-the-art models on the PF-PASCAL and PF-WILLOW datasets, showing the effectiveness of the proposed approach
RR, Our experiments on three incremental sequence classification benchmarks show the generalizability of the proposed methods over different pre-trained language models
RR, In this way, we achieve a scalable, time-efficient, and diverse distributed MARL learning framework with high system throughput
RR, Experiments show PINO outperforms previous ML methods on many popular PDE families while retaining the extraordinary speed-up of FNO compared to solvers
RR, We demonstrate the efficacy of HNI with image classification tasks and 3 different types of interactions: (1) Explaining the reasoning logic of NNs so humans can intuitively identify and locate errors of NN; (2) human users can correct the errors and improve NN’s performance by modifying the c-SCG and distilling the knowledge back to the original NN; (3) human users can intuitively guide NN and provide a new solution for zero-shot learning
RR, We illustrate the efficacy of AdaRL through a series of experiments that vary factors in the observation, transition and reward functions for Cartpole and Atari games
RR, We compare our technique against state-of-the-art baselines and demonstrate competitive empirical results quantitatively and qualitatively
RR, Our results show that self-learning consistently increases performance under distribution shifts, irrespective of the model architecture, the pre-training technique or the type of distribution shift We present state-of-the art adaptation results on CIFAR10-C (8.5% error),  ImageNet-C (22.0% mCE), ImageNet-R (17.4% error) and ImageNet-A (14.8% error), theoretically study the dynamics of self-supervised adaptation methods and propose a new classification dataset (ImageNet-D) which is challenging even with adaptation
RR, Our experiments show that DAIR consistently outperforms ERM and DA-ERM with little marginal cost
RR, We conduct extensive experiments on several datasets and show that the robustness of RVFR outperforms different baselines against diverse types of attacks
RR, Experimental results confirm the effectiveness of the proposed MUSML algorithm
RR, In experiments, we demonstrate the advantages of MVP by extensive empirical studies and comparisons with popular pruning methods
RR, Experiments show that our proposed switch-GLAT outperform the multilingual Transformer with as much as 1.16 BLEU improvement and 6.6x faster decoding speed in inference
RR, Our evaluation includes adaptations of state-of-the-art visual domain adaptation methods to time series data in addition to recent methods specifically developed for time series data We conduct extensive experiments to evaluate 10 state-of-the-art methods on 3 representative datasets spanning 15 cross-domain scenarios Our results suggest that with careful selection of hyper-parameters, visual domain adaptation methods are competitive with methods proposed for time series domain adaptation
RR, Finally, comprehensive experiments show that GraphMVP can consistently outperform existing graph SSL methods
RR, Experiments on different tasks, including neural machine translation (4 IWSLT14 translation tasks, multilingual translation task, and WMT16 Romanian\toEnglish translation), natural language understand (GLUE benchmark), and image classification (CIFAR-100 dataset), well demonstrate the superiority of DM-CT by obtaining significant and consistent performance improvements
RR, With a linear reward function, we demonstrate that our algorithm achieves a near-optimal regret We prove that this algorithm also achieves a near-optimal regret Finally, we provide an empirical evaluation of the algorithms on both synthetic functions and various benchmark datasets Our experiments show that our algorithms work well and consistently outperform existing approaches
RR, We demonstrate that these models can be used to parameterize predictive conditional distributions and improve the quality of probabilistic forecasts on tasks ranging from time series forecasting to object detection
RR, Our learned graph grammar yields state-of-the-art results on generating high quality molecules for three monomer datasets that contain only {\sim20 samples each Our approach also achieves remarkable performance in a challenging polymer generation task with only 117 training samples and is competitive against existing methods using 81k data points
RR, Experiments on three popular datasets show CUMA achieves superior fairness in robustness against distribution shifts, without more sacrifice on either overall accuracies or the in-distribution fairness
RR, Extensive large-scale experiments on standard vision tasks show that CACR not only consistently outperforms existing CL methods on benchmark datasets in representation learning, but also shows better robustness when generalized to pretrain on wild large image datasets
RR, Experiments on a wide range of MARL benchmarks show the superior performance of PMIC compared with other MARL algorithms
RR, In experiments, we validate the efficacy of our method on SSL variants of CIFAR10-LT, CIFAR100-LT, and ImageNet-127
RR, Further, our model attains strong performance on a subset of tasks from the BIG-Bench benchmark, out-performing models 6× its size
RR, Finally, we empirically evaluate our method on several well-known DG benchmarks, where it achieves state-of-the-art results
RR, Extensive comparisons of our approach to state-of-the-art SSND methods on standard image data sets (SVHN/CIFAR-10/CIFAR-100) and medical image data sets reveal significant gains with negligible increase in computational cost
RR, We demonstrate that the trained CoopFlow is capable of synthesizing realistic images, reconstructing images, and interpolating between images
RR, Our experiments in 5 environments ranging from 2D grid world to 3D visual navigation in realistic environments demonstrate the benefits of LAQ over simpler alternatives, imitation learning oracles, and competing methods
RR, This also significantly reduces the overall model footprint across several tasks that can now share a common PLM encoder as backbone for inference The results show that LiST improves by 35% over classic fine-tuning and 6% over prompt-tuning with 96% reduction in the number of trainable parameters when fine-tuned with only 30 labeled examples from the target domain
RR, Our experiments show that the proposed approach consistently improves existing methods, obtains good out-of-distribution generalization and achieves new state-of-the-art results on widely used environments
RR, Our method achieves superior log-likelihood on the test set and outperforms previous baselines in designing antibodies capable of neutralizing the SARS-CoV-2 virus
RR, Experiments are conducted with several DeiT backbones on the ImageNet dataset, and our approach consistently outperforms recent competitors
RR, We demonstrate our method on various tasks on different manifolds: 1) learning panoramas on the sphere, 2) learning probability distributions on the rotation manifold, 3) learning neural radiance fields on the product of cube and sphere, and 4) learning light fields represented as the product of spheres
RR, Our SpaLoc shows superior accuracy and efficiency on synthetic datasets compared with prior art and achieves state-of-the-art performance on several real-world knowledge graph reasoning benchmarks
RR, Experiments on text analysis demonstrate that the proposed method, which is amenable to mini-batch stochastic gradient descent based optimization and hence scalable to big corpora, provides competitive performance in discovering more coherent and diverse topics and extracting better document representations
RR, In experiments, we show that our simple and efficient approach is surprisingly competitive against more computationally expensive strong baselines, and even achieving new state-of-the-art performances in several cases
RR, Surprisingly, we find that contrastive pre-training learns features that are very far apart between the source and target domains
RR, Our experimental evaluation on challenging real-world image data demonstrates that our method increases certified individual fairness by more than 60%, without significantly affecting task utility
RR, We empirically validate that resmax is comparable to or outperforms \varepsilon-greedy and softmax across a variety of environments in tabular and deep RL
RR, Empirically, we demonstrate our framework can faithfully imitate the behavior of a complex infectious disease simulator with a small number of examples, enabling rapid simulation and scenario exploration
RR, Empirical evaluations on synthetic and real-world datasets demonstrate that our method is superior to the state-of-the-art approaches, and achieves more stable training for instance-dependent label noise
RR, Our experiments show that the proposed method outperforms the state-of-the-art anomaly detection and anomaly segmentation methods for the MVTec AD dataset
RR, Our experiments show that D^2ULO outperforms the existing state-of-the-art AL strategies equipped with domain adaptation over various domain shift settings (e,g,, real-to-real data and synthetic-to-real data)
RR, Substantial experiments demonstrate a superior performance of our method, both in terms of computation times and robustness, compared to the state-of-the art
RR, We experimentally show that the repelling parasiamese network achieves state-of-the-art results on this task
RR, Broadly, our results demonstrate that automatically extracting foreground-background structure from pretrained deep generative models can serve as a remarkably effective substitute for human supervision
RR, We also improve certified robust accuracy by 7.6% on the CIFAR-10 dataset We further demonstrate that different generative models brings a disparate improvement in the performance in robust training
RR, Extensive experiments validate our proposals with multiple network architectures on diverse datasets, including CIFAR-10/100 and Tiny-ImageNet
RR, Our experiments demonstrate that our tessellated networks provides a more robust defence mechanism against gradient-based adversarial attacks in comparison to conventional deep neural models
RR, We validate our novel architecture through extensive experiments on image classification and object detection tasks with different backbones Our experiments show consistent improvements in performances against their counterparts We also conduct experiments that probe the robustness of the learned representations
RR, Our results show that EEGNet is significantly vulnerable to adversarial attacks with an attack success rate of more than 50%
RR, Our experiments further demonstrate that Adversarial Surprise leads to the emergence of complex and meaningful skills, and outperforms state-of-the-art unsupervised reinforcement learning methods in terms of both exploration and zero-shot transfer to downstream tasks
RR, The experiments show the critical role of the policy network as a powerful heuristic guiding  A*, which can lead to  left tails with polynomial scaling by avoiding exploring exponential size sub-trees early on in the search Our results also demonstrate the importance of random restart strategies, as are widely used in traditional combinatorial solvers, for deep reinforcement learning and deep AI planning systems to avoid left and right heavy tails
RR, We demonstrate our approach on the image completion task, and show that it outperforms state-of-the-art GAN-based approaches at faithfully representing the inherent uncertainty
RR, Experiments spanning several text generation tasks demonstrate that adding imagination with our ImaginE displays great potential in introducing multi-modal information into NLG evaluation, and improves existing automatic metrics’ correlations with human similarity judgments in many circumstances
RR, Extensive experiments on image restoration tasks including single image super-resolution, image denoising, image demosaicing, and image compression artifacts reduction validate the effectiveness and efﬁciency of ACLA
RR, We experimentally verify the above two advantages on various graph deep learning benchmark tasks, showing a significant improvement over many existing graph neural networks
RR, Extensive experimental results and ablation studies under various settings demonstrate the effectiveness of the proposed algorithm
RR, We evaluate our method on predicting trajectories of simulated Newton mechanics systems with both full and partially observed data, as well as the equilibrium state of small molecules (molecular conformation) evolving as a statistical mechanics system Experimental results across multiple tasks demonstrate that our model achieves best or competitive performance on baseline models in various types of datasets
RR, On investigation, we find the proposed models ineffective at learning such polynomials due to their designs Our model achieves performance gains of up to 10% over the state-of-the-art models and outperforms existing polynomial filter-based approaches in general
RR, Extensive experiments show that despite its simpler design, GSMT outperforms existing TAL methods, achieving new state-of-the-art performance on two benchmarks
RR, Our work demonstrates that a small compute spent on careful labeled data selection brings big annotation efficiency and model performance gain without changing the learning pipeline
RR, To demonstrate the effectiveness of TranSlowDown, we conduct a systematic evaluation on three public-available NMT systems: Google T5, Facebook Fairseq, and Helsinki-NLP translator  The experimental results show that TranSlowDown increases NMT systems’ response latency up to 1232%and 1056% on Intel CPU and Nvidia GPU respectively by inserting only three characters into existing input sentences Our results also show that the adversarial examples generated byTranSlowDowncan consume more than 30 times battery power than the original benign example Such results suggest that further research is required for protecting NMT systems against efficiency-oriented threats
RR, Our experiments encompass 16 alignment techniques (including ours), evaluated across 6 language pairs, synthetic and 4 NLP tasks We demonstrate that our solutions are particularly effective in the scenarios of limited and no parallel data
RR, Empirically, we observe comparable or better performance to supervised learning techniques in several medical imaging tasks, including sparse-view CT and undersampled MRI, while demonstrating remarkably better generalization to unknown measurement processes
RR, We conduct thorough evaluation of COPA on different games trained with different offline RL algorithms: (1) The proposed temporal aggregation in COPA significantly improves the certified robustness; (2) Our certifications for both per-state action stability and  cumulative reward bound are efficient and tight; (3) The certification for different training algorithms and games are different, implying their intrinsic robustness properties
RR, Experiments on the environment with various shapes of roads showed that our method finds the nearly optimal policies from early episodes, outperforming a baseline hierarchical reinforcement learning method, especially in narrow and complex roads
RR, We demonstrate improved convergence in practice on \ell_2-regularized logistic regression problems
RR, We demonstrate the proposed algorithms on diverse problems: classification with negative training examples, learning from rankings, weakly and self-supervised aerial imagery segmentation, co-segmentation of video frames, and coarsely supervised text classification
RR, Our empirical results on CarRacing and VizDoom demonstrate a clear advantage of learning and using ASRs for policy learning
RR, We empirically show that our approach substantially outperforms various conventional NPs in 1D regression and lotka-Volterra problem as well as image completion The proposed method achieves comparable results with state-of-the-art methods in the MovieLens-10k dataset, one of the real-world problems with limited users, perform well for the image completion task even with very limited meta-training dataset
RR, Experiments on popular implementations of gradient boosting show that the proposed method does not affect the complexity of learning algorithms and significantly increases quality on most standard benchmarks up to 1.5%
RR, We evaluate our approach on synthetic and real world datasets and generalize to novel scene configurations, producing photorealistic, physically accurate renderings of multi-object scenes
RR, To the best of our knowledge, DDT and UDT together constitute the first successes for offline state-marginal matching and inverse-RL imitation learning, allowing us to propose first benchmarks for these two important subfields and greatly expand the role of powerful sequence modeling architectures in modern RL
RR, On multiple datasets, we demonstrate that learning self symmetry maps with a deep functional map projects 3D shapes into a low dimensional canonical embedding that facilitates non-rigid shape correspondence via a simple nearest neighbor search Our framework outperforms multiple recent learning based methods on FAUST and SHREC benchmarks while being computationally cheaper, data-efficient, and robust
RR, Experimental evaluations show that our NAS-crafted MAKPConv network uses 96% fewer parameters on 3D point cloud classification and segmentation benchmarks with better performance Compared with state-of-the-art NAS-crafted model SPVNAS, our NAS-crafted MAKPConv network achieves ~1% better mIOU with 83% fewer parameters and 52% fewer Multiply-Accumulates
RR, Experiments show the agreement of RTD with the intuitive assessment of data representation similarity
RR, Experiments are conducted to show the effectiveness of the proposed method
RR, We demonstrate that LatentKeypointGAN provides an interpretable latent space that can be used to re-arrange the generated images by re-positioning and exchanging keypoint embeddings, such as generating portraits by combining the eyes, nose, and mouth from different images
RR, Experiments for image classification on EMNIST and CIFAR datasets, and next word prediction on the Stack Overflow dataset show that the proposed algorithm can effectively mitigate the impact of the distribution shift and significantly improve the final model performance
RR, We demonstrate the advantages of our learned equivariant representations for Atari games, in a data-efficient setting limited to 100k steps of interactions with the environment
RR, We evaluate the practicality of our objective through experiments on a set of navigation and locomotion tasks Our results verify the benefits suggested by our theory and show that our algorithms is able to recover near-optimal policies with fewer expert trajectories
RR, Empirical results show that our approach successfully generalizes uniform alignment, naturally incorporates domain information represented by graphs, and improves upon existing domain adaptation methods on both synthetic and real-world datasets
RR, We  empirically show the competitive performance of nSGDA on real-world datasets
RR, Our experiments show that MoDIR robustly outperforms its baselines on 10+ ranking datasets from the BEIR benchmark in the zero-shot setup, with more than 10% relative gains on datasets where the evaluation of DR models is sensitive enough
RR, We empirically show that PCR outperforms the current state-of-the-art baselines in terms of data efficiency on a series of pixel-based control tasks in the DeepMind control suite
RR, Our evaluation on the NAS-Bench-201 and MobileNet-based search space demonstrate that SUMNAS shows improved ranking ability and finds architectures whose performance is on par with existing state-of-the-art NAS algorithms
RR, We validate this hypothesis with semi-synthetic experiments as well as rigorous mathematical analyses on a simplified setting
RR, Numerical experiments for natural image denoising and low-dose computational tomography denoising demonstrate that proposed SSRL significantly improves the denoising quality over several existing self-supervised denoising methods
RR, Experimental results show that our Co-SSAT achieves the state-of-the-art performances on CIFAR-10 with \ell_{\infty adversaries and also has a good generalization ability of unseen attacks, i,e, other \ell_p norms, or larger perturbations due to the smoothness property of the loss surface
RR, Experimental results on benchmark datasets prove the effectiveness of CrossMatch on enhancing the performance of SDG methods in the OS-SDG setting
RR, In experiments, we show that our approach learns interpretable graph topologies, induces effective coordination, and improves performance across a variety of cooperative multi-agent tasks
RR, Our experiments show that DeepAA improves significantly over previous state-of-the-art automated data augmentation methods
RR,   We demonstrate that word and graph embeddings trained on standard datasets using several popular algorithms consistently share two distinct properties: (1) a decreasing neighbor frequency concentration with rank, and (2) specific clustering velocities and power-law based community structures
RR, In our experiments, we find that agents trained in an offline-online manner can outperform agents trained only offline or online, sometimes by a large margin, for different dataset sizes and data-collection policies
RR, Our experimental results show that the proposed sample probing approach improves the ZSL results even when integrated into state-of-the-art generative models
RR, By experimenting on state-of-the-art datasets, the results show that our framework outperforms previous works on both tasks
RR, We demonstrate that BAM generalizes many popular Bayesian update rules for non-stationary environments Through a variety of experiments, we demonstrate the ability of BAM to continuously adapt in an ever-changing world
RR, With larger learning rates, our results consistently suggest pruning outperforms training from scratch on multiple networks (ResNets, VGG11) and datasets (MNIST, CIFAR10, ImageNet) over most pruning ratios Our results suggest that weight removal in pruning breaks dynamical isometry, which fundamentally answers for the performance gap between a large finetuning LR and~a small one
RR, Our analyses and experimental results using state-of-the-art quantization methods on ImageNet and CIFAR-10 show the importance of using CSQ for weight in place of the conventional quantization scheme at extremely low-bit precision (2\sim3 bits)
RR, Our loss also produces more accurate models, with up to 4.0 points of lift across 9 tasks
RR, Experiments on both low and high-dimensional data show that SDOT-EGD is much faster and converges much better than state-of-the-art SDOT algorithms We also show our method's potential to improve GAN training by avoiding the oscillation caused by randomly changing the association between noise and the real images
RR,  In our experiments, ATC estimates target performance 2\text{--4\times more accurately than prior methods
RR, We evaluate our method on two popular physical system simulation datasets with various types of multi-body interactions Experimental results show that the proposed IRCM achieves the state-of-the-art performance on both the counterfactual reasoning and future forecasting tasks
RR, Experiments using MuJoCo locomotion tasks highlight that our method compares favorably to the baselines for ILO with transition dynamics mismatch
RR,  Our experiments emphasize that DisTop is agnostic to the ground state representation and that the agent can discover the topology of its environment whether the states are high-dimensional binary data, images, or proprioceptive inputs We demonstrate that this paradigm is competitive on MuJoCo benchmarks with state-of-the-art algorithms on both single-task dense rewards and diverse skill discovery without rewards
RR, Our dense pre-trained models also compare favorably against BERT pre-trained models in the few-shot setting, and achieves state-of-the-art performance on the BEIR benchmark when fine-tuned on MS-MARCO
RR, We demonstrate the effectiveness of SEAL on 7 feature-based and 3 text-based multi-label classification datasets
RR, Empirical evaluations on the commonly used Mujoco benchmark and a novel GridChaos task demonstrate that OVD-Explorer can alleviate over-exploration and outperform state-of-the-art methods
RR, We validate our findings through numerical experiments where our theory accurately predicts empirical findings and remains consistent with observations in deep neural networks
RR, We demonstrate the superior performance of the LIGS framework in challenging tasks in Foraging and StarCraft II and show LIGS is capable of tackling tasks previously unsolvable by MARL methods
RR, Experiments demonstrate that our method outperforms existing regularizers such as Jacobian, confidence penalty, and label smoothing
RR,  We provide experimental results by training deep learning end-to-end compression systems for performing denoising on SVHN and super-resolution on MNIST, and demonstrate consistency with our theoretical results
RR, We experiment on several public datasets and demonstrate significantly improved overall performance on forecasts at different levels of the hierarchy, compared to existing state-of-the-art hierarchical models
RR, Through experiments on three summarization datasets (CNN, Newsroom, XSum), we demonstrate that this gating mechanism automatically learns to assign contrasting summary styles to different HydraSum decoders under the standard training objective without the need for additional supervision Finally, our experiments demonstrate that our decoder framework is highly flexible: during inference, we can sample from individual decoders or mixtures of different subsets of the decoders to yield a diverse set of summaries and enforce single- and multi-style control over summary generation
RR, Finally, we demonstrate our model's robustness against adversarial attacks, where it outperforms the full-resolution model
RR, We find that CLD outperforms previous SGMs in synthesis quality for similar network architectures and sampling compute budgets We show that our novel sampler for CLD significantly outperforms solvers such as Euler--Maruyama
RR, The resulting locally smoothed model yields strong collective guarantees while maintaining high prediction quality on both image segmentation and node classification tasks
RR, In our experiments, we demonstrate that, by using comments, our method is able to learn better, more contextualised, representations, while also achieving competitive results on standard video-text retrieval benchmarks
RR, Our results on several synthetic and real graph datasets demonstrate that embeddings obtained from a random GNN paired with Maximum Mean Discrepancy (MMD) satisfies the aforementioned properties of a strong metric and performs competitively
RR, Experimental evaluation on simulated datasets and real mmWave wireless channels demonstrate that the proposed methods can adapt the MDN model using very limited number of samples, and improve or maintain the error rate of the autoencoder under changing channel conditions
RR, Extensive experiments on several benchmarks show that FAHGNN significantly outperforms other state-of-the-art methods for node classification tasks
RR, The experimental results on several benchmark datasets demonstrate that our approach can significantly reduce dynamics prediction errors and improve the performance of model-based RL methods on zero-shot new environments with unseen dynamics
RR, Experimental results show that EIL significantly outperforms strong baselines and approaches the level of training from the perfect demonstration on various simulated continuous control tasks and a ``learning-from-slides'' task
RR, Extensive link prediction experiments over 8 real-world networks demonstrate the advantages of PEG in generalization and scalability
RR, Extensive experiments also demonstrate our method often outperforms comparable SOTA methods with lesser data argumentation needed, smaller fine-tuning budget required, and sometimes even with much simpler procedure executed (e,g,, one-shot pruning v. iterative pruning, standard fine-tuning v. custom fine-tuning)
RR, We outperform competing baselines on standard classification datasets such as CIFAR-10, CIFAR-100, ImageNet, as well as long-tailed datasets such as CIFAR-100-LT
RR, Our experiments demonstrate that our method outperforms state-of-the-art max-loss strategy by 7.7% on CIFAR10 while achieving 6.3x speedup, and by 4.7% on SVHN while achieving 2.2x speedup, using 10% and 30% subsets, respectively
RR, Extensive experiment results are provided to demonstrate the power of the proposed recurrent parameter generator
RR, We perform experiments on MNIST and CIFAR10 datasets using different established feed-forward architectures and show that the winning ticket obtained from the proposed algorithm is much more robust
RR, Empirically, we observe significant ML API shifts from 2020 to 2021 among 12 out of 36 applications using commercial APIs from Google, Microsoft, Amazon, and other providers Extensive experiments show that MASA can estimate such API shifts more accurately than standard approaches given the same budget
RR, Extensive experiments demonstrate that our framework significantly improves the existing summary networks on learning more powerful summary statistics from sets and can be successfully integrated into metric-based few-shot classification and generative modeling applications, providing a promising tool for addressing set-input and meta-learning problems
RR, Overall, our results shed light on what equilibria or structural assumptions on the game may enable sample-efficient learning with many players
RR, We show experimentally how FCR allows us to disentangle the contributions of various components of graph datasets, and demonstrate the superior performance of Cold Brew on several public benchmarks and proprietary e-commerce datasets
RR, Finally, we experimentally corroborate our theory in the image classification setting, showing that FKD is amenable to ensemble distillation, can transfer knowledge across datasets, and outperforms both vanilla KD & other feature kernel based KD baselines across a range of architectures & datasets
RR, We empirically demonstrate that such a hybrid model improves upon a purely attention-based model for both solution quality and computational efficiency Our experiments on the min-max Capacitated Vehicle Routing Problem (mmCVRP) also confirm that the hybrid model is more suitable for coordinated routing of multiple vehicles than the attention-based model
RR, Extensive experiments on benchmark datasets show that our framework significantly outperforms state-of-the-art methods and achieves better accuracy and robustness under the same privacy guarantee
RR, Empirically, we investigate the model performance on a variety of different tasks: we use two new simulated tasks tasks to study the model's ability to handle extremely long-range dependencies, we demonstrate competitive performance on the challenging Pathfinder problem using vanilla attention
RR, We show that CNC significantly improves worst-group accuracy over existing state-of-the-art methods on popular benchmarks, e,g,, achieving 7.7% absolute lift in worst-group accuracy on the CelebA data set, and performs almost as well as GDRO trained with group labels
RR, Third, a wide range of experiments are conducted to further compare the weighting schemes in different modes
RR, In our experiments, we use two benchmark datasets for keyword spotting (Google speech command V2-35 and LibriWords) and the VoiceBank dataset for the speech enhancement task In all experiments, speech-MLP surpassed transformer-based solutions, achieving state-of-the-art performance with fewer parameters and simpler training schemes
RR, Empirically, we achieve significant running time improvements over existing protein docking software and predict qualitatively plausible protein complex structures despite not using heavy sampling, structure refinement, or templates
RR, Extensive experiments on the ranking problem, the knapsack problem, and the shortest path problem have demonstrated that our proposed method can achieve a significant performance compared to the other methods designed for PTO problems
RR, We demonstrate that these two distinctions prevent current program synthesis techniques from leveraging LARC to its full potential, and give concrete suggestions on how to build the next-generation program synthesizers
RR, Extensive experiments demonstrate that our approach improves both clean and robust accuracy compared to related techniques and state-of-the-art baselines
RR, Second, we show that we can achieve similar improvements using a single model instead of two with our proposed cocktail finetuning, which augments full finetuning via distillation from a lightweight model
RR, Through experiments, we demonstrated the effectiveness of the proposed method in both performance and the accuracy of the explanations provided
RR, We demonstrate these in two toy problems of sequence classification, and in a temporally-encoded MNIST dataset where our RC model achieves 99.19% accuracy after the first input time-step, outperforming the state of the art in temporal coding with SNNs, as well as in spoken-word classification of Google Speech Commands, outperforming non-RC-trained early inference with LSTMs
RR, Our experiments on several benchmark datasets demonstrate that our algorithm indeed exhibits fast convergence speed in practice and validate our theory
RR, These observations not only verify our intuition about data quality but may also open new opportunities to advance adversarial training
RR, Through extensive experiments, Our proposed framework shows better qualitative and quantitative performance with Visual Storytelling benchmark compared to conventional story-to-image models
RR, Extensive experiments conducted upon several objective recognition tasks show that STU-KD can improve the inference accuracy by up to 14.7%, as compared to the state-of-the-art schemes
RR, We demonstrate our approach in real-business-cycle models, a representative family of DGE models, with 100 worker-consumers, 10 firms, and a government who taxes and redistributes We validate the learned meta-game \epsilon-Nash equilibria through approximate best-response analyses, show that RL policies align with economic intuitions, and that our approach is constructive, e,g,, by explicitly learning a spectrum of meta-game \epsilon-Nash equilibria in open economic models
RR, Finally, we demonstrate that this policy set can be useful in a realistic lifelong reinforcement learning setting
RR, Experimental results demonstrate that adding discrete representation on four architecture variants strengthens ViT robustness by up to 12% across seven ImageNet robustness benchmarks while maintaining the performance on ImageNet
RR, We empirically demonstrate its relevance for complex tasks on graphs such as partitioning, clustering and completion
RR, We evaluate our method on classical RL control tasks, and show substantial improvements in sample efficiency and wall-clock time over state-of-the-art RL and differentiable simulation-based algorithms In addition, we demonstrate the scalability of our method by applying it to the challenging high-dimensional problem of muscle-actuated locomotion with a large action space, achieving a greater than 17x reduction in training time over the best-performing established RL algorithm
RR, We show this unified training framework achieves high-quality transcription results across a range of datasets, dramatically improving performance for low-resource instruments (such as guitar), while preserving strong performance for abundant instruments (such as piano)
RR, Experimental results show that the query-based framework can generate informative input-output examples which achieve and even outperform well-designed input-output examples of state-of-the-art methods
RR, Our extensive experimental results show that FedFB significantly outperforms existing approaches, sometimes achieving a similar tradeoff as the one trained on centralized data
RR,  We evaluate our algorithm on the Unsupervised Reinforcement Learning Benchmark, which consists of a long reward-free pre-training phase followed by a short adaptation phase to downstream tasks with extrinsic rewards
RR, In particular, we demonstrate the existence of feature overcorrelation in deeper GNNs, reveal potential reasons leading to this issue, and validate that overcorrelation and oversmoothing present different patterns though they are related Experimental results on various datasets demonstrate that DeCorr can help train deeper GNNs effectively and is complementary to methods tackling oversmoothing
RR, Experiments on corrupted MNIST, CU-Birds, and COCO datasets demonstrate the effectiveness of our approach
RR, Empirical evaluation on both synthetic and real-world data sets validates the effectiveness of our approach
RR, We demonstrate that our approach discovers interpretable discrete and continuous variables describing neuronal identity in two single-cell RNA sequencing datasets, each profiling over a hundred cortical neuron types
RR,  We demonstrate its quantitative and qualitative effectiveness by evaluating the latent representations learned on two benchmark datasets, DSprites and CelebA
RR, Our experiments on GNNs for transductive and inductive node classification demonstrate that ensembles with ENI are simultaneously more accurate (by up to 4.6% and 3.8%) and  faster (by up to 2.8\times and 5.7\times) when compared to the best-performing single models and ensembles without ENI, respectively In addition, GNN ensembles with ENI are consistently more accurate than single models and ensembles without ENI when subject to pruning, leading to additional speedups of up to 5\times with no loss in accuracy
RR, Evaluated on the Waymo Open Dataset and KITTI, our method demonstrates significant improvement compared to widely algorithms as well as recent deep methods based on the point cloud representation, in both the point cloud reconstruction quality and the downstream perception model performance
RR, Extensive experiments as well as ablation studies on two benchmarks demonstrate its superior performance in tackling the multiplicity of cross-modal retrieval
RR, Our results provide a general, systematic framework for analyzing different graph representation learning methods and demonstrate when a given approach is expected to perform well
RR, Extensive experiments on both synthetic and real-world data sets verify the efficacy of the proposed method
RR, Experimental results demonstrate that the proposed memory-driven semi-parametric approach produces more realistic images than purely parametric approaches, in terms of both visual fidelity and text-image semantic consistency
RR, Experiments on multiple benchmarks show that GeoDiff is superior or comparable to existing state-of-the-art approaches, especially on large molecules
RR, We demonstrate a speedup of up to 2.92x compared to the baseline model for image, video, and 3D shape representation and rendering tasks
RR,  Extensive experimental results suggest that:  1) on multi-stage tasks that are infeasible for the vanilla differentiable physics solver, our approach discovers contact points that efficiently guide the solver to completion;  2) on tasks where the vanilla solver performs sub-optimally or near-optimally, our contact point discovery method performs better than or on par with the manipulation performance obtained with handcrafted contact points
RR, We show through experiments on the MNIST and CIFAR10 datasets that this method is capable of representing networks which achieve identical performance to direct weight representation, and that transfer done this way preserves much of the performance between two networks that are distant in parameter space
RR, We demonstrate that our architecture makes action decisions by correctly attending to the relevant actions in both value-based and policy-based RL
RR, We demonstrate through experiments in both discrete and continuous domains that our learning algorithms recover a set of RatSkills by observing and explaining other agents’ movements, and plan efficiently for novel goals by composing learned skills
RR, We show state-of-the-art results on multiple semantic audio-visual navigation benchmarks, within the Habitat-Matterport3D simulator, where we also show improvements in generalisation to unseen regions and novel sounding objects
RR, Benchmark and real-world experiments show that our method outperforms several state-of-the-art approaches with respect to predictive uncertainty quality, robustness, and OOD samples identification
RR, Our empirical validation demonstrates that the proposed method outperforms prior arts by large margins
RR, We present numerical results verifying the predictions of our theory for non-convex subgradient descent
RR, In the experiments, we consistently observe an imbalance in conditional utilization rate between modalities, across multiple tasks and architectures
RR, Through systematic evaluation, we show that GMSA, even with weak instantiations, can break previous transductive-learning based defenses, which were resilient to previous attacks, such as AutoAttack (Croce and Hein, ICML 2020)
RR, We demonstrate the performance and scalability of our algorithm with several numerical examples
RR, As demonstrated in our experimental results, ignoring the impact of other agents actions on individual value can have a significant impact on the overall performance when there is increased competition among vehicles for demand
RR, Our results improve and extend the work of Liu and Talwar (STOC 2019)
RR, We achieve state-of-the-art results for OOD detection in the audio and EEG domain and observe considerable gains in semantically corrected vision benchmarks
RR, Our extensive evaluation on two datasets and two popular ODE solvers show that the samples generated through NODEAttack can increase up to 168%  energy consumption than average energy consumption of benign test data during inference time Our evaluation also shows the attack transferability is feasible across solvers and architectures
RR, In addition, we provide experiments on two-way MNIST and four-way fiber sensing datasets to demonstrate the effectiveness of our proposed model
RR, Our experiments ablate various components of CTRL and demonstrate that in combination with PPO it achieves better generalization performance on the challenging Procgen benchmark suite (Cobbe et al, 2020)
RR, Experiments show that SPLID outperforms previous Goal-Conditioned RL methods with a substantial margin
RR, Meanwhile, we also find that FIP improves data efficiency, achieving up to 10.0 points top-1 accuracy gain on few-shot classification
RR, In further, we demonstrate an interesting result that our method works better than one of state-of-the-art agents GATA, which uses environment rewards for some text-based games
RR, Our subsequent experimental results demonstrate the improvement LBGM obtains on communication overhead compared to federated learning baselines
RR, Extensive experiments demonstrate that Dict-BERT can significantly improve the understanding of rare words and boost model performance on various NLP downstream tasks
RR, Experimentally, we show that this term explicitly models the environment's stochasticity and can also be used in place of \epsilon-greedy exploration methods during transfer
RR, We demonstrate that our approach achieves highly competitive zero-shot performance compared to existing zero- and few-shot semantic segmentation methods, and even matches the accuracy of traditional segmentation algorithms when a fixed label set is provided
RR, Experimentally, we benchmark TRUFL on constrained generation tasks, and find that it outperforms prior approaches In addition, we find that rejection sampling results in higher validity for the constrained problems
RR, Empirical evaluation shows that FaST significantly improves over state-of-the-art methods on benchmark single/multi-objective molecular optimization tasks
RR, Lastly, our experiments demonstrate advantages of the proposed method compared to state-of-the-art streaming algorithms
RR, Our analytical results reveal that a) the metric losses on positive sample pairs are related to intra-class alignment; b) the metric losses on negative sample pairs serve as uniformity regularization on hypersphere
RR, These experiments will continue to improve with advances in generative modeling that enables simulating more interesting distribution shifts outside of standard augmentations
RR, Experiments are conducted to show that the proposed SignRFF is consistently better than the previous RFF-based method, and also outperforms other data-dependent and deep learning based hashing methods with sufficient number of hash bits Moreover, we also validate that the proposed ranking efficiency aligns well with the empirical search performance
RR, We demonstrate two metrics measuring separately the quality of the supernet's shared weights and the quality of the learned sampling distribution, as well as corresponding statistics approximating the reliance of the second stage search on these components of the supernet
RR, Empirically, we find that our methods improve predictive uncertainties on several tasks with minimal computational and implementation overhead
RR, We validate our approach on several retrieval benchmarks, querying with images and their associated free-form text modifiers Our method obtains state-of-the-art results without resorting to side information, multi-level features, heavy pre-training nor large architectures as in previous works
RR, We demonstrate the effectiveness of our model on both synthetic and real-world datasets
RR, Through our extensive experiments, we show that the proposed method provides robust OSR results with a simple inference process
RR, Comprehensive experiments and ablation studies over different losses and applications validate our theoretical analysis, as well as the effectiveness of our explicit label-marginal regularizers
RR,  We demonstrate that our architecture learns meaningful representations of drug datasets and provides a platform for goal-directed drug synthesis
RR, Extensive experiments demonstrate that our model achieves state-of-the-art performance on multiple face restoration benchmarks
RR, We further find contrastive methods achieve state-of-thet-art disentanglement performance on several widely used benchmarks, such as dSprites and Car3D
RR, We demonstrate the performance through numerical experiments on image classification, continuous normalizing flow, and time series regression We show that PNODE achieves the highest memory efficiency when compared with other reverse-accurate methods
RR, This significantly improves over the best prior rate of O(T^{-1/2)
RR, Extensive experiments show that our method with self-supervised and semi-supervised settings is able to considerably outperform previous deep learning methods on different datasets and a public bioinformatics benchmark
RR, We demonstrate an effective approach to training the sense disambiguation mechanism in our model with a distribution over word senses extracted from the output layer embeddings of BERT Experiments on the contextual word similarity and sense induction tasks show that this method is superior to or competitive with state-of-the-art multi-sense embeddings on multiple benchmark data sets, and experiments with an embedding-based topic model (ETM) demonstrates the benefits of using this multi-sense embedding in a downstream application
RR, Through quantitative experiments and listening tests, we demonstrate that this hierarchy can reconstruct high-fidelity audio, accurately predict performance attributes for a note sequence, independently manipulate the attributes of a given performance,  and as a complete system, generate realistic audio from a novel note sequence
RR, Experimental results show that AR2 consistently and significantly outperforms existing dense retriever methods and achieves new state-of-the-art results on all of them
RR, Experiments on random graphs, social graphs, and offlineQ-learning demonstrate the effectiveness of our method over common baselines
RR, We show that Guided-TTS achieves comparable performance with the existing methods without any transcript for LJSpeech Our results further show that a single speaker-dependent phoneme classifier trained on multispeaker large-scale data can guide unconditional DDPMs for various speakers to perform TTS
RR, Our empirical results show that it leads to improved performance across a range of different multi-task offline RL scenarios, including robotic manipulation from visual inputs and ant-maze navigation
RR, We show that the encoder successfully combine the higher-depth and the lower-depth log-signature knowledge, which greatly stabilizes the training process and increases the model accuracy We conduct experiments with multiple very long time-series benchmark datasets and the improvement ratio by our method is up to 75% in terms of various classification and regression evaluation metrics
RR, Our experiments conducted both on synthetic and real datasets show that MixRL significantly outperforms state-of-the-art data augmentation baselines
RR, Our results show that our schemes demonstrate faster convergences against standard optimization alternatives
RR, Through extensive evaluation, we demonstrate that our method outperforms all existing online BPP methods and is versatile in terms of incorporating various practical constraints
RR, Moreover, our model achieves good scalability for large graphs via the fast stochastic gradient variational Bayes inference algorithm The experimental results on real-world graphs demonstrate that our proposed model achieves the state-of-the-art performances on link prediction and community detection tasks while generating interpretable node representations
RR, Through numerical experiments of manifold-gradient mutual information, we show this behavior acts as a function of the effective problem dimensionality Our results suggest that taking the manifold-gradient mutual information into account can thus inform better robust model design in the future, and avoid leakage of the sensitive data manifold information
RR, Surprisingly, we find that ZerO achieves state-of-the-art performance over various image classification datasets, including ImageNet, which suggests random weights may be unnecessary for modern network initialization
RR, We show through examples and experiments that these defenses are inherently over-cautious We empirically demonstrate the efficacy of such attacks against state-of-the-art certifiable defenses
RR, Our objectives achieve state-of-the-art results on DomainBed, and give insights into the robustness of recent methods, such as CLIP
RR, Furthermore, we demonstrate that the HJ safety value can be learned directly on vision context, the highest-dimensional problem studied via the method to-date We evaluate our method on several benchmark tasks, including Safety Gym and Learn-to-Race (L2R), a recently-released high-fidelity autonomous racing environment
RR, Our results demonstrate that adversarial bias can also worsen a model's fairness gap on test data, even though the model satisfies the fairness constraint on training data
RR, We demonstrate that the proposed method significantly outperforms the state-of-the-art on multi-step forecasting benchmarks, while enjoying reduced computational complexity on several real-world datasets
RR, Despite the simplicity of our scheme, we demonstrate that it can significantly improve the performance of existing self-supervised learning methods for various visual tasks, including object detection and semantic segmentation
RR, Experiments show that our model not only outperforms the current state-of-the-art in terms of generalization on the newly augmented unseen test data, but is also able to retain its memorization capabilities - achieving competitive results on the standard test data
RR, We show that our algorithm can achieve O(\sqrt{1/K) convergence rate when the buffer size satisfies certain conditions Our experiments validate our theoretical results and show that our algorithm outperforms the traditional Adam SGD for GNN training with a small memory overhead
RR, Extensive experimental results on benchmark datasets and a real-life medical dataset demonstrate the effectiveness of our method
RR, Here, we demonstrate Q-learning on the closed-loop Dynamic Optical Micro-Environment (DOME) platform to control the motion of light-responsive Volvox agents The results show that Q-learning is efficient in autonomously learning how to reduce the speed of agents on an individual basis
RR, We prove the convergence of low-precision SGLD on strongly log-concave distributions, showing that with full-precision gradient accumulators, SGLD is more robust to quantization error than SGD; however, with low-precision gradient accumulators, SGLD can diverge arbitrarily far from the target distribution with small stepsizes We demonstrate that the resulting low-precision SGLD algorithm is comparable to full-precision SGLD and outperforms low-precision SGD on deep learning tasks
RR, We also demonstrate that the learned hierarchy of representations can be used for a scene-retrieval application with object-centric re-ranking
RR, Empirically, OPP can compete with the ideal dynamical isometry recovery method on linear networks
RR, Our experiments span three pretrained models, ten datasets, and four active learning approaches
RR, Empirical results validate that our model achieves comparable or better results than related state-of-the-art methods w,r,t, both fairness metrics and prediction performance
RR, We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta-learning
RR, We conduct extensive experiments to verify our theoretical findings, and show several counterexamples where robustness and generalization are negatively correlated when the sufficient conditions are not satisfied
RR, Experiments prove the effectiveness of the proposed approach using the large-scale PartNet-Mobility dataset in SAPIEN environment and show promising generalization capabilities to novel test shapes, unseen object categories, and real-world data
RR, Further, we conduct an extensive evaluation of prior representative out-of-distribution (OOD) detection methods on NAS datasets and observe an inconsistency in their performance
RR, Experimental studies demonstrate that Gradient Assisted Learning can achieve performance close to centralized learning when all data, models, and objective functions are fully disclosed
RR, We perform a thorough empirical evaluation in several simulated domains, including a novel application to a quantum compiling domain
RR, In this paper, we demonstrate that diffusion models can also serve as an excellent tool for semantic segmentation, especially in the setup when labeled data is scarce
RR, Our experiments on the real-world scanned 3D environments of Gibson and Matterport3D show that our method obtains 2 - 6× higher sample-efﬁciency and up to 57% higher performance over standard image-representation learning
RR, Experiments on benchmark and real-world datasets demonstrate the effectiveness of FedUL
RR, We show that it achieves a stronger notion of avoiding sharp minima: it can effectively eliminate sharp local minima entirely from its training trajectory
RR, We validate our theoretical results via extensive experiments on both diagnostic and high-dimensional domains including robot manipulation, maze navigation and Atari games, when learning from a variety of data sources
RR, Our empirical study on two different tasks verifies the improvement over recent baselines
RR,  Our results also corroborate recent findings that QR-DQN is able to generalize  to new observations better than DQN in the offline setting These findings demonstrate the need for careful consideration  of generalization in RL, and we hope that this line of research will continue  to shed light on generalization claims in the literature
RR, We also demonstrate our model on general sequences of sets and provide illustrative experiments modeling the sequential structure of the multiple strokes that make up symbols in the Omniglot data
RR, Experimental results corroborate our theoretical analysis on both regret and incentive costs
RR, Our approach achieves state-of-the-art accuracy on multiple reasoning benchmarks; it learns compact models with much less data and produces not only answers but also checkable proofs
RR, Empirical results show that current state-of-the-art neural models struggle on \mydata with very poor accuracy (the best result is 30.10% for \mydata and 36.15% for Chinese \mydata), while human experts can perform nearly 100% accuracy Further results indicate that human imitations can significantly help models learn logic from natural text
RR, We empirically demonstrate the advantage of Transformer-MGK in a range of practical applications including language modeling and tasks that involve very long sequences
RR, We validate our approach on four real-world or synthetic benchmark datasets, including two CIFARs, Clothing1M, and mini-WebVision, demonstrate significant improvements over representative methods on this challenging task set-up
RR, We conduct experiments on various datasets and show that our method can notably improve the performance of optimal transport methods in semi-supervised domain adaptation
RR, Our extensive experiments demonstrate that our method refines the performance of the diverse set of top models for COVID-19 forecasting and GDP growth forecasting In addition, we show that our model improves model evaluation too; hence policy-makers can better understand the true accuracy of forecasting models in real-time
RR, Our experiments show that this simultaneously improves prediction accuracy and calibration compared to a multitude of output regularization methods without impacting the uncertainty-based separability in multiple classification settings, including under distributional shift
RR, This significantly improves exploration efficiency, leads to robust long-horizon planning, and enables effective vision-and-language grounding
RR, We demonstrate that measuring the exemplar-rule trade-off while controlling for feature-level bias provides a more complete picture of extrapolation behavior than existing formalisms
RR, Experimental results on various few-shot NLG tasks including data-to-text generation and text summarization demonstrate that the proposed selective token generation significantly outperforms the previous additive learning algorithms based on the PLMs
RR, We also evaluate our method on the more challenging problem of estimating average marginal effects with continuous treatments, using semi-synthetic data of gasoline price changes on gasoline demand
RR, Through extensive experiments on synthetic and real datasets for conditional generative and probabilistic forecasting tasks, we demonstrate the efficacy and versatility of our theoretically motivated model as a distribution estimator and conditioner
RR, Our extensive experiment results show that GARNET increases adversarial accuracy over state-of-the-art GNN (defense) models by up to 9.96% and 15.17% on homophilic and heterophilic graphs, respectively
RR, Empirical results show significant gains over strong baselines on standard continual learning benchmarks
RR, Experimental results demonstrate that the proposed framework significantly reduces the computational costs of ViTs while maintaining comparable performance on image classification
RR, We demonstrate that DPP-TTS generates more expressive samples than baselines in the side-by-side comparison test while not harming the naturalness of the speech
RR, We conduct empirical evaluations on six robot control tasks with randomized dynamics The results demonstrate that VRDR can consistently accelerate the convergence of policy training in all tasks, and achieve even higher rewards in some specific tasks
RR, We evaluate our method on standard offlineRL benchmarks and show that it learns using as few as 5 demonstrations, and yields up to 17 times higher score compared to strong existing offline RL, behavior cloning (BC), and domain randomization baseline, thus successfully leveraging both offline datasets and simulators for better RL
RR, Empirically, in our experiments on eight datasets from diverse domains including image recognition, pose prediction, molecule property prediction, and medical image classification, we find that the proposed general MLTI framework is compatible with representative meta-learning algorithms and consistently outperforms other state-of-the-art strategies
RR, Experiments show our approach consistently outperforms both task-specialized algorithms and the previous RL methods
RR, In this new setting, we again demonstrate that there is negligible difference between the strong baseline and the existing state-of-the-art
RR, We quantitatively evaluate the method across domain adaptation tasks with shifts in label distributions Our experiments show that the proposed method is more robust against these shifts than other alignment-based baselines
RR, Furthermore, we demonstrate the capability of the proposed framework in learning representations that can improve label efficiency in downstream tasks
RR, Extensive experiments on supervised and self-supervised scenarios show that AMix consistently outperforms leading methods by a large margin
RR, We provide empirical evidence across a wide range of deep learning experiments showing a correlation between the problems in which AdamW exhibits an advantage over Adam-\ell_2 and the degree to which we expect the gradients of the network to exhibit multiple scales, thus motivating the hypothesis that the advantage of AdamW could be due to the scale-free updates
RR, We show that our simplification approach successfully removes superfluous information for tasks with injected distractors
RR, We demonstrate several strategies of SemiFL that enhance efficiency and prediction and develop intuitions of why they work Extensive empirical evaluations demonstrate that our communication efficient method can significantly improve the performance of a labeled server with unlabeled clients Moreover, we demonstrate that SemiFL can outperform many existing FL results trained with fully supervised data, and perform competitively with the state-of-the-art centralized SSL methods
RR, To demonstrate its generality, we instantiate our method on a large variety of isometry groups acting on the Euclidean space \mathbb{R^3
RR, In all three cases, we demonstrate improvements relative to both fully-equivariant and non-equivariant baselines
RR, We then demonstrate the usefulness of this layer in learning more model-efficient classifiers in a number of computer vision and natural language processing tasks
RR, We show AMAs are able to effectively circumvent action-dependent stochastic traps that immobilise conventional curiosity driven agents
RR, We demonstrate our method in obtaining tighter bounds on the worst-case performance of large convolutional networks in image classification and reinforcement learning settings
RR, Our results suggest that a general convergence characterization of optimal permutations cannot capture the nuances of individual function classes, and can  mistakenly indicate that one cannot do much better than random
RR, We conduct extensive experiments in two representative settings: (i) image restoration with the deep image prior, using an untrained DNN; and (ii) compressive sensing image reconstruction, using a pre-trained GAN generator Our results validate the prevailing existence of LIP, and that it can be found by iterative magnitude pruning (IMP) with surrogate tasks
RR, Empirically, we demonstrate state-of-the-art performance on six diverse environments, achieving near-optimal OOD detection performance
RR, Our results show that the learned policies are more symmetric, periodic and robust compared to methods found in previous literature
RR, Numerical results demonstrate that the ASWD significantly outperforms other Wasserstein variants for both synthetic and real-world problems
RR, We demonstrate our proposed method could significantly outperform conventional approaches in a control domain as well as using a medical simulator
RR, Finally, we demonstrate the performance of MetaTag in comparison with state-of-the-art algorithms for temporal graph classification problems
RR, We show for multiple few-shot benchmarks with different architectures and datasets that our method beats or matches that of the traditional learning methods in a few-shot regime
RR, Surprisingly, we find that these same models are able to perform complex multi-step computations --- even in the few-shot regime --- when asked to perform the operation "step by step", showing the results of intermediate computations
RR, Experimental evaluations based on \xi-learning with function approximation demonstrate the prominent advantage of \xi-learning over available mechanisms not only for general reward functions, but also in the case of linearly decomposable reward functions
RR, Empirical results show OOTD-based planner significantly outperforms model-free baselines in terms of sample efficiency and running scores
RR, We also show that NetVec is capable of generating high quality embeddings for real-world hypergraphs with millions of nodes and hyperedges in only a couple of minutes while existing hypergraph systems either fail for such large hypergraphs or may take days to produce the embeddings
RR, Extensive experiments conducted on WIDER FACE demonstrate the state-of-the-art accuracy-efficiency trade-off for the proposed SCRFD family across a wide range of compute regimes
RR, Extensive experiments show that DictFormer outperforms prior light-weight transformer by \sim2 BLEU for machine translation task and achieves \sim 1.7 lower perplexity for the language modeling task, when matching or reducing the model size of the original transformer
RR, We show that RegPGD, RegCVX, and RadialRL achieve high certiﬁed robustness among these Furthermore, we demonstrate that our certiﬁcations are often tight by evaluating these algorithms against adversarial attacks
RR, Extensive experiments on real-world datasets show that LLCG can significantly improve the efficiency without hurting the performance
RR, Moreover, results on real-world images indicate that our framework can disentangle interpretable features
RR, Experimental results show that Eigencurve can significantly outperform step decay in image classification tasks on CIFAR-10, especially when the number of epochs is small
RR, To demonstrate the effectiveness and efficiency of Count-GNN, we conduct extensive experiments on a number of benchmark graphs Results show that Count-GNN achieves superior performance in comparison to the state-of-the-art baselines
RR, Through a series of experiments in visually complex environments we compare different approaches for leveraging task information within the TARP framework with prior unsupervised representation learning techniques and (1) find that task-induced representations allow for more sample efficient learning of unseen tasks and (2) formulate a set of best-practices for task-induced representation learning
RR, We perform an extensive empirical evaluation, and show that FCause achieves state of the art results in several causal discovery benchmarks under different conditions reflecting real-world application needs
RR, Extensive experimental results show that the optimal choice within the PolyLoss is indeed dependent on the task and dataset
RR, Our experiments find that more complex design choices, such as the large sequence models and value-based weighting schemes used in some prior works, are generally not necessary Our results show that carefully designed RvS methods can attain results that match or exceed the best prior methods across a range of different offline RL benchmarks, including datasets with little or no optimal data Our results help to identify the limits of current RvS methods and identify some important open problems
RR, We tested our modulating fully connected layer on multiple classification, regression, and imputation problems, and it either improved performance or generated comparable performance to conventional neural network architectures concatenating reliability to the inputs
RR, We empirically demonstrate that the proposed SplitRegex framework substantially improves the previous regex synthesis approaches over four benchmark datasets
RR,  We show from experiments on a wide array of ML approaches from naive Bayes to logistic regression, that DL approaches are more robust (and accurate) to such adverarial attacks to the input sequences, while k-mer based feature vector representations are more robust than the baseline one-hot embedding
RR, We test them in two medical domains with plausible spurious signals
RR, We hope our results draw a clearer picture of what offline learning should look like when linear representations are provided
RR, Second, our result clearly indicates further improvements to be made in NLM pretraining for the benefit of Natural Language Understanding tasks
RR, We demonstrate that by carefully utilizing the widespread supervision among the image-text pairs, our De-CLIP can learn generic visual features more efficiently Our DeCLIP-ResNet50 outperforms its counterpart in 8 out of 11 visual datasets when transferred to downstream tasks
RR, Experimental results show that the proposed method PLS outperforms state-of-the-art baselines by a large margin, where both PLS and the baselines leverage the CLIP pre-trained image feature extractor
RR, We demonstrate the effectiveness of this approach across a number of classification tasks in natural language processing, computer vision, and computational chemistry
RR, Experiments on VAE and Bayesian neural network show that the proposed f-TVO performs better than cooresponding baseline f-divergence variational inference
RR,As a result of the latter point, BFT enables, for the first time, approximate public belief state search in imperfect-information games where the number of possible information states is too large to track tabularly
RR, Our findings show that the gap of evaluation accuracy between supervised learning and unsupervised learning in FL is both small and reasonable
RR, Results show that TrMRL achieves or surpasses state-of-the-art performance, sample efficiency, and out-of-distribution generalization in these environments
RR, We demonstrate empirically that DISDAIN improves skill learning both in a tabular grid world (Four Rooms) and the 57 games of the Atari Suite (from pixels)
RR, We demonstrate on MNIST, Fashion-MNIST and CIFAR-10 that this approach does not drastically deteriorate the clean test accuracy relative to PGD whilst it is robust to PGD attacks
RR, Through empirical evaluation, we demonstrate that WAFL generalizes better than the vanilla FedAvg in non-i,i,d, settings, and is more robust than other related methods in distribution shift settings Further, using benchmark datasets we show that WAFL is capable of generalizing to unseen target domains
RR, We validate DISSECT on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that it performs consistently well Finally, we present experiments showing applications of DISSECT for detecting potential biases of a classifier and identifying spurious artifacts that impact predictions
RR, We empirically evaluate the proposed AAVAE on image classification, similar to how recent contrastive and non-contrastive learning algorithms have been evaluated Our experiments confirm the effectiveness of data augmentation as a replacement for KL divergence regularization
RR, These results suggest RIRL is a powerful tool towards building AI agents that can mimic real human behavior
RR, Experiments reveal that our method achieves the comparable performance to the baseline on fast learning a set of vision-based tasks through watching a single video demonstration
RR,    Our extensive experiments on CIFAR-10 and CIFAR-100 datasets demonstrate that the proposed ECO convolution is faster than SOC in evaluation while leading to competitive standard and certified robust accuracies
RR, We demonstrate E-SSL’s effectiveness empirically on several popular computer vision benchmarks Furthermore, we demonstrate usefulness of E-SSL for applications beyond computer vision; in particular, we show its utility on regression problems in photonics science
RR, Finally, we experimentally show compression can reduce communication by over 90% without a significant decrease in accuracy over VFL without compression
RR, These demonstrations show that our metrics can address specific needs for measuring bias in multi-class classification
RR, Using a new behavioral Map Induction Task, we demonstrate that this computational framework explains human exploration behavior better than non-inductive models and outperforms state-of-the-art planning algorithms when applied to a realistic spatial navigation domain
RR, Additional experiments explore the role of language-based encodings in these results; we find that it is possible to train a simple adapter layer that maps from observations and action histories to LM embeddings, and thus that language modeling provides an effective initializer even for tasks with no language as input or output Together, these results suggest that language modeling induces representations that are useful for modeling not just language, but natural goals and plans; these representations can aid learning and generalization even outside of language processing
RR, Extensive experiments demonstrate that our method provides better in-situ customization than the existing heterogenous-architecture FL methods
RR, We demonstrate the effectiveness of our approach to both empirical and certified robustness on six recent state-of-the-art models and using several datasets Our results show that our method effectively reduces robust and inaccurate samples by up to 97.28%
RR,We prove that minimizing a weighted mean results in optimizing the higher-order moments of the loss distribution such as the variance, skewness, and kurtosis Experimental results show that the weighted mean trick exhibits similar performance with other specialized robust loss functions when training on noisy datasets while providing a stronger theoretical background
RR, We show a strong correlation between human evaluation of stories and those of CARP
RR, We show significant improvement by on-target adaptation, which learns the representation purely on target data, with only source predictions for supervision (without source data or parameter fine-tuning)
RR, In experiments, we also demonstrate the interpretability of SHAQ that is lacking in other state-of-the-art baselines
RR, Asymptotically, the cost of our approach scales only linearly with the number of clients, whereas the naive MPC median scales quadratically Empirically, we show that our method inherits the robustness properties of the median while converging faster than the naive MPC median for even a small number of clients
RR, The results can also be used to stably combine recurrent networks and physical systems with quantified contraction properties We perform experiments with these combined networks on benchmark sequential tasks (e.g permuted sequential MNIST) to demonstrate their capacity for processing information across a long timescale in a provably stable manner
RR, We evaluate our differentiable nonparametric belief propagation (DNBP) method on a set of articulated pose tracking tasks and compare performance with learned baselines Results from these experiments demonstrate the effectiveness of using learned factors for tracking and suggest the practical advantage over hand-crafted approaches
RR, We demonstrate the effectiveness of iLQR-VAE on a range of synthetic systems, with autonomous as well as input-driven dynamics
RR,  Experiments demonstrate that AutoCoG produces state-of-the-art performance at standard benchmarks including Cora, PubMed, and Citeseer, outperforming both state-of-the-art hand-crafted  GNNs as well as recent  GNN-NAS methods  Our method consistently achieves state-of-the-art (SOTA) results on Cora, Citeseer, Pubmed, and ogbn-arxiv
RR,   We demonstrate the empirical efficacy of our algorithm by showing privacy/utility trade-offs on linear regression, and deep learning benchmark datasets (CIFAR-10, EMNIST, and WikiText-2) We show that our algorithm not only significantly improves over traditional DP-SGD, which does not have access to public data, but also improves over DP-SGD on models that have been pretrained with the public data to begin with
RR, We demonstrate the effectiveness of the proposed framework on a diverse set of summarization datasets, including narrative, conversational, scientific documents and news Our model achieves (1) competitive or better performance on short documents with higher memory and compute efficiency, compared to full attention transformers, and (2) state-of--the-art performance on a wide range of long document summarization benchmarks, compared to recent efficient transformers These results indicate the general applicability and benefits of the proposed framework
RR, Extensive experiments on different types of backdoor attacks show that, by introducing a lightweight alteration for minimax optimization to the existing noisy-label defense algorithms, the robustness against backdoor attacks can be substantially improved, while the intial form of those algorithms would fail in presence of a backdoor attacks
RR, Experimental results show that our LSSAMP can generate peptides with multiply ideal physical attributes and a high probability of being predicted as AMPs by public AMP prediction models
RR, We also perform extensive experiments and show that the use of hybrid model aggregation via D2D and D2E communications in HL-SGD can largely improve the communication efficiency of federated learning
RR, Extensive experimental results demonstrate our proposed algorithm to achieve state-of-the-art performance under strong adversarial attacks
RR, We conduct experiments using weakly supervised, fair, and hard negatives contrastive learning, showing CCL-K outperforms state-of-the-art baselines
RR, In our experiments, we empirically find that standard graph convolutional networks (GCNs) can actually achieve better performance than such carefully designed methods on some commonly used heterophilous graphs
RR, Our results show that finer categorization of benchmarks on the basis of characteristics like the density of reward, planning horizon of the problem, presence of task-irrelevant components, etc., is crucial in evaluating algorithms
RR, Our experiments show that our network outperforms state-of-the-art matting methods significantly on the Video240K SD dataset
RR,  Empirical results show that FedDrop can drastically reduce the amount of FLOPs required for training with a small increase in communication, and push the Pareto frontier of communication/computation trade-off further than competing FL algorithms
RR, Furthermore, we demonstrate that relative to non-affordance-aware methods, HAL agents are better able to efficiently learn complex tasks, navigate environment stochasticity, and acquire diverse skills in the absence of extrinsic supervision---all of which are hallmarks of human learning
RR, We experimentally verified that the developed method can achieve the same performance as existing methods
RR, As a result, we achieve high-quality single view indoor scene reconstruction results learning directly from a real-world scanned dataset (e,g, ScannetV2) We comfortably advanced the state-of-the-art results with several established datasets including ShapeNet and ScannetV2; extensive quantitative analysis confirmed that our proposed DGS module plays an essential role in achieving this performance improvement
RR, We assess the prediction and control performance of ICGNN on several benchmarks and physical heat diffusion problems, respectively We confirm that ICGNN significantly outperforms non-input convex GNN to solve the design optimization problem
RR, Empirical studies on public datasets demonstrate that our method significantly outperforms the state-of-the-art GNN-based AL methods in terms of both accuracy and labeling cost
RR, Results on a simulated human-assisted navigation problem demonstrate the effectiveness of our framework: aided with an interaction policy learned by our method, a navigation policy achieves up to a 7× improvement in task success rate compared to performing tasks only by itself
RR, Empirically, we demonstrate the use of CAGE for: (a) inferring cause-effect relationships within a deep generative model trained on both synthetic and high resolution images, and (b) guiding data augmentations for robust classification where CAGE achieves improvements over current default approaches on image datasets
RR, We show that our proposed method can reduce quantization error and converges faster compared with the methods directly quantizing the model updates
RR, Extensive experiments on real image datasets (e,g,, CIFAR-10 and CelebA) verify the effectiveness and correctness of our method, even when compared to a model trained with fully annotated datasets
RR, We show that our method (SS-MAIL) outperforms prior state-of-the-art methods on real-world prediction tasks, as well as on custom-designed synthetic experiments
RR,  Our results show that the compositional training approach dramatically improves both the feature representations and the testing AUC score compared with traditional deep learning approaches, and yields better performance than the two-stage approaches for DAM as well
RR, In our experiments, we demonstrate that this approach consistently improves robust ResNet and vision transformer models, achieving accuracy gains of 1-8% over standard model evaluation and also generally outperforming prior augmentation and adaptation strategies We achieve state-of-the-art results for test shifts caused by image corruptions (ImageNet-C), renditions of common objects (ImageNet-R), and, among ResNet-50 models, adversarially chosen natural examples (ImageNet-A)
RR, We show that our approach can reconstruct high-frequency textures for arbitrary document shapes in both synthetic and real scenarios We also demonstrate the usefulness of our system by applying it to document texture editing
RR, We conduct experiments on both discrete and continuous control problems to show our approach's efficacy and examine the practical implication of our method in an autonomous driving application
RR, Extensive experiments show consistent performance improvements and an efficient personalization of FedBABU
RR, Our results motivate the systematic collection and curation of data during shaping by demonstrating curriculum learning in RNNs as a tool to probe and differentiate learning principles used by biological systems, over conventional statistical analyses of learned state spaces
RR, Our experiments demonstrate the superiority of FSL in real-world FL settings; in particular, (1) FSL achieves similar performances as state-of-the-art FedAvg with significantly lower communication costs: for CIFAR10, FSL achieves same performance as Federated Averaging while reducing communication cost by \sim35%
RR, We further demonstrate that the most popular MCC architecture in deep learning can be mathematically formulated as a LTR pipeline equivalently, with a specific set of choices in terms of ranking model architecture and loss function Comprehensive empirical results on both text and image classification tasks, with diverse datasets and backbone models (e,g,, BERT for text classification and ResNet for image classification) show the value of our proposed framework
RR, We verify experimentally that simply using GNP for every aggregation and readout operation enables GNNs to extrapolate well on many node-level, graph-level, and set-related tasks; and GNP sometimes performs even better than the best-performing choices among existing pooling functions
RR, Concretely, we demonstrate that instruction modeling significantly improves performance in planning environments when training with a limited number of demonstrations on the BabyAI and Crafter benchmarks
RR, We validate the proposed PNN-RISE method via a synthetic dataset, power system dataset, and mass-damper system dataset Numerical results show the universal capability of the PNN-RISE approach to quickly identify the hidden physical models without local optima, opening the door for the fast and highly accurate discovery of the physical laws or systems with external loads
RR, We demonstrate DESTA’s ability to tackle challenging tasks and compare against state-of-the-art RL methods in Safety Gym Benchmarks which simulate real-world physical systems and OpenAI’s Lunar Lander
RR, To show the effectiveness of our approach, we contribute new data sets to the research community and use both new as well as existing data sets to empirically verify that we can separate normal from abnormal internal appliance behaviour independent of the external signals in data sets from IoT and DevOps
RR, Empirically, we show that current unsupervised feature learning frameworks already generate dense features whose correlations are semantically consistent
RR, Our result shows that the number of iterations to reach an \epsilon-NE scales linearly, instead of exponentially, with the number of agents
RR, Experiments show that our algorithm is able to learn valid prediction sets and improve the efficiency significantly over existing approaches in several applications such as prediction intervals with improved length, minimum-volume prediction sets for multi-output regression, and label prediction sets for image classification
RR, Extensive experiments on image classification datasets and a single-cell dataset demonstrate that ORCA consistently outperforms alternative baselines, achieving 25% improvement on seen and 96% improvement on novel classes of the ImageNet dataset
RR, Extensive experiments and ablation study on the D4RL dataset validate our framework and the effectiveness of our algorithmic designs
RR, Experiments show that SVIB outperforms current SOTA methods in multiple benchmarks
RR, Our results demonstrate two very different behaviors when the model dimension exceeds the number of features versus the opposite scenario We demonstrate experimentally that our theoretical results on optimal early stopping time corresponds to the training process of deep neural network
RR, We also demonstrate empirically that our method instantiated with a well-designed general purpose mixture likelihood family can obtain superior performance for a variety of tasks across time-series forecasting and regression datasets with different data distributions
RR, Empirical results in various domains show high-quality solutions can be learned for large domains when using our methods
RR, Experimental results demonstrate that AFML can maintain the highest personalized accuracy compared to alternative leading frameworks, yet with a minimal number of communication rounds and local updates needed
RR,  In this paper, we demonstrate that under suitable settings, this stochastic trick can be reduced to a more interpretable deterministic form, allowing us to better explain its behavior, including an emergent regularization effect, and motivate broader application scenarios  Our experimental results corroborate these analyses while also demonstrating improved node classification performance applying the label trick in new domains
RR, Extensive experiments on a wide range of tasks and models demonstrate that AdaMomentum exhibits state-of-the-art performance consistently
RR, Experiments on two different domains, Java source code and Wikipedia text, demonstrate that locality features improve model efficacy over models without access to these features, with interesting differences
RR, Corresponding with our analysis, experimental results show that the construction in our proof can help inject convolutional bias into Transformers and significantly improve the performance of ViT in low data regimes
RR, ii) We archive much better results in i2u candidates generation compare to strong baselines
RR, We demonstrate dramatically large gains in accuracy on a wide variety of continual learning benchmarks
RR, Extensive experiments show that SparRL outperforms all prevailing sparsification methods in producing high-quality sparsified graphs concerning a variety of objectives
RR,  Our results show that the proposed architecture is better able to reflect variable uncertainty through time due to sparse and irregular sampling than a range of baseline and traditional models, as well as recently proposed deep latent variable models that use homoscedastic output layers
RR, Our experiments validate the superior performance of PER-ETD and its advantage over ETD
RR, Empirically, we show that our path auxiliary algorithms considerably outperform other generic samplers on various discrete models for sampling, inference, and learning
RR, Our experiments with T5, GPT, BERT, and BART models show that MEND is the only approach to model editing that produces effective edits for models with tens of millions to over 10 billion parameters
RR, Experiment results show that our model (using seismic data alone) yields comparable accuracy to the supervised counterpart (using both seismic data and velocity map)
RR, In experiments, our model successfully learns to capture word analogies from word2vec representations and shows better performance than other learning-based strategies
RR, We confirm that the proposed method, named Stein Latent Optimization for GANs (SLOGAN), successfully learns balanced or imbalanced attributes and achieves state-of-the-art unsupervised conditional generation performance even in the absence of attribute information (e,g,, the imbalance ratio) Moreover, we demonstrate that the attributes to be learned can be manipulated using a small amount of probe data
RR, We show that our approach performs better than the current state-of-the-art methods in terms of accuracy and forgetting
RR, We show that the performance steadily improves when we increase the size of memory up to 131k tokens
RR, We show that our approach outperforms previous work in improving the robustness of publicly available pretrained image classifiers to common corruptions on such challenging benchmarks as ImageNet-C
RR, We also demonstrate that as a readily plug-in operation with negligible computation overhead, TAdaConv can effectively improve many existing video models with a convincing margin
RR, We experimentally demonstrate the advantages of PIS compared with other start-of-the-art sampling methods on a variety of tasks
RR, Our comprehensive experimental study reveals that both  the community quality and entropy can provide new insights into the deep learning models' performances, thus paves a novel way of explaining deep learning models directly from the neurons' activation pattern 
RR, Our numerical experiments show that the proposed algorithm is competitive against other state-of-the-art active learning techniques in the context of domain adaptation, in particular on large data sets of around one hundred thousand images
RR, The experimental results underscore the importance of regularization in TP in practice
RR,  Experiments show that our approach, Transform2Act, outperforms prior methods significantly in terms of convergence speed and final performance
RR, In our experiments we show that OSSuM can perform similar to gradient-based pruning techniques at initialization, prior to training Further, we empirically demonstrate that OSSuM can be used to efficiently prune trained networks as well
RR, In our evaluation, we compare I-BAU with six state-of-art backdoor defenses on seven backdoor attacks over two datasets and various attack settings, including the common setting where the attacker targets one class as well as important but underexplored settings where multiple classes are targeted
RR, Experiments show that kernel deformed exponential families can attend to non-overlapping intervals of time
RR, Comprehensive experiments on Cifar-10 and ImageNet demonstrate that by attacking the MSM, we can obtain stronger transferable adversarial examples to fool black-box models including adversarially trained ones, with much higher success rates than existing methods
RR, Our experiments establish that each of the following lead to a measurable drop in robustness: i) layers that linearly reduce dimension, ii) sparsity induced by ReLU activations and, iii) mismatches in the attacker constraints at train and test time
RR, We validate the effectiveness compared to existing keypoint detection methods
RR, We demonstrate the power of this mechanism-based perspective by showing that we can leverage our results to generalize existing identifiable representation learning results These results suggest that by exploiting inductive biases on mechanisms, it is possible to design a range of new identifiable representation learning approaches
RR, We confirm that the maximizer of the objective in the EQUMRL directly corresponds to an MV efficient policy under a certain condition
RR, This demonstrates that the perceived variance in RL is not necessarily inherent to the problem definition and may be addressed through simple architectural modifications
RR, Our experiments indicate our feature-attention mechanism enables CPS to generalize better than recurrent architectures that attend to observations with spatial attention
RR, Through numerical experiments with two datasets, we demonstrate that unsupervised learning can be aided by the nonlinearity inherent in our kernel method
RR, Experimental results on several clustering benchmark datasets as well as ImageNet-1K demonstrate that the proposed NCC outperforms the state-of-the-art methods by a significant margin
RR, In the experiments, we demonstrate that our algorithm outperforms the state-of-the-art in task-oriented dialogue benchmarks including MultiWOZ and ConvLab
RR, Extensive experiments illustrate the efficiency and the accuracy of our method for solving Green's function
RR, Such results highlight the need to develop an appropriate theory with convergence guarantees that are powerful enough to inform practice
RR, We conduct experiments on large-scale bird dataset as well as on cell classification problems Our results demonstrate that LAM achieves a lower hierarchical cost-sensitive loss in high accuracy regions, compared to previous methods and their modified versions for a fair comparison, even though they are not directly optimizing this loss
RR, This modification results in a smoother gradient profile, which we demonstrate empirically and theoretically We demonstrate that modifying the softmax gradients in ConvNets may result in increased training accuracy, thus increasing the fit across the training data and maximally utilizing the learning capacity of neural networks
RR, We demonstrate the merits of cell2state in challenging downstream tasks including cell state prediction and finding dynamically stable clusters
RR, Experiments show that our framework has comparable performance to the optimal scheme
RR, We evaluate our model on standard and compositional action recognition on Something-Something V2, standard action recognition on Epic-Kitchen100 and Diving48, and spatio-temporal action detection on AVA We show strong improvement in performance across all tasks and datasets considered, demonstrating the value of a model that incorporates object representations into a transformer architecture
RR, Our experiments show that DSE significantly outperforms the state-of-the-art DiffAI method on these tasks
RR, Extensive experiments and in-depth investigations, with diverse network backbones (i,e, BERT, GPT-2, and DeBERTa) on dozens of datasets, consistently demonstrate highly impressive parameter-/training-/inference-efficiency, while maintaining competitive downstream transfer performance
RR, In addition, it is further shown that the private information in each view can be provably disentangled from the shared using proper regularization design
RR, Extensive experiments conducted on two large-scale challenging datasets, i,e, YouCookII and ActivityNet Captions, demonstrate that the proposed method performs favorably against the state-of-the-art methods
RR, Experiment results show that our method outperforms many state-of-the-art alternatives
RR, Our results show significant improvement in terms of sample efficiency on discrete and continuous control tasks
RR, Extensive experiments on real graph datasets, for both SED and GED, establish that NeuroSED achieves \approx 2 times lower RMSE than the state of the art and is \approx 18 times faster than the fastest baseline
RR, In this work, we demonstrate the feasibility of learning a closed-loop control policy for additive manufacturing
RR, We evaluate our algorithms on real datasets and show significant improvements in the quality of clustering
RR, Second, our approach performs the best in most cases, when comparing our approach with other baseline representation learning methods that also leverage auxiliary data information Third, we show that our approach also works well with unsupervised constructed clusters (e,g,, no auxiliary information), resulting in a strong unsupervised representation learning approach
RR, Finally, we conduct quantitative and qualitative experiments on synthetic and real-world datasets to demonstrate the effectiveness of DEGREE on node classification and graph classification tasks
RR, Our experiments on the CUB and the Oxford Flower datasets show that the HMVAE can represent multimodal heterogeneity and outperform existing methods in sample generation quality and quantitative measures as the held-out log-likelihood
RR, Empirically we find that these regularizers dramatically improve the stability of TD and FVI, while allowing RM to match and even sometimes surpass their generalization performance with assured stability
RR, We experimentally verify the efficacy of our approach, first on models that lie close to the theoretical assumptions (large width, proper initialization, etc) and, further, on more practical scenarios, with those assumptions relaxed In addition,  we show that our perturbations exhibit strong transferability between models
RR, Remarkably, our results show that the popular heuristic based on the Shapley value may choose the worst data subset in certain practical scenarios, which sheds lights on its large performance variation observed empirically in the past work Our evaluation shows that DataSifter achieves and most often significantly improves the state-of-the-art performance over a wide range of tasks, including backdoor, poison, noisy/mislabel data detection, data summarization, and data debiasing
RR,  Empirically,  we demonstrate that our method not only improves the sample efficiency of prior methods but also successfully solves temporally extended navigation and manipulation tasks,  where prior goal-conditioned RL methods (including those based on graph search) fail to solve
RR, Our results show that combining the two techniques, EsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation, outperforming prior arts with around an order magnitude of higher throughput
RR, Finally, we demonstrate that AutoOED can control and guide real-world hardware experiments in a fully automated way without human intervention
RR, We tested our model on two tasks, program classification and binary similarity detection, and obtained accuracy of 80.35% and 45.16%, respectively
RR, Meanwhile, through extensive experiments on various practical settings, we demonstrate that {\sf DSM consistently outperforms existing solutions in terms of model utility by a large margin
RR, We evaluate our method on MS COCO and Pascal VOC under different experiment protocols, and our method significantly improves on previous state of the arts
RR, Experiments on two real-world 3D scan datasets, Replica, and Matterport3D verify the effectiveness and the robustness of the agent trained under our designed environment when transferred to the clean environment or the one containing sound attackers with random policy
RR, We demonstrate on multiple public datasets that HD-cos matches the quality of the more expensive baselines
RR, As a consequence, we can optimize the amount of adaptation necessary to solve a new task using stochastic gradient descent, in addition to learning the initial conditions as is standard practice in gradient-based meta-learning
RR, Empirical results show that our proposed PA-AD universally outperforms state-of-the-art attacking methods in various Atari and MuJoCo environments
RR, Our experiments show that the proposed fast ensemble method is able to substantially improve the speed vs  quality trade-off
RR,  Our results on similarity tests show that the VAE approach is able to accurately represent the temporal attributes of the original data
RR, We demonstrate that heatmap output enables a higher level of control on the predicted trajectories compared to vanilla multi-modal trajectory regression, allowing to incorporate additional constraints for tighter sampling or collision-free predictions in a deterministic way We report our results on the Interaction multi-agent prediction challenge and rank 1^{st on the online test leaderboard
RR, Extensive experiments have demonstrated the effectiveness of proposed MSF-GNNs on node classification compared to state-of-the-art methods
RR, Experiments on ImageNet show that MILe achieves higher accuracy and ReaL score than when using the standard training procedure, even when fine-tuning from self-supervised weights
RR, Extensive experiments demonstrate the effectiveness of CDD-RED under different evaluation metrics (ranging from the pixel-level, prediction-level to the attribution-level alignment) and a variety of attack generation methods (e,g,, FGSM, PGD, CW, AutoAttack, and adaptive attacks)
RR, We empirically validate that Pixelated Butterfly is 3\times faster than Butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs
RR, We demonstrate that the FR facilitates exploration, the selection of efficient paths to desired states, allows the agent, under certain conditions, to plan provably optimal trajectories defined by a sequence of subgoals, and induces similar behavior to animals avoiding threatening stimuli
RR,  We demonstrate the system's effectiveness on a sequence of problem classes of increasing difficulty and show that it outperforms clustering-based methods, classic filters, and unstructured neural approaches
RR, Experiments show that our method achieves the state-of-the-art on both miniImageNet and tieredImageNet
RR,  We demonstrate the superior performance of our algorithm over state-of-the-art approaches on a number of  benchmark environments with sparse rewards {and censored state  Further, we demonstrate the value of our approach via implementing LOGO on a mobile robot for trajectory tracking and obstacle avoidance, where it shows excellent performance
RR, Our experiments demonstrate that our approach significantly improves the feedback-efficiency of the state-of-the-art preference-based method on a variety of locomotion and robotic manipulation tasks
RR, We evaluate our proposed method on various image recognition and NLP tasks, covering a variety of network architectures, and confirm that AdaFocal consistently achieves significantly better calibration than the competing state-of-the-art methods without loss of accuracy
RR, Our experiments demonstrate our proposed methods consistently help ViT benefit from deeper architectures, bringing \ge 1.0% performance gain with little parameter overhead
RR, Empirically, we demonstrate that neural composition indeed captures the underlying structure of this space of problems
RR, Using our loss function, we achieve new state-of-the-art tradeoffs between privacy and accuracy on MNIST, FashionMNIST, and CIFAR10
RR, In experiments, we show that this simple architecture achieves zero-shot generation of novel images without text and better quality in generation than the models based on mixture decoders
RR, We validate our method with (a) the recovery of molecules using conditional generation, (b) the identification of synthesizable structural analogs, and (c) the optimization of molecular structures given oracle functions relevant to bioactivity and drug discovery
RR, Experiments on both direction of arrival estimation task and the physical location estimation task shows our framework outperforms existing methods by a large margin
RR, Experiments show that RSPO is able to discover a wide spectrum of strategies in a variety of domains, ranging from single-agent particle-world tasks and MuJoCocontinuous control to multi-agent stag-hunt games and StarCraftII challenges
RR, Experiments on various real-world TS datasets demonstrate that DeepFIB outperforms state-of-the-art methods by a large margin, achieving up to 65.2% relative improvement in F1-score
RR, Extensive experiments show that SIT consistently outperforms the recent state-of-the-art methods and baseline on different datasets with significant margins
RR, AMOS outperforms ELECTRA and recent state-of-the-art pretrained models by about 1 point on GLUE and SQuAD benchmarks for BERT base-sized models
RR, We show that this framework is compatible with most modern 3D detection architectures and can substantially improve their average precision on multiple autonomous driving datasets, most notably by more than 300% on the challenging cases
RR, In particular, our approach demonstrates strong and promising performance against SOTA methods on several public datasets including Kinetics400, Moments In Time, and Something-Something V2 (SSV2), Jester and Diving48 The results on both Kinetics400 and SSV2 are comparable to some of the best-performed CNN approaches based on spatio-temporal modeling
RR, Empirically, our PC-based (de)compression algorithm runs 5-20x faster than state-of-the-art neural compression algorithms while achieving similar bitrates Our results highlight the potential impact that non-standard learning architectures may have on neural data compression
RR, Empirically, we show that CMLMC achieves state-of-the-art NAR performance when trained on raw data without distillation and approaches AR performance on multiple datasets
RR, We empirically validate our conclusions using two experimental setups: a standard bilinear min-max problem, and large-scale distributed adversarial training of transformers
RR, Our results contribute to clarifying the role of learning regime, architecture, and dataset structure in promoting systematic generalization, and provide theoretical support for empirical observations that iterated learning can improve systematicity
RR, To this end, we adopt various state-of-the-art deep learning concepts, such as autoencoders (AEs), generative adversarial networks (GANs), neural ordinary differential equations (NODEs), neural controlled differential equations (NCDEs), and so on Our method achieves the state-of-the-art synthesis performance for the irregular and intermittent time-series synthesis task
RR, Extensive experiments verify that GIA with HAO can break homophily-based defenses and outperform previous GIA attacks by a significant margin
RR, Empirical evaluations for maze-solving and robotic manipulation tasks demonstrate that our approach improves long-horizon performance and enables better zero-shot generalization than alternative model-free and model-based methods
RR, Our robustness certificates guarantee that the final total reward obtained by policy smoothing remains above a certain threshold, even though the actions at intermediate time-steps may change under the attack Our experiments on various environments like Cartpole, Pong, Freeway and Mountain Car show that our method can yield meaningful robustness guarantees in practice
RR, We demonstrate the effectiveness of our approach using nine network architectures across four diverse tasks, including ImageNet classification and transformers
RR, In conclusion, divisive normalization enhances image recognition performance, most strongly when combined with canonical normalization, and in doing so it reduces manifold capacity and sparsity in early layers while increasing them in final layers, and increases low- or mid-wavelength power in the first-layer receptive fields
RR, We perform experiments that quantify the dependence of the maximum deviation on model smoothness and certification set size The experiments also illustrate how the solutions that maximize deviation can suggest safety risks
RR, Our findings also indicate potential benefits of moving language emergence forward with natural language resources and models